---
title: "conditional"
execute: 
  echo: true
format: revealjs
editor: visual
---

## Conditional models

-   We are now going to introduce predictors to our growth models beyond time. These predictors are similar to predictors in standard regression -- dummy for nominal, interactions change lower order terms, etcetera.

-   This is all just regression, so the same interpretation is the same. It is now only harder because we have multiple levels and lots of potential for interactions

## Level 2 group predictors

-   Do groups differ in their initial status? Level 2, person variable that is dummy coded. Note that group here only is measured once, it is a between person variable.

level 1: $${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2:

$${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + U_{1i}$$

## Interpretation of conditional fixed effects

Notice we have a new gamma term, $\gamma_{01}$. How do we interpret this new fixed effect, especially in the presence of other fixed effects?

$\gamma_{00}$ is the intercept and can be considered the value when G = 0 and time = 0, whereas the $\gamma_{01}$ is the difference in initial values between groups.

The value for group = 1? $\gamma_{00} + \gamma_{01}$

------------------------------------------------------------------------

Combined: $${Y}_{ti} = \gamma_{00} + \gamma_{01}G_{i} + \gamma_{10} (Time_{ti})+ U_{0i}  + U_{1i}(Time_{ti}) + \varepsilon_{ti}$$ $${Y}_{ti} = (\gamma_{00} + \gamma_{01}G_{i} + U_{0i} ) + [(\gamma_{10}+ U_{1i})(Time_{ti})] + \varepsilon_{ti}$$

## Interpretation of random effects

-   One thing to keep in mind is that we are now changing the meaning of the random effect. Now that we have a predictor in the model, the $U_{0j}$ is the person specific deviation from the group predicted intercept, not the grand mean intercept.

Level 1:\
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2:

$${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + U_{1i}$$

## Does your covariance structure change?

Level 2 covariance matrix $$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     & \tau_{00}^{2} & \tau_{01}\\ 
  0, & \tau_{01} & \tau_{10}^{2}
\end{pmatrix}$$

Level 1 residual variance will also be different

$$ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  $$

## Equations for each group

$${Y}_{ti} = (\gamma_{00} + \gamma_{01}G_{i} + U_{0i} ) + [(\gamma_{10}+ U_{1i})(Time_{ti})] + \varepsilon_{ti}$$

Understanding how to re-write the equation will help for calculating estimated scores for your predictors in addition to being able to interpret the coefficients.

Estimated value for an individual in group = 0\
$$\hat{Y}_{ti} = (\gamma_{00} + U_{0i} ) + [(\gamma_{10}+ U_{1i})(Time_{ti})]$$ group = 1 individual $$\hat{Y}_{ti} = (\gamma_{00} + \gamma_{01}+ U_{0i} ) + [(\gamma_{10}+ U_{1i})(Time_{ti})]$$

------------------------------------------------------------------------

We can also create group mean estimated trajectories

$${Y}_{ti} = (\gamma_{00} + \gamma_{01}G_{i} + U_{0i} ) + [(\gamma_{10}+ U_{1i})(Time_{ti})] + \varepsilon_{ti}$$

group = 0 trajectory $$\hat{Y}_{ti} = (\gamma_{00}) + (\gamma_{10})(Time_{ti})$$

group = 1 trajectory $$\hat{Y}_{ti} = (\gamma_{00}+ \gamma_{01}) + (\gamma_{10})(Time_{ti})$$

## Slope and Intercept Group Predictors

Predicting the intercept only can only answer static questions, not about change.

Level 1:\
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2:

$${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} +  \gamma_{11}G_{i} +U_{1i}$$

Similar to before, the interpretation of $U_{1i}$ changes. The term is now what is left over after accounting for group differences in the mean slope.

## cross level interactions

Level 1:\
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2:

$${\beta}_{0i} = \gamma_{00} + \gamma_{01}G_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} +  \gamma_{11}G_{i} +U_{1i}$$

Combined: $${Y}_{ti} = \gamma_{00} + \gamma_{01}G_{i}+  \gamma_{10} (Time_{ti}) + \gamma_{11}(G_{i}*Time_{ti}) + $$ $$ U_{0i} + U_{1i}(Time_{ti}) + \varepsilon_{ti}$$

------------------------------------------------------------------------

Notice that when we combine Level 1 and Level 2, the slope effect predictor becomes an interaction with time. Anytime you have a predictor of time that will be an interaction with time in that we are asking does group status (or what ever variable) differs in their relationship between time and your DV.

## interpretation of lower order terms

Combined: $${Y}_{ti} = \gamma_{00} + \gamma_{01}G_{i}+  \gamma_{10} (Time_{ti}) + \gamma_{11}(G_{i}*Time_{ti}) + $$ $$ U_{0i} + U_{1i}(Time_{ti}) + \varepsilon_{ti}$$

As with regression, the lower order terms are now conditional on the higher order interaction.

$\gamma_{10}$, the fixed effect representing our slope is now the simple slope for group = 0

$\gamma_{01}$, the fixed effect for group is the difference between groups when time = 0, e.g. the intercept

$\gamma_{00}$ is the intercept, it is the value for when our predictors are zero.

## residual structure

Level 2 covariance matrix

$$\begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix} \sim\mathcal{N} \begin{pmatrix} 0, \tau_{00}^{2} & \tau_{01}\\  0,  \tau_{01} & \tau_{10}^{2} \end{pmatrix}$$

How does your variance-covariance matrix change?

Level 1 residual variance $${R}_{ij} \sim \mathcal{N}(0, \sigma^{2})$$

How does your residual change relative to a model without group effects? Can you graph conceptually what this now captures?

## predictive equation

Thinking about the equation as a predictive engine will help us later with graphing

Alternative combined $${Y}_{ti} = [\gamma_{00} + U_{0i} +\gamma_{01}G_{i}] + [(\gamma_{10}  + \gamma_{11}G_{i}+  U_{1i})(Time_{ti})] + \varepsilon_{ti}$$

This is just rearranged so you can see that different groups have different intercepts and slopes

$$\hat{Y}_{ti} = [\gamma_{00} +\gamma_{01}G_{i}] + [(\gamma_{10}  + \gamma_{11}G_{i})(Time_{ti})]$$

Notice how when G = 0, the equation simplifies:

$$\hat{Y}_{ti} = \gamma_{00} + \gamma_{10} (Time_{ti})$$

## Plotting

```{r}
#| code-fold: true
library(tidyverse)
alcohol <- read.csv("https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/alcohol1_pp.csv")
head(alcohol)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
library(lme4)
fit1 <- lmer(alcuse ~  age_14 + (age_14 | id), data = alcohol)
summary(fit1)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit2 <- lmer(alcuse ~  age_14 + male +  (age_14 | id), data = alcohol)
summary(fit2)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit3 <- lmer(alcuse ~  age_14 * male + (age_14 | id), data = alcohol)
summary(fit3)
```

--------

```{r}
#| code-fold: true
library(easystats)
model_parameters(fit3)
```







------------------------------------------------------------------------

```{r}
#| code-fold: true
library(modelr)
alcohol %>% 
  data_grid(age_14 = seq_range(age_14, n = 5), male, .model = alcohol)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
alcohol %>% 
  data_grid(age_14 = seq_range(age_14, n = 5), male, .model = alcohol) %>% 
    add_predictions(fit3) 
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
alcohol %>% 
  data_grid(age_14 = seq_range(age_14, n = 5), male, .model = alcohol) %>% 
    add_predictions(fit3) %>% 
   ggplot(aes(y = pred, x = age_14, group = male)) +
  geom_line(alpha = .2) 
```

## L2 Continuous predictors

A continuous L2 predictor is similar to the group predictors.

Level 1:\
$${Y}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2:

$${\beta}_{0i} = \gamma_{00} + \gamma_{01}C_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} +  \gamma_{11}C_{i} +U_{1i}$$

Combined: $${Y}_{ti} = \gamma_{00} + \gamma_{01}C_{i}+  \gamma_{10} (Time_{ti}) + \gamma_{11}(C_{i}*Time_{ti}) +  $$ $$ U_{0i} + U_{1j}(Time_{ti}) + \varepsilon_{ti}$$

## Continuous conditional interpretation

$${Y}_{ti} = \gamma_{00} + \gamma_{01}C_{i}+  \gamma_{10} (Time_{ti}) + \gamma_{11}(C_{i}*Time_{ti}) +  $$ $$ U_{0i} + U_{1j}(Time_{ti}) + \varepsilon_{ti}$$

The $\gamma_{11}$ interaction coefficient indexes the difference in slopes at different levels of C

The $\gamma_{10}$ is now the effect of time when the continuous predictor is zero.

The $\gamma_{01}$ is now the effect of C when time is zero.

$\gamma_{00}$ is the intercept, it is the value for when our predictors are zero.

$U_{0i}$ Is the random effect for intercept after accounting for C.

$U_{1i}$ Is the random effect for the slope after accounting for C.

## Equations for plotting

The same logic for plotting continuous variables except this time we have to choose the values for simple slopes

$$\hat{Y}_{ti} = [\gamma_{00} + \gamma_{01}C_{i}]+  [\gamma_{10} + \gamma_{11}C_{i}]*Time_{ti}$$

Assuming C is standardized: -1sd $$\hat{Y}_{ti} = [\gamma_{00} +(\gamma_{01}*-1)] + [\gamma_{10}  + (\gamma_{11}*-1)]*Time_{ti}$$

Mean $$\hat{Y}_{ti} = \gamma_{00} + \gamma_{10} * (Time_{ti})$$

+1sd $$\hat{Y}_{ti} = [\gamma_{00} +\gamma_{01}] + [\gamma_{10}  + \gamma_{11}]*Time_{ti}$$

## individual level trajectories

What do individual level trajectories look like?

$$\hat{Y}_{ti} = [\gamma_{00} + \gamma_{01}C_{i}+  U_{0i}] + $$ $$ [\gamma_{10}  + (\gamma_{11}*C_{i}) + U_{1i}]  * Time_{ti}$$

If you know someones random effects, and you know the fixed effects, you can create predicted values for any level of time.

## Example

```{r}
#| code-fold: true
fit4 <- lmer(alcuse ~  age_14 + peer + (age_14 | id), data = alcohol)
summary(fit4)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit5 <- lmer(alcuse ~  age_14 + cpeer + (age_14 | id), data = alcohol)
summary(fit5)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
fit6 <- lmer(alcuse ~  age_14 * cpeer + (age_14 | id), data = alcohol)
summary(fit6)
```



-------------

```{r}
#| code-fold: true
model_parameters(fit6)
```




## Plotting

```{r}
library(report)
alcohol %>% 
   report_sample()
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
alcohol %>% 
  data_grid(age_14 = seq_range(age_14, n = 5), cpeer = c(-.73,0,.73), .model = alcohol) %>% 
    add_predictions(fit6) 
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
alcohol %>% 
  data_grid(age_14 = seq_range(age_14, n = 5), cpeer = c(-.73,0,.73), .model = alcohol) %>% 
    add_predictions(fit6) %>% 
   ggplot(aes(y = pred, x = age_14, group = cpeer)) +
  geom_line(alpha = .2) 
```

## Multiple predictors

Interpretations with multiple predictors in regression extend to MLMs. For example, health across time, examining the effects of an intervention, while controlling for initial exercise status?:

level 1: $${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2: $${\beta}_{0i} = \gamma_{00} + \gamma_{01}Exercise_{i} +  \gamma_{02}Intervention_{i} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} + \gamma_{11}Intervention_{i} + U_{1i}$$


## SEM

```{r}
alcohol.wide <- alcohol %>% 
  dplyr::select(-X, -age_14, -ccoa) %>% 
  pivot_wider(names_from = "age", 
              names_prefix = "alcuse_",
              values_from  = alcuse) 
```



## SEM covariates/predictors

The strategy for including predictors is similar to working with a level 2 mlm equation. Do you want to predict the intercept or the slope? No need to explicitly model an interaction as that is implied. 

```{r}
#| code-fold: true
library(lavaan)
sem.1 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
               
# regressions
    
    i ~ male 
    s ~ male
'
fit.1 <- growth(sem.1, data = alcohol.wide)
summary(fit.1)
```


--------

```{r}
#| code-fold: true
library(lavaan)
sem.2 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
               
# regressions
    
    i ~ peer 
    s ~ peer
'
fit.2 <- growth(sem.2, data = alcohol.wide)
summary(fit.2)
```



## centered predictors

```{r}
#| code-fold: true

sem.3 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
               
# regressions
    
    i ~ cpeer
    s ~  cpeer
'
fit.3 <- growth(sem.3, data = alcohol.wide)
summary(fit.3)
```


## Multiple predictors


```{r}
#| code-fold: true

sem.4 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
               
# regressions
    
    i ~ cpeer + male
    s ~ cpeer + male
'
fit.4 <- growth(sem.4, data = alcohol.wide)
summary(fit.4)
```



## Centering mlm redux

Changing the scale of your predictors changes the interpretation of your model.

1.  Original metric (no centering)

2.  Group-mean centering (our group/nesting is person so this is also called person centering). This will be more appropriate when we talk about level 1 predictors.

3.  Group grand-mean centering (centering around person avg)

4.  Grand-mean centering (this is taking the average across every obs)

5.  Centering on a value of theoretical or applied interest

## Time considerations

-   We typically center time around each person's initial time to make the intercept more interpretable. However, this can cause correlations between an intercept and a slope.

-   If high, the correlation can be problematic in terms of estimation. Often we center time in the middle of the repeated assessments to minimize this association.

-   Doing so is especially important if you want to use some variable to predict intercept and slope (or use intercept/slope to predict some variable).

------------------------------------------------------------------------

-   What if people don't have the same number of assessment waves or the same timespan? Where do you center? One option, is to center within each person's own time, regardless of whether it lines up with others. This is nice because it makes the $\gamma_{00}$ interpretable as the average score across people.

-   However, what is the average score? If you are looking at longitudinal data where people span in age from 20 to 80 and the time each person was in the study differed from 1 to 10 years. How do you interpret the average person intercept?

-   Thus you may want to center on something constant across people, like age. The $\gamma_{00}$ can now easily be interpreted as age 40, for example. Buuut, this results in wonky residual terms, perhaps leading to greater covariance between intercept and slope.

## Level 2 centering

-   Because level 2 is involved with cross-level interactions, it is always helpful to at least consider centering. For level 2, the centering options are much easier, as one can generally go with grand mean centering.

-   As everyone has only 1 value to contribute to, the calculation and the interpretation is more straightforward. It is thus also equivalent to group grand-mean centering.

------------------------------------------------------------------------

What are the interpretations for the coefficients for exercise centered vs non-centered?

level 1: $${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \varepsilon_{ti}$$

Level 2: $${\beta}_{0i} = \gamma_{00} + \gamma_{01}Exercise_{i}  +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} +  \gamma_{11}Exercise_{i} +  U_{1i}$$

Combined: $$\hat{Health}_{ti} = [\gamma_{00} + \gamma_{01}Exercise_{i}+  U_{0i}] + $$ $$ [\gamma_{10}  + \gamma_{11}Exercise_{i} + U_{1i}]  * Time_{ti}$$

## Level 1 predictors

-   These are predictors that repeat.

-   Some variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income).

-   These variables can be treated as another predictor with the effect of "controlling" for some level 1 variable as well as a focal variable.

------------------------------------------------------------------------

Consider health across time (1 = yes, exercised).

level 1: $${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}Exercise_{ti} + \varepsilon_{ti}$$

Level 2: $${\beta}_{0i} = \gamma_{00} +   U_{0i}$$

$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$

$${\beta}_{2i} = \gamma_{20}$$

Combined:

$${Health}_{ti} =  [\gamma_{00} +   \gamma_{10}Time_{ti}   + \gamma_{20}Exercise_{ti}] + $$\
$$ [ U_{0i}  + U_{1i}Time_{ti}+ \varepsilon_{ti}]$$

# Interpretation 


Tvs can be treated as another predictor with the effect of "controlling" for some TVC. Thus the regression coefficients in the model are conditional on this covariate. 

$\gamma_{10}$ is the average rate of change in health, controlling for exercise at each time point

$\gamma_{20}$ is the average difference in health when exercising vs when not, across each time (so controlling for) 

$\gamma_{00}$ is the average health at Time = 0 for those that are not exercising. Ie when both predictors are at zero.  

---

How would you visualize the fixed effects for varying combinations of exercise? 


## What if time is not focal?

Usually time is not an important factor for studies that last for weeks or less. Instead we often we want to look at repeated associations or correlations across time. What happens to the time variable? Two options: 

1. Ignore it. A time variable is asking whether the variable we care about is "associated" with time. Maybe that isn't important to us, theoretically. 

2. Use it to control to increases in your DV across time, but mostly disregard. Time could serve to control for the messiness of daily life, even if it is not a focal variable. 



## Introducing a random slope for a TVC

Person specific residuals make the interpretation of parameters a little more difficult as the model says that the gap between exercise and not exercise is the same for everyone. Should we allow it to be this way? 

level 1: 
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}Exercise_{ti} + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} +   U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$  

$${\beta}_{2i} = \gamma_{20} +  U_{2i}$$  



------------

Level 2 variance-covariance matrix: 

$$\begin{pmatrix} {U}_{0i} \\ {U}_{1i} \\ {U}_{2i} \end{pmatrix} \sim \mathcal{N} \begin{pmatrix}  0,  &  \tau_{0}^{2} & \tau_{01}   & \tau_{02}  \\  0, & \tau_{10} & \tau_{1}^{2} & \tau_{12}  \\ 0, & \tau_{20} & \tau_{21} & \tau_{2}^{2} \end{pmatrix}$$



Residual variance at level 1

$${R}_{ti} \sim \mathcal{N}(0, \sigma^{2})$$


## Where does the variance go? 

Compared to time invariant (level 2) predictors, tvc/level 1 predictors are likely to explain variance level 1 and level 2 variance terms. This is because level 1 predictors can include both level 1 and level 2 information. 

Typically level 2 predictors tend to only reduce level 2 variance. It is possible, however, that including a level 1 predictor will increase the variance in level 2 variance components.


## Interactions among level 1 variables

Couldn't exercise levels influence the slope of health? The previous models constrained the slopes to be the same, saying that people differ on level when exercising vs not but not on rate of change. 

$${Health}_{ti} =  [\gamma_{00} +   \gamma_{10}Time_{ti}   + \gamma_{20}Exercise_{ti} + \gamma_{30}TimeXExercise_{ti}]$$ 
$$+ [ U_{0i}  + U_{1i}Time_{ti}+ \varepsilon_{ti}]$$

- How could you visualize this model? 

- How do you interpret each of the terms (knowing what you know about interactions)?

- How would all of this change if our level 1 variable was continuous? 



## Centering redux

- If we just leave the exercise variable by itself there is often a combination of within and between person information. The two sources are "smushed" together. 

- I could exercise more than the average person (between person) but my research question is about what happens when *I* exercise a lot, does that correspond to health. Because my excercise level varies there is within person information. 

- It is not clear whether exercise is associated with health BECAUSE people who exercise tend to be healthier (a between person question that could be addressed with cross sectional data) or is it because when someone exercises they feel healthier. 

- How is $\gamma_{20}Exercise_{ti}$ interpreted? 



## person mean centering

- Typically for level 1 we will want to within person-mean center to help with the issue of "smushed" variance. Especially when you are working with level 1 interactions, centering is important to interpret your lower order terms. 

- However, this gets rid of all mean level information for a person. The question at hand is not whether you exercise more or less it is compared to your typical levels, what happens when you exercise more or less. 

- If you are including a level 1 person centered variable in the model, note that 1) the average level of exercise is not controlled for and 2) the variation around the level will likely be related to the persons mean score. In other words, the within and between person variance of exercise is not neatly decomposed.

## Level 1 intensive vs long term

- The question of whether to center or not at level 1 partially depends on the type of data we are analyzing. 

- For intensive data, we will want to center for two related reasons: 1. makes the coefficient about within person deviations and 2. helps us separate within and between person effects 

- For longer term data, level 1 is often used as a covariate rather than a focal coefficient. As a result, centering becomes less important. 

## Seperating between and within person effects

We will create a new variable out of the existing level 1 variable, a person mean. This makes the effects of the person level uncorrelated with the effects of the between person level


$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{ti}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$  

$${\beta}_{2i} = \gamma_{20}$$

--------

What would we be testing if we included included person mean exercise to the person level effect? 

$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{ti}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$


$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$ 

$${\beta}_{2i} = \gamma_{20}+ \gamma_{21}\overline{Exercise_{i}}$$


## Multiple level 1 predictors

$${Health}_{ti} =  [\gamma_{00} +   \gamma_{10}Time_{ti}   + \gamma_{20}(Exercise_{ti}-\overline{Exercise_{i}}) + \gamma_{30}Mood_{ti}$$
$$+\gamma_{40}(Mood*(Exercise_{ti}-\overline{Exercise_{i}}))_{ti} ] + [ U_{0i}  + U_{1i}Time_{ti}+ \varepsilon_{ti}]$$
 
How would $\gamma_{20}$ and $\gamma_{40}$ change in interpretation if exercise was not person centered?

-------

Level 1: 
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{ti}-\overline{Exercise_{i}}) +$$
$$\beta_{3i}Mood_{ti}+\beta_{4i}Mood_{ti}*(Exercise_{ti}-\overline{Exercise_{i}})  +\varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$  

$${\beta}_{2i} = \gamma_{20}$$
$${\beta}_{3i} = \gamma_{30}$$
$${\beta}_{4i} = \gamma_{40}$$

-----

- For longer term data, where the focus is on a trajectory, and the fators that impact it, controlling for TVC like exercise often is done level 1 and not also at level 2. 

- The level 1 has both between and within person variance, and given you are not trying to seperate the influence, you end up controlling for both when putting in level 1 TVC. 


## What happens if we want to add a level 2 predictor? 
 
$${Health}_{ti} =  [\gamma_{00} + \gamma_{10}Time_{ti} + \gamma_{20}(Exercise_{ti}-\overline{Exercise_{i}}) +\gamma_{30}Mood_{ti}+\gamma_{31}(Mood*\overline{Exercise_{i}})_{ti}$$ 
 
 $$+\gamma_{40}(Mood*(Exercise_{ti}-\overline{Exercise_{i}}))_{ti} ] + [ U_{0i}  + U_{1i}Time_{ti}+ \varepsilon_{ti}]$$
 
 
 ---

Level 1: 
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{ti}-\overline{Exercise_{i}}) + \beta_{3i}Mood_{ti} +\varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$  

$${\beta}_{2i} = \gamma_{20}$$
$${\beta}_{3i} = \gamma_{30} + \gamma_{31}  (\overline{Exercise_{i}})$$





## Lagged models
  
What if I want to predict something in the future? 

Level 1
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{(t-1)i}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$ 


$${\beta}_{1i} = \gamma_{10} + \gamma_{11}\overline{Exercise_{i}} + U_{1i}$$  

$${\beta}_{2i} = \gamma_{20}$$ 


--------------

How does this model differ from the prior model? What does the $\gamma_{21}$ test? Would you want to include this term or not? 

Level 1
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{(t-1)i}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}\overline{Exercise_{i}} + U_{0i}$$ 


$${\beta}_{1i} = \gamma_{10} + \gamma_{11}\overline{Exercise_{i}} + U_{1i}$$  


$${\beta}_{2i} = \gamma_{20} + \gamma_{21}\overline{Exercise_{i}}$$


## Contextual models

Grand mean centering our level 1 variables is another option. We have already seen that when we use time as level 1 predictor. The between and within person variance is now "smushed" together when we do not person center. 

Level 1:
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}Time_{ti} + \beta_{2i}(Exercise_{ti}-\overline{Exercise_{..}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}(\overline{Exercise_{i}}-\overline{Exercise_{..}})+ U_{0i}$$  


$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$  

$${\beta}_{2i} = \gamma_{20}$$


-------

When $\gamma_{01}\overline{Exercise_{i}}$ is included at level 2 when we have person-centered level 1 the effects separate the between and within. But level 2 controls for between meaning that $\beta_{2i}(Exercise_{ti}-\overline{Exercise_{..}})$ is now the within person effect, same as if we within person centered! 

--------

How to interpret  $\gamma_{01}\overline{Exercise_{i}}$ ?

The extent that daily exercise is higher for a given person than for the rest of the sample, we can expect that person’s mean exercise to be higher than the rest of the sample as well. Thus, part of the between-person effect can be captured just by including time-varying, level 1 exercise. As a result, $\gamma_{01}\overline{Exercise_{i}}$ represents the difference needed to get from the part of the between person effect carried by just level 1 to the total between-person effect that belongs in the level-2 model

After controlling for the absolute amount of timevarying exercise, the incremental (unique) effect of average exercise is given by its level-2 contextual effect. For every unit higher person mean exercise, mean daily exercise are expected to be higher by the level-2 contextual effect. 

In summary, similar in interpretation to the within and between separation. The contextual effect = between effect minus within person effect
 
 
-------
centering and random effects

Level 1
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}(Exercise_{ti}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$
$${Health}_{ti} = \beta_{0i} +  \beta_{1i}(Exercise_{ti}-\overline{Exercise_{..}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}(\overline{Exercise_{i}}-\overline{Exercise_{..}}) + U_{0i}$$ 
$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$
Interpretation: 
$$U_{1i}$$ 

For PMC, the effect is relative association compared to typical person centered effect -- maybe some people have an effect where deviations of exercise influences health but others these deviations do not.
For GMC, the effect contains both within and between person effects, so the interpretation is messier

-----------

Level 1
$${Health}_{ti} = \beta_{0i}  + \beta_{1i}(Exercise_{ti}-\overline{Exercise_{i}}) + \varepsilon_{ti}$$
$${Health}_{ti} = \beta_{0i} +  \beta_{1i}(Exercise_{ti}-\overline{Exercise_{..}}) + \varepsilon_{ti}$$

Level 2: 
$${\beta}_{0i} = \gamma_{00} + \gamma_{01}(\overline{Exercise_{i}}-\overline{Exercise_{..}}) + U_{0i}$$ 
$${\beta}_{1i} = \gamma_{10} +  U_{1i}$$
Interpretation: 

$$U_{0i}$$ 
For PMC, the effect represents intercept differences when people are at their person mean. Are they healthier than the avg person, at their avg level of exercise?
For GMC, the effect when people are at the grand mean. Are they healthier than the at the average level of exercise?


## categorical TVC

Depending on your goal it might be confusing/difficult to person mean center. The issue is that centering in this manner leads to values that do not exist e.g. -.5 when your TVC = 0, for example. This also leads to potentially strange/uninterpretable intercepts or other lower order terms. 

Suggested to keep raw tvc at level 1 but GMC at level 2.

Sometimes time variables can be like this. For a recent paper we coded a life event as a tvc, which effectively switched on and of when the person encountered a life event. 


|ID | wave           | event  |
| ------------- |:-------------:| -----:|
| 1    | 0 | 0 |
| 1     |1      |   1 |
| 1 | 2     |    1|
| 2 | 0     |    0 |
| 2 | 1      |    1|


## SEM TVCs

Level 1 variables (tvs) are flexibly handled in SEM. Whereas previously a level 1 variable had a constant effect (ie you just had a single parameter) here you can model separate effects for different times. Or you can have a tvc for one timepoint but not another. Doing this in an MLM framework is awkward at best.  


```{r, echo = FALSE}
#| code-fold: true
  tvc1 <- rnorm(n = 82, 0,1)
  tvc2 <- rnorm(n = 82, 0,1)
  tvc3 <- rnorm(n = 82, 0,1)
  
temp1 <- cbind(alcohol.wide,tvc1 )  
temp2 <- cbind(temp1,tvc2 )
alcohol.wide <- cbind(temp2,tvc3 )
```

----------

```{r}
#| code-fold: true

model.5 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male + cpeer
    s ~ male + cpeer
    
# time-varying covariates

    alcuse_14 ~ tvc1
    alcuse_15 ~ tvc2
    alcuse_16 ~ tvc3

'
fit.5 <- growth(model.5, data = alcohol.wide)

summary(fit.5)
```

Can we fix the TVCs to be the same? 



## MELSM

Mixed effects location scale model 
location = mean
scale = variance (usually)

Basically it means modeling multiple distributional parameters simultaneously. 

```{r}
#| echo: FALSE
melsm <- read.csv("~/Library/CloudStorage/Box-Box/Bayesian Statistics/bayes22/static/Lectures/melsm.csv")

melsm <- melsm %>% 
  mutate(day01 = (day - 2) / max((day - 2)))
```


```{r}
#| code-fold: true
melsm %>% 
distinct(record_id) %>% 
  count()
```


---

Participants filled out daily affective measures and physical activity

```{r}
#| code-fold: true
melsm %>% 
    count(record_id) %>% 
  ggplot(aes(x = n)) +
  geom_bar() +
  scale_x_continuous("number of days", limits = c(0, NA))
```




## Participant level

```{r}
#| code-fold: true
melsm %>% 
  nest(data = !record_id) %>% 
  slice_sample(n = 16) %>% 
  unnest(data) %>% 
  ggplot(aes(x = day, y = N_A.lag)) +
  geom_line(color = "black") +
  geom_point(color = "black", size = 1/2) +
  ylab("negative affect (standardized)") +
  facet_wrap(~record_id)
```




## Standard MLM treatment

```{r}
#| code-fold: true
library(brms)
melsm.1 <-
  brm(family = gaussian,
      N_A.std ~ 1 + day01 + (1 + day01 | record_id),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(exponential(1), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.1")
```


---

```{r}
#| code-fold: true
summary(melsm.1)
```


----


```{r}
#| code-fold: true
m1 <- lmer(N_A.std ~ 1 + day01 + (1 + day01 | record_id), data = melsm)
summary(m1)
```


---

If we do not care about random effects, specify re_formula = NA

```{r}
#| code-fold: true
library(tidybayes)
fixed.slope<- melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20)) %>% 
  add_epred_draws(melsm.1, re_formula = NA )
  fixed.slope
```


---

If we want random effects we need to specify the variable to get ALL levels. 

```{r}
#| code-fold: true
melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), record_id) 
```
193 people * 20 = 3860

---

Then we need to state re_forumla = NULL (which is the default)

```{r}
#| code-fold: true
melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), record_id) %>% 
  add_epred_draws(melsm.1, re_formula = NULL )
```

---

re_formula formula helps us focus on either the average trajectory (gammas) or the group specific (person Us). If NULL (default), include all group-level effects; if NA, include no group-level effects.

 160,000 = 20 (different values of X we chose) x 8000 iterations (2k x 4 chains)
 30,880,000 = 20* 8000 iterations * 193 people in our dataset

---

```{r}
#| code-fold: true
 melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), record_id) %>% 
  add_epred_draws(melsm.1, re_formula = NA) %>% 
  ggplot()+
  aes(x = day01, y = .epred) +
  stat_lineribbon(.width = 0.95, alpha = .5)
```

---

```{r}
#| code-fold: true
## https://www.tjmahr.com/sample-n-groups/
sample_n_of <- function(data, size, ...) {
  dots <- quos(...)
  group_ids <- data %>% 
    group_by(!!! dots) %>% 
    group_indices()
  sampled_groups <- sample(unique(group_ids), size)
  data %>% 
    filter(group_ids %in% sampled_groups)
}

sample_n_of <- function(data, size, ...) {
  dots <- quos(...)
  
  group_ids <- data %>% 
    group_by(!!! dots) %>% 
    group_indices()
  
  sampled_groups <- sample(unique(group_ids), size)
  
  data %>% 
    filter(group_ids %in% sampled_groups)
}


melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), record_id) %>% 
    sample_n_of(5,record_id) %>% 
  add_epred_draws(melsm.1, re_formula = NULL) %>% 
  ggplot(aes(x = day01)) +
  stat_lineribbon(aes(y = .epred, group = record_id))



```



## Sigma is unmodeled variation

```{r}
#| code-fold: true
newd <-
  melsm %>% 
  filter(record_id %in% c(30, 115)) %>% 
  select(record_id, N_A.std, day01)

fits <- newd %>%
  add_epred_draws(melsm.1)

preds <- newd %>%
  add_predicted_draws(melsm.1)

fits %>% 
ggplot(aes(x = day01, y = N_A.std)) +
  stat_lineribbon(aes(y = .epred),.width = c(.95), alpha = 1/4, color ="grey") +
  stat_lineribbon(data = preds, aes(y = .prediction),.width = c(.90), alpha = 1/4, color ="blue") +
  geom_point(data = newd) +
  facet_wrap(~record_id)
```




---

$$NA_{ij} \sim \operatorname{Normal}(\mu_{ij}, \sigma_{i})$$

$$\mu_{ij}  = \beta_0 + \beta_1 time_{ij} + u_{0i} + u_{1i} time_{ij}$$
$$\log(\sigma_i )  = \eta_0 + u_{2i}$$

$$\begin{bmatrix} u_{0i} \\ u_{1i} \\ {u_{2i}} \end{bmatrix}  \sim \operatorname{MVNormal}\begin{pmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, \mathbf S \mathbf R \mathbf S \end{pmatrix}$$
$$\mathbf S  = \begin{bmatrix} \sigma_0 & 0 & 0 \\ 0 & \sigma_1 & 0 \\ 0 & 0 & \sigma_2 \end{bmatrix}$$
$$\mathbf R = \begin{bmatrix} 1 & \rho_{12} & \rho_{13} \\ \rho_{21} & 1 & \rho_{23} \\ \rho_{31} & \rho_{32} & 1 \end{bmatrix}$$
$$\beta_0  \sim \operatorname{Normal}(0, 0.2)$$
$$\beta_1 \text{and } \eta_0  \sim \operatorname{Normal}(0, 1) $$
$$ \sigma_0,\dots, \sigma_2 \sim \operatorname{Exponential}(1) $$
$$\mathbf R  \sim \operatorname{LKJ}(2)$$

---

note: 1) brms default is to use log-link when modeling sigma
2) |i| syntax within the parentheses allow for correlated random effects. Without this, the random intercept and slope would not be correlated with the random sigma term, effectively setting the correlation equal to zero 

```{r}
#| code-fold: true
melsm.2 <-
  brm(family = gaussian,
      bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),
         sigma ~ 1 + (1 |i| record_id)),
                prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(normal(0, 1), class = Intercept, dpar = sigma),
                prior(exponential(1), class = sd, dpar = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.2")
```

---
 

```{r}
#| code-fold: true
summary(melsm.2)
```


---

```{r}
#| code-fold: true
melsm.2 %>% 
  spread_draws(b_sigma_Intercept) %>% 
  exp() %>% 
  median_qi()
```

```{r}
#| code-fold: true
melsm.2 %>% 
  spread_draws(b_day01) %>% 
  median_qi()
```


---

```{r}
#| code-fold: true
melsm.2 %>% 
spread_draws(b_sigma_Intercept,r_record_id__sigma[ID, term]) 
```

8000 samples * 193 participants = 1544000 

---

```{r}
#| code-fold: true
melsm.2 %>% 
spread_draws(b_sigma_Intercept,r_record_id__sigma[ID, term]) %>% 
  mutate(b_sigma_Intercept = exp(b_sigma_Intercept)) %>% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) %>% 
   median_qi(estimate = b_sigma_Intercept + r_record_id__sigma) %>% 
  ggplot(aes(x = reorder(ID, estimate), y = estimate, ymin = .lower, ymax = .upper)) +
   geom_pointinterval(point_colour = "black", interval_color = "grey", point_alpha = .25) + scale_x_discrete("Participants ranked by posterior SD", breaks = NULL) + ylab("sigma estimate") + theme_light()
```


---

```{r}
#| code-fold: true
fits2 <- newd %>%
  add_epred_draws(melsm.2)

preds2 <- newd %>%
  add_predicted_draws(melsm.2)

fits2 %>% 
ggplot(aes(x = day01, y = N_A.std)) +
  stat_lineribbon(aes(y = .epred),.width = c(.95), alpha = 1/4, color ="grey") +
  stat_lineribbon(data = preds2, aes(y = .prediction),.width = c(.90), alpha = 1/4, color ="blue") +
  geom_point(data = newd) +
  facet_wrap(~record_id)
```


## time as a predictor of sigma

```{r}
melsm.3 <-
  brm(family = gaussian,
      bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),
         sigma ~ 1 + day01 + (1 + day01 |i| record_id)),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sd),
                prior(normal(0, 1), class = Intercept, dpar = sigma),
                prior(normal(0, 1), class = b, dpar = sigma),
                prior(exponential(1), class = sd, dpar = sigma),
                prior(lkj(2), class = cor)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      data = melsm,
      file = "melsm.3")
```


---


```{r}
#| code-fold: true
summary(melsm.3)
```


---

```{r}
#| code-fold: true
melsm.3 %>% 
  gather_draws(b_sigma_Intercept, b_sigma_day01) %>% 
  median_qi()
```

---

```{r}
#| code-fold: true
melsm.3 %>% 
  spread_draws(b_sigma_Intercept) %>% 
  mutate(sigma_day0 = exp(b_sigma_Intercept)) %>% 
  select(sigma_day0) %>% 
  median_qi()
```

```{r}
#| code-fold: true
melsm.3 %>% 
  spread_draws(b_sigma_Intercept, b_sigma_day01) %>% 
  mutate(sigma_day1 = exp(b_sigma_Intercept + b_sigma_day01)) %>% 
  select(sigma_day1) %>% 
  median_qi()
```

---

```{r}
#| code-fold: true
 melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), .model = melsm) %>% 
  add_epred_draws(melsm.3, re_formula = NA, dpar = T) %>% 
  ungroup() %>% 
  select(day01, .epred, sigma)
```
20 * 8000 = 160000

---

```{r}
#| code-fold: true
melsm %>% 
  data_grid(day01 = seq_range(day01, n = 20), .model = melsm) %>% 
  add_epred_draws(melsm.3, re_formula = NA, dpar = T) %>% 
  ungroup() %>% 
  select(day01, .epred, sigma) %>% 
  ggplot()+
  aes(x = day01, y = sigma) +
  stat_lineribbon(.width = 0.95, alpha = .5)
```



---

Lets look at random effects. Remember we fit a random effect for both the intercept and the slope (time)

```{r}
#| code-fold: true
melsm.3 %>% 
spread_draws(r_record_id__sigma[ID, term]) %>% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) 
```

193 * 8000 *2

---

```{r}
#| code-fold: true
melsm.3 %>% 
spread_draws(r_record_id__sigma[ID, term]) %>% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) %>% 
   median_qi(estimate = r_record_id__sigma) 
```

---


```{r}
#| code-fold: true
melsm.3 %>% 
spread_draws(r_record_id__sigma[ID, term]) %>% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) %>% 
   median_qi(estimate = r_record_id__sigma) %>% 
  filter(term == "Intercept") %>% 
  ggplot(aes(x = reorder(ID, estimate), y = estimate, ymin = .lower, ymax = .upper)) +
   geom_pointinterval(point_colour = "black", interval_color = "grey", point_alpha = .25) + scale_x_discrete("Participants ranked by posterior SD", breaks = NULL) + ylab("sigma estimate") + theme_light()
```

---


```{r}
#| code-fold: true
melsm.3 %>% 
spread_draws(r_record_id__sigma[ID, term]) %>% 
  mutate(r_record_id__sigma = exp(r_record_id__sigma)) %>% 
   median_qi(estimate = r_record_id__sigma) %>% 
  filter(term == "day01") %>% 
  ggplot(aes(x = reorder(ID, estimate), y = estimate, ymin = .lower, ymax = .upper)) +
   geom_pointinterval(point_colour = "black", interval_color = "grey", point_alpha = .25) + scale_x_discrete("Participants ranked by posterior SD", breaks = NULL) + ylab("sigma estimate") + theme_light()
```


---

When on the log scale you can more easily see some people are less than average where some are more than average in terms of sigma
```{r}
#| code-fold: true
melsm.3 %>% 
spread_draws(r_record_id__sigma[ID, term]) %>% 
   median_qi(estimate = r_record_id__sigma) %>% 
  filter(term == "day01") %>% 
  ggplot(aes(x = reorder(ID, estimate), y = estimate, ymin = .lower, ymax = .upper)) +
   geom_pointinterval(point_colour = "black", interval_color = "grey", point_alpha = .25) + scale_x_discrete("Participants ranked by posterior SD", breaks = NULL) + ylab("sigma estimate") + theme_light()
```


---

```{r}
#| code-fold: true
fits3 <- newd %>%
  add_epred_draws(melsm.3)

preds3 <- newd %>%
  add_predicted_draws(melsm.3)

fits3 %>% 
ggplot(aes(x = day01, y = N_A.std)) +
  stat_lineribbon(aes(y = .epred),.width = c(.95), alpha = 1/4, color ="grey") +
  stat_lineribbon(data = preds3, aes(y = .prediction),.width = c(.90), alpha = 1/4, color ="blue") +
  geom_point(data = newd) +
  facet_wrap(~record_id)
```


---

```{r}
#| code-fold: true
preds3 %>% 
ggplot(aes(x = .prediction)) +
  stat_halfeye(aes(x = .prediction)) +
  facet_wrap(~record_id) + xlim(-4,4)

```


