[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Longitudinal Data Analyses",
    "section": "",
    "text": "9/13: Homework #1 is posted! Due by 9/26\n8/30: Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/longitudinal-2022\nPlease check the homepage regularly for class updates and announcements."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro-1.html#how-to-think-longitudinal-y",
    "href": "intro-1.html#how-to-think-longitudinal-y",
    "title": "Thinking longitudinally",
    "section": "how to think longitudinal-y",
    "text": "how to think longitudinal-y\n\n\n\nlines/ trajectories\nvariance decomposition"
  },
  {
    "objectID": "intro-1.html#goals-for-today",
    "href": "intro-1.html#goals-for-today",
    "title": "Thinking longitudinally",
    "section": "Goals for today:",
    "text": "Goals for today:\n\n\n\n\nGet a feeling for how to think/talk about longitudinal/repeated measures data\nIntroduce some important terms\nBegin to develop a framework for analysis"
  },
  {
    "objectID": "intro-1.html#how-do-we-define-change",
    "href": "intro-1.html#how-do-we-define-change",
    "title": "Thinking longitudinally",
    "section": "How do we define “change”?",
    "text": "How do we define “change”?\n\nTypes of change (most common):\nDifferential / rank order consistency/rank order stability (correlations)\nMean level/ absolute change (mean differences)"
  },
  {
    "objectID": "intro-1.html#how-do-we-defining-change",
    "href": "intro-1.html#how-do-we-defining-change",
    "title": "Thinking longitudinally",
    "section": "How do we defining “change”?",
    "text": "How do we defining “change”?\n\nBecause there are many types of change, we will view change in terms of the model.\n(Usually) it is clearer to refer to the type of change in terms of an equation or pictorially. Putting a word onto it usually causes confusion, which is why there are a lot of redundant terms in the literature.\nModels may be able to tell us about two different types of change (within person vs between person change)"
  },
  {
    "objectID": "intro-1.html#prerequisites",
    "href": "intro-1.html#prerequisites",
    "title": "Thinking longitudinally",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOrdinal or greater scale of measurement us easiest. Dichotomous is hard.\nConstruct has the same meaning across measurement occasions. Usually the same items. Called measurement invariance. Complicates developmental work.\n2 or more measurement occasions. More is better! Though often 3 - 10 is practically fine for some models. With 30+ occasions you have “intensive” longitudinal data which presents new models and opportunities."
  },
  {
    "objectID": "intro-1.html#defining-time-metric",
    "href": "intro-1.html#defining-time-metric",
    "title": "Thinking longitudinally",
    "section": "Defining time metric",
    "text": "Defining time metric\n\nTime is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable.\nWhat is the process that is changing someone? Age? Time in study? Year? Wave?\nIs it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. If you are running an intervention you may want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level.\n\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#defining-a-time-metric",
    "href": "intro-1.html#defining-a-time-metric",
    "title": "Thinking longitudinally",
    "section": "Defining a time metric",
    "text": "Defining a time metric\n\nTime is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable.\nWhat is the process that is changing someone? Age? Time in study? Year? Wave?\nIs it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. If you are running an intervention you may want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level."
  },
  {
    "objectID": "intro-1.html#example",
    "href": "intro-1.html#example",
    "title": "Thinking longitudinally",
    "section": "Example",
    "text": "Example\n\nDataPlot\n\n\n\n\n# A tibble: 225 × 51\n      ID group  time    CON CON_SAL CON_SMN7   DAN DAN_CON DAN_SAL DAN_SMN7\n   <dbl> <chr> <dbl>  <dbl>   <dbl>    <dbl> <dbl>   <dbl>   <dbl>    <dbl>\n 1     6 PD        4 0.193   0.0708   0.0088 0.162  0.116   0.0739   0.0041\n 2     6 PD        5 0.195   0.0862   0.004  0.168  0.117   0.0812   0.0204\n 3     6 PD        6 0.181   0.0916  -0.0037 0.215  0.164   0.113    0.0155\n 4    29 PD        4 0.159   0.0438  -0.0253 0.175  0.0161  0.0621   0.0121\n 5    29 PD        5 0.0881  0.0446  -0.0288 0.136  0.0377  0.0277   0.0444\n 6    34 CTRL      4 0.137   0.0113  -0.0792 0.166  0.0045 -0.0075   0.0432\n 7    34 CTRL      6 0.0746  0.0009  -0.0089 0.140  0.0635 -0.024    0.0483\n 8    36 CTRL      1 0.139   0.0438  -0.0466 0.152  0.0279  0.0137   0.0513\n 9    36 CTRL      4 0.180   0.0772  -0.0648 0.205 -0.0116 -0.0477   0.0415\n10    37 PD        3 0.226   0.0412  -0.0565 0.219  0.0971  0.0195   0.0644\n# … with 215 more rows, and 41 more variables: DMN6 <dbl>, DMN6_CON <dbl>,\n#   DMN6_DAN <dbl>, DMN6_SAL <dbl>, DMN6_SMN7 <dbl>, SAL <dbl>, SAL_SMN7 <dbl>,\n#   SMN7 <dbl>, wave <dbl>, date <date>, Exclude <chr>, RSNdata <dbl>,\n#   RSNexclude <chr>, RSNexcludeDevDem <chr>, CogDate_0 <date>,\n#   CCIRtrio_MR_date_0 <date>, Dur_PDsx_0 <dbl>, PIBpos18 <chr>,\n#   Neuro_Dx <chr>, NeuroCDR_0 <chr>, NeuroCDR_1 <chr>, NeuroCDR_2 <chr>,\n#   NeuroCDR_3 <chr>, NeuroCDR_4 <chr>, NeuroCDR_5 <chr>, NeuroCDR_6 <chr>, …\n\n\n\n\n\nggplot(example,\n   aes(x = year, y = SMN7, group = ID)) + geom_point()"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Motivation and handling data\nLongitudinal MLM\nLongitudinal SEM\nLines that Bend\nPredictors"
  },
  {
    "objectID": "intro-1.html#individual-level",
    "href": "intro-1.html#individual-level",
    "title": "Thinking longitudinally",
    "section": "Individual level",
    "text": "Individual level\n\nggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)"
  },
  {
    "objectID": "intro-1.html#theoretical-model-of-change",
    "href": "intro-1.html#theoretical-model-of-change",
    "title": "Thinking longitudinally",
    "section": "Theoretical model of change",
    "text": "Theoretical model of change\n\nThe shape we want to model - linear, quadradic, cyclical, etc.\nIs shape related to calendar time, age, or maybe artificial time such as grade"
  },
  {
    "objectID": "intro-1.html#section",
    "href": "intro-1.html#section",
    "title": "Thinking longitudinally",
    "section": "",
    "text": "# A tibble: 20 × 6\n# Groups:   ID [10]\n      ID term        estimate std.error statistic  p.value\n   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1    67 (Intercept)  0.0921    0.0161      5.72    0.0292\n 2    67 week         0.00662   0.00657     1.01    0.420 \n 3    75 (Intercept)  0.126   NaN         NaN     NaN     \n 4    75 week         0.00771 NaN         NaN     NaN     \n 5    87 (Intercept)  0.0787  NaN         NaN     NaN     \n 6    87 week        -0.0227  NaN         NaN     NaN     \n 7    99 (Intercept)  0.111     0.0236      4.69    0.0426\n 8    99 week         0.00545   0.0122      0.446   0.699 \n 9   101 (Intercept)  0.111     0.0424      2.62    0.232 \n10   101 week         0.0421    0.0217      1.94    0.303 \n11   103 (Intercept)  0.0887  NaN         NaN     NaN     \n12   103 week        -0.0168  NaN         NaN     NaN     \n13   105 (Intercept)  0.0465    0.00658     7.06    0.0896\n14   105 week         0.00122   0.00467     0.261   0.838 \n15   142 (Intercept)  0.197   NaN         NaN     NaN     \n16   142 week         0.0130  NaN         NaN     NaN     \n17   149 (Intercept)  0.0801  NaN         NaN     NaN     \n18   149 week         0.00497 NaN         NaN     NaN     \n19   152 (Intercept)  0.0921  NaN         NaN     NaN     \n20   152 week        -0.0172  NaN         NaN     NaN     \n\n\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#how-do-we-define-change-1",
    "href": "intro-1.html#how-do-we-define-change-1",
    "title": "Thinking longitudinally",
    "section": "How do we define “change”?",
    "text": "How do we define “change”?\n\nBecause there are many types of change, we will view change in terms of the model.\n(Usually) it is clearer to refer to the type of change in terms of an equation or pictorially. Putting a word onto it usually causes confusion, which is why there are a lot of redundant terms in the literature.\nModels may be able to tell us about two different types of change (within person vs between person change)"
  },
  {
    "objectID": "intro-1.html#questions-we-need-to-answer",
    "href": "intro-1.html#questions-we-need-to-answer",
    "title": "Thinking longitudinally",
    "section": "Questions we need to answer",
    "text": "Questions we need to answer\n\nWhat is the theoretical shape we want to model - linear, quadradic, cyclical, etc?\nIs shape related to calendar time, age, or maybe artificial time such as grade?"
  },
  {
    "objectID": "intro-1.html#temporal-design",
    "href": "intro-1.html#temporal-design",
    "title": "Thinking longitudinally",
    "section": "Temporal design",
    "text": "Temporal design\n\nI.e., timing, frequency, and spacing of assessments.\nHow longitudinal data are collected will impact our ability to model the theoretical shape.\nBecause of the difficulty of collecting longitudinal data, a lot of longitudinal data are under specified."
  },
  {
    "objectID": "intro-1.html#statistical-model",
    "href": "intro-1.html#statistical-model",
    "title": "Thinking longitudinally",
    "section": "Statistical model",
    "text": "Statistical model\n\nWith a theoretical model of change in mind, and a good temporal design, we can then choose our statistical model.\nThis matching of theory with design with a model is similar to all of stats. We will be using three general purpose models that are related, but have pros and cons in different areas: MLM, SEM, and GAMs"
  },
  {
    "objectID": "intro-1.html#simple-to-begin-with",
    "href": "intro-1.html#simple-to-begin-with",
    "title": "Thinking longitudinally",
    "section": "Simple to begin with",
    "text": "Simple to begin with\nBefore we get too fancy, lets just run some regressions."
  },
  {
    "objectID": "intro-1.html#individual-regression-output",
    "href": "intro-1.html#individual-regression-output",
    "title": "Thinking longitudinally",
    "section": "Individual regression output",
    "text": "Individual regression output\n\n\nCode\nlibrary(broom)\nregressions <- example2 %>% \n  group_by(ID) %>% \n  do(tidy(lm(SMN7 ~ week, data=.)))\n\nregressions\n\n\n# A tibble: 20 × 6\n# Groups:   ID [10]\n      ID term        estimate std.error statistic  p.value\n   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1    67 (Intercept)  0.0921    0.0161      5.72    0.0292\n 2    67 week         0.00662   0.00657     1.01    0.420 \n 3    75 (Intercept)  0.126   NaN         NaN     NaN     \n 4    75 week         0.00771 NaN         NaN     NaN     \n 5    87 (Intercept)  0.0787  NaN         NaN     NaN     \n 6    87 week        -0.0227  NaN         NaN     NaN     \n 7    99 (Intercept)  0.111     0.0236      4.69    0.0426\n 8    99 week         0.00545   0.0122      0.446   0.699 \n 9   101 (Intercept)  0.111     0.0424      2.62    0.232 \n10   101 week         0.0421    0.0217      1.94    0.303 \n11   103 (Intercept)  0.0887  NaN         NaN     NaN     \n12   103 week        -0.0168  NaN         NaN     NaN     \n13   105 (Intercept)  0.0465    0.00658     7.06    0.0896\n14   105 week         0.00122   0.00467     0.261   0.838 \n15   142 (Intercept)  0.197   NaN         NaN     NaN     \n16   142 week         0.0130  NaN         NaN     NaN     \n17   149 (Intercept)  0.0801  NaN         NaN     NaN     \n18   149 week         0.00497 NaN         NaN     NaN     \n19   152 (Intercept)  0.0921  NaN         NaN     NaN     \n20   152 week        -0.0172  NaN         NaN     NaN"
  },
  {
    "objectID": "intro-1.html#if-we-simply-average-these-we-get",
    "href": "intro-1.html#if-we-simply-average-these-we-get",
    "title": "Thinking longitudinally",
    "section": "If we simply average these we get",
    "text": "If we simply average these we get\nPopulation level estimates ::: {.cell}\nregressions %>% \n  group_by(term) %>% \n  summarise(avg.reg = mean(estimate))\n\n# A tibble: 2 × 2\n  term        avg.reg\n  <chr>         <dbl>\n1 (Intercept) 0.102  \n2 week        0.00244\n\n:::\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#average-each-regression",
    "href": "intro-1.html#average-each-regression",
    "title": "Thinking longitudinally",
    "section": "Average each regression",
    "text": "Average each regression\n\nregressions %>% \n  group_by(term) %>% \n  summarise(avg.reg = mean(estimate))\n\n# A tibble: 2 × 2\n  term        avg.reg\n  <chr>         <dbl>\n1 (Intercept) 0.102  \n2 week        0.00244\n\n\nThis is not that far off from what MLM gives us."
  },
  {
    "objectID": "intro-1.html#spaghetti-plot",
    "href": "intro-1.html#spaghetti-plot",
    "title": "Thinking longitudinally",
    "section": "Spaghetti Plot",
    "text": "Spaghetti Plot\n\n\nCode\nggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = \"lm\", se = FALSE) +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = \"black\"), method = \"lm\", size = 2) + guides(fill=\"none\")+ theme(legend.position=\"none\")"
  },
  {
    "objectID": "intro-1.html#thinking-longitudinally",
    "href": "intro-1.html#thinking-longitudinally",
    "title": "Thinking longitudinally",
    "section": "Thinking longitudinally",
    "text": "Thinking longitudinally\nAlmost all of the questions we have can be simplified down to: lines/trajectories.\n\nPerson level trajectories index change for a person\nAverage person trajectory is the average trajectory\nPredictors of change are just a correlation with the trajectory and the predictor"
  },
  {
    "objectID": "intro-1.html#lines-regardless-of-the-stat-model",
    "href": "intro-1.html#lines-regardless-of-the-stat-model",
    "title": "Thinking longitudinally",
    "section": "Lines, regardless of the stat model",
    "text": "Lines, regardless of the stat model\n\nMLM and SEM (and even GAMs) can be equivalent\nWe will start with MLM/HLM as it is simple extension of standard regression models. Best suited to run models when the time of measurement differs from person to person (compared to equal intervals). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person"
  },
  {
    "objectID": "intro-1.html#why-not-rm-anova",
    "href": "intro-1.html#why-not-rm-anova",
    "title": "Thinking longitudinally",
    "section": "Why not RM ANOVA?",
    "text": "Why not RM ANOVA?\n\nCannot handle missing data\nAssumes rate of change is the same for all individuals.\nTime is categorical.\nAccounting for correlation across time uses up many parameters (df penalty).\nCannot handle some types of predictors\nSpecial case of MLM, might as well learn/use flexible model"
  },
  {
    "objectID": "intro-1.html#how-to-think-longitudinal-y-1",
    "href": "intro-1.html#how-to-think-longitudinal-y-1",
    "title": "Thinking longitudinally",
    "section": "how to think longitudinal-y",
    "text": "how to think longitudinal-y\n\nlines/ trajectories\nvariance decomposition"
  },
  {
    "objectID": "intro-1.html#modeling-dependency",
    "href": "intro-1.html#modeling-dependency",
    "title": "Thinking longitudinally",
    "section": "Modeling dependency",
    "text": "Modeling dependency\nWe have multiple DVs per person with longitudinal data. If we ignored the person aspect, the residuals would likely be related, violating standard regression assumption. MLM accounts for residuals for outcomes from the same person through modeling different “levels”\nWith longitudinal data we have people nested within observations.\nLevel 1: observation level (observation specific variance)\nLevel 2: person level (person specific variance)"
  },
  {
    "objectID": "intro-1.html#person-specific-variance",
    "href": "intro-1.html#person-specific-variance",
    "title": "Thinking longitudinally",
    "section": "Person specific variance",
    "text": "Person specific variance\nSome people start at different levels and some people change at different rates\n\n\nCode\nggplot(example, aes(x = year, y = SMN7, group = ID, colour = ID)) + stat_smooth(method = \"lm\", se = FALSE, alpha = .5) + theme(legend.position=\"none\")"
  },
  {
    "objectID": "intro-1.html#observation-level-variance",
    "href": "intro-1.html#observation-level-variance",
    "title": "Thinking longitudinally",
    "section": "Observation level variance",
    "text": "Observation level variance\nAfter account for a person starting level and their slope, there is still residual variance left over.\n\n\nCode\nob.var <- example %>% \n  filter(ID %in% c(\"67\",\"82\", \"110\")) \n\nexample3 <-\n  left_join(ob.var, example)  \n  \nggplot(example3,\n   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method=\"lm\", se = FALSE) + facet_wrap( ~ID)"
  },
  {
    "objectID": "intro-1.html#thinking-about-variation",
    "href": "intro-1.html#thinking-about-variation",
    "title": "Thinking longitudinally",
    "section": "Thinking about variation",
    "text": "Thinking about variation\nA goal of longitudinal data analysis (and all other data analysis) is to explain this variation. We will fit models that includes predictors and model constraints (e.g. are people similar or different) to see how it impacts variation.\nTo the extent that we can put variance into different “piles” (eg people change at different rates, a random slope) we will have more explained variance and less unexplained variance."
  },
  {
    "objectID": "intro-1.html#speaking-of-variation",
    "href": "intro-1.html#speaking-of-variation",
    "title": "Thinking longitudinally",
    "section": "Speaking of variation",
    "text": "Speaking of variation\n\nBetween-Person (BP) Variation/Level-2/INTER-individual differences/Time-Invariant\nBP = More/less than other people\nWithin-Person (WP) Variation/Level-1/INTRA-individual Differences/Time-Varying\nWP = more/less than one’s average\nAny variable measured over time usually has both BP and WP variation"
  },
  {
    "objectID": "intro-1.html#within-person-focus",
    "href": "intro-1.html#within-person-focus",
    "title": "Thinking longitudinally",
    "section": "Within person focus",
    "text": "Within person focus\nWithin-Person Change: Systematic (lasting) change Magnitude or direction of change can be different across individuals. Can refer to between person (inter individual differences) in within person change (intra individual)\nWithin-Person Fluctuation: No systematic change Outcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#change-vs-fluctuations",
    "href": "intro-1.html#change-vs-fluctuations",
    "title": "Thinking longitudinally",
    "section": "Change vs fluctuations",
    "text": "Change vs fluctuations\n\nFuzzy boundary, but:\nWithin-Person Change: Systematic (lasting) change. Can refer to between-person (inter-individual) differences in within-person change (intra-individual)\nWithin-Person Fluctuation: No systematic change Outcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual"
  },
  {
    "objectID": "intro-1.html#what-are-data",
    "href": "intro-1.html#what-are-data",
    "title": "Thinking longitudinally",
    "section": "What Are Data?",
    "text": "What Are Data?\nData are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean .csv, .xls, .sav, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.\nWhy are we thinking about data? Because 80%, maybe more, of your time spent with “analysis” is spent getting data in order and setting up your model of interest."
  },
  {
    "objectID": "intro-1.html#wide-vs-long",
    "href": "intro-1.html#wide-vs-long",
    "title": "Thinking longitudinally",
    "section": "Wide vs long",
    "text": "Wide vs long\n\nmultivariate vs stacked\nperson vs person period\nuntidy vs tidy*\nLong is what MLM, ggplot2 and tidyverse packages expect whereas SEM and a lot of descriptives are calculated using wide dataframes."
  },
  {
    "objectID": "intro-1.html#tidyr",
    "href": "intro-1.html#tidyr",
    "title": "Thinking longitudinally",
    "section": "tidyr",
    "text": "tidyr\nFor longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows.\n\n\n# A tibble: 225 × 4\n      ID  wave group   DAN\n   <dbl> <dbl> <chr> <dbl>\n 1     6     1 PD    0.162\n 2     6     2 PD    0.168\n 3     6     3 PD    0.215\n 4    29     1 PD    0.175\n 5    29     2 PD    0.136\n 6    34     1 CTRL  0.166\n 7    34     2 CTRL  0.140\n 8    36     1 CTRL  0.152\n 9    36     2 CTRL  0.205\n10    37     1 PD    0.219\n# … with 215 more rows"
  },
  {
    "objectID": "intro-1.html#pivot_wider",
    "href": "intro-1.html#pivot_wider",
    "title": "Thinking longitudinally",
    "section": "pivot_wider",
    "text": "pivot_wider\nThe pivot_wider() function takes two arguments: names_from which is the variable whose values will be converted to column names and values_from whose values will be cell values.\n\nwide.ex <- long %>% \n  pivot_wider(names_from = wave, values_from = DAN) \nwide.ex\n\n# A tibble: 91 × 6\n      ID group    `1`    `2`    `3`   `4`\n   <dbl> <chr>  <dbl>  <dbl>  <dbl> <dbl>\n 1     6 PD    0.162  0.168   0.215    NA\n 2    29 PD    0.175  0.136  NA        NA\n 3    34 CTRL  0.166  0.140  NA        NA\n 4    36 CTRL  0.152  0.205  NA        NA\n 5    37 PD    0.219  0.158   0.259    NA\n 6    48 PD    0.130  0.270   0.248    NA\n 7    53 CTRL  0.211  0.152  NA        NA\n 8    54 PD    0.220  0.152   0.192    NA\n 9    58 PD    0.380  0.215   0.204    NA\n10    61 PD    0.0818 0.0628 NA        NA\n# … with 81 more rows"
  },
  {
    "objectID": "intro-1.html#pivot_longer",
    "href": "intro-1.html#pivot_longer",
    "title": "Thinking longitudinally",
    "section": "pivot_longer",
    "text": "pivot_longer\nThe pivot_longer function takes three arguments: - cols is a list of columns that are to be collapsed. The columns can be referenced by column number or column name. - names_to is the name of the new column which will combine all column names. This is up to you to decide what the name is. - values_to is the name of the new column which will combine all column values associated with each variable combination."
  },
  {
    "objectID": "intro-1.html#seperate-and-unite",
    "href": "intro-1.html#seperate-and-unite",
    "title": "Thinking longitudinally",
    "section": "Seperate and Unite",
    "text": "Seperate and Unite\n\nMany times datasets are, for a lack of a better term, messy.\nOne common way to represent longitudinal data is to name the variable with a wave signifier.\n\n\n\n# A tibble: 3 × 4\n     ID ext_1 ext_2 ext_3\n  <dbl> <dbl> <dbl> <dbl>\n1     1     4     4     4\n2     2     6     5     4\n3     3     4     5     6"
  },
  {
    "objectID": "intro-1.html#date-time-metrics",
    "href": "intro-1.html#date-time-metrics",
    "title": "Thinking longitudinally",
    "section": "Date time metrics",
    "text": "Date time metrics\n\nlibrary(lubridate)\n\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). These are called POSIXct in R.\n\ntoday()\n\n[1] \"2022-08-31\"\n\n\n\nnow()\n\n[1] \"2022-08-31 13:52:03 CDT\""
  },
  {
    "objectID": "intro-1.html#projects-and-rmarkdown",
    "href": "intro-1.html#projects-and-rmarkdown",
    "title": "Thinking longitudinally",
    "section": "Projects and Rmarkdown",
    "text": "Projects and Rmarkdown\nAs with any project, but especially for longitudinal data, one of the most important aspects of data analysis is A. not losing track of what you did and B. being organized.\n\nrstudio projects 2. git and 3. codebooks are helpful in accomplishing these two goals. We will talk about #1 and #3 but I also encourage you to read about git. These are not the only way to do these sorts of analyses but I feel that exposure to them is helpful, as often in the social sciences these sort of decisions are not discussed."
  },
  {
    "objectID": "intro-1.html#packages",
    "href": "intro-1.html#packages",
    "title": "Thinking longitudinally",
    "section": "Packages",
    "text": "Packages\nPackages seems like the most basic step, but it is actually very important. Depending on what gets loaded you might overwrite functions from other packages. (Note: I will often reload or not follow this advice within lectures for didactic reasons, choosing to put library calls above the code)\n\n# load packages\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "intro-1.html#codebook",
    "href": "intro-1.html#codebook",
    "title": "Thinking longitudinally",
    "section": "Codebook",
    "text": "Codebook\nThe second step is a codebook. Arguably, this is the first step because you should create the codebook long before you open R and load your data.\nWhy a codebook? Well, because you typically have a lot of variables and you will not be able to remember all the details that go into each one of them (rating scale, what the actual item was, was it coded someway, etc). This is especially true now that data are being collected online, which often provides placeholder variable names that then need to be processed somehow.\nThis codebook will serve as a means to document RAW code. It will also allow us to automate some tasks that are somewhat cumbersome, facilitate open data practices, and efficiently see what variables are available. Ultimately, we want to be able to show how we got from the start, with the messy raw data, to our analyses and results at the end? A codebook makes this easier."
  },
  {
    "objectID": "intro-1.html#data-1",
    "href": "intro-1.html#data-1",
    "title": "Thinking longitudinally",
    "section": "Data",
    "text": "Data\nFirst, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.\nThis code below shows how I would read in and rename a wide-format data set using the codebook I created.\n\nold.names <- codebook$old_name # get old column names\nnew.names <- codebook$new_name # get new column names\n\nsoep <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/soepdata.csv\")\n\n soep <-  soep %>% # read in data\n  dplyr::select(old.names) %>% # select the columns from our codebook\n  setNames(new.names) # rename columns with our new names\nsoep"
  },
  {
    "objectID": "intro-1.html#recode-variables",
    "href": "intro-1.html#recode-variables",
    "title": "Thinking longitudinally",
    "section": "Recode Variables",
    "text": "Recode Variables\nMany of the data we work with have observations that are missing for a variety of reasons. In R, we treat missing values as NA, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit NA values."
  },
  {
    "objectID": "intro-1.html#reverse-scoring",
    "href": "intro-1.html#reverse-scoring",
    "title": "Thinking longitudinally",
    "section": "Reverse-Scoring",
    "text": "Reverse-Scoring\nMany scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.\nThere are a few ways to do this in R. Below, I’ll demonstrate how to do so using the reverse.code() function in the psych package in R. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).\nBefore we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook."
  },
  {
    "objectID": "intro-1.html#create-composites",
    "href": "intro-1.html#create-composites",
    "title": "Thinking longitudinally",
    "section": "Create Composites",
    "text": "Create Composites\nNow that we have reverse coded our items, we can create composites.\nWe’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.\nThe “simplest” way, which is also the longest way because you’d have to do it for each scale, in each year is to use a function like rowMeans which I don’t recommend as that will be MANY MANY lines of code."
  },
  {
    "objectID": "intro-1.html#metric-variables",
    "href": "intro-1.html#metric-variables",
    "title": "Thinking longitudinally",
    "section": "metric variables",
    "text": "metric variables\n\n\nCode\nb5_soep_long_des <- b5_soep_long %>%\n  unite(tmp, trait, year, sep = \"_\") \nhead(b5_soep_long_des)\n\n\n# A tibble: 6 × 5\n  Procedural__SID tmp    value   DOB   Sex\n            <dbl> <chr>  <dbl> <dbl> <dbl>\n1             901 A_2005  4.67  1951     2\n2             901 A_2009  4.33  1951     2\n3             901 A_2013  4.67  1951     2\n4             901 C_2005  5     1951     2\n5             901 C_2009  5     1951     2\n6             901 C_2013  5     1951     2"
  },
  {
    "objectID": "intro-1.html#count-variables",
    "href": "intro-1.html#count-variables",
    "title": "Thinking longitudinally",
    "section": "count variables",
    "text": "count variables\nWe have life event variable in the dataset that is a count variable. It asks did someone experience a life event during the previous year. also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).\n\n\nCode\nevents_long  <-soep_long %>%\n  filter(type == \"Life Event\") \nhead(events_long )\n\n\n# A tibble: 6 × 12\n  Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex type  trait\n            <dbl>            <dbl>            <dbl>            <dbl> <chr> <chr>\n1             901               94             1951                2 Life… MomD…\n2             901               94             1951                2 Life… MomD…\n3            2301              230             1946                1 Life… Move…\n4            2301              230             1946                1 Life… Part…\n5            2301              230             1946                1 Life… Part…\n6            2305              230             1946                2 Life… Move…\n# … with 6 more variables: item <chr>, year <chr>, value <dbl>, reverse <int>,\n#   mini <int>, maxi <int>"
  },
  {
    "objectID": "intro-1.html#zero-order-correlations",
    "href": "intro-1.html#zero-order-correlations",
    "title": "Thinking longitudinally",
    "section": "Zero-Order Correlations",
    "text": "Zero-Order Correlations\nTo run the correlations, we will need to have our data in wide format\n\n\nCode\nb5_soep_long %>%\n  unite(tmp, trait, year, sep = \"_\") %>%\n  pivot_wider(names_from = tmp, values_from = value) %>% \n  select(-Procedural__SID) %>%\n  cor(., use = \"pairwise\") %>%\n  round(., 2)\n\n\n         DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009\nDOB     1.00  0.00   0.02   0.05   0.05   0.13   0.17   0.17  -0.03  -0.01\nSex     0.00  1.00  -0.01  -0.02   0.01  -0.06  -0.05  -0.05   0.10   0.09\nA_2005  0.02 -0.01   1.00   0.30   0.26   0.21   0.07   0.05   0.28   0.12\nA_2009  0.05 -0.02   0.30   1.00   0.32   0.08   0.21   0.06   0.15   0.28\nA_2013  0.05  0.01   0.26   0.32   1.00   0.05   0.07   0.19   0.13   0.15\nC_2005  0.13 -0.06   0.21   0.08   0.05   1.00   0.30   0.26   0.24   0.09\nC_2009  0.17 -0.05   0.07   0.21   0.07   0.30   1.00   0.37   0.09   0.25\nC_2013  0.17 -0.05   0.05   0.06   0.19   0.26   0.37   1.00   0.06   0.07\nE_2005 -0.03  0.10   0.28   0.15   0.13   0.24   0.09   0.06   1.00   0.39\nE_2009 -0.01  0.09   0.12   0.28   0.15   0.09   0.25   0.07   0.39   1.00\nE_2013 -0.08  0.12   0.11   0.15   0.28   0.08   0.11   0.22   0.38   0.42\nN_2005 -0.08  0.13   0.23   0.07   0.08   0.09  -0.02  -0.04   0.18   0.07\nN_2009 -0.04  0.14   0.10   0.23   0.10   0.01   0.10  -0.01   0.09   0.16\nN_2013 -0.05  0.13   0.09   0.07   0.21   0.02  -0.01   0.08   0.09   0.10\nO_2005  0.11  0.06   0.23   0.14   0.13   0.24   0.14   0.13   0.36   0.23\nO_2009  0.10  0.05   0.10   0.24   0.13   0.14   0.23   0.12   0.19   0.34\nO_2013  0.05  0.07   0.10   0.12   0.23   0.11   0.11   0.20   0.19   0.21\n       E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013\nDOB     -0.08  -0.08  -0.04  -0.05   0.11   0.10   0.05\nSex      0.12   0.13   0.14   0.13   0.06   0.05   0.07\nA_2005   0.11   0.23   0.10   0.09   0.23   0.10   0.10\nA_2009   0.15   0.07   0.23   0.07   0.14   0.24   0.12\nA_2013   0.28   0.08   0.10   0.21   0.13   0.13   0.23\nC_2005   0.08   0.09   0.01   0.02   0.24   0.14   0.11\nC_2009   0.11  -0.02   0.10  -0.01   0.14   0.23   0.11\nC_2013   0.22  -0.04  -0.01   0.08   0.13   0.12   0.20\nE_2005   0.38   0.18   0.09   0.09   0.36   0.19   0.19\nE_2009   0.42   0.07   0.16   0.10   0.23   0.34   0.21\nE_2013   1.00   0.08   0.07   0.17   0.20   0.21   0.35\nN_2005   0.08   1.00   0.34   0.32   0.14   0.03   0.05\nN_2009   0.07   0.34   1.00   0.39   0.05   0.13   0.03\nN_2013   0.17   0.32   0.39   1.00   0.04   0.06   0.15\nO_2005   0.20   0.14   0.05   0.04   1.00   0.58   0.55\nO_2009   0.21   0.03   0.13   0.06   0.58   1.00   0.61\nO_2013   0.35   0.05   0.03   0.15   0.55   0.61   1.00"
  },
  {
    "objectID": "intro-1.html#tidyr-pivot-functions",
    "href": "intro-1.html#tidyr-pivot-functions",
    "title": "Thinking longitudinally",
    "section": "tidyr pivot functions",
    "text": "tidyr pivot functions\nFor longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows.\n\n\n# A tibble: 225 × 4\n      ID  wave group   DAN\n   <dbl> <dbl> <chr> <dbl>\n 1     6     1 PD    0.162\n 2     6     2 PD    0.168\n 3     6     3 PD    0.215\n 4    29     1 PD    0.175\n 5    29     2 PD    0.136\n 6    34     1 CTRL  0.166\n 7    34     2 CTRL  0.140\n 8    36     1 CTRL  0.152\n 9    36     2 CTRL  0.205\n10    37     1 PD    0.219\n# … with 215 more rows"
  },
  {
    "objectID": "intro-1.html#descriptives",
    "href": "intro-1.html#descriptives",
    "title": "Thinking longitudinally",
    "section": "Descriptives",
    "text": "Descriptives\nDescriptives of your data are incredibly important. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.\nThere are lots of ways to create great tables of descriptives.For now, we’ll use a wonderfully helpful function from the psych package called describe() in conjunction with a small amount of tidyr to reshape the data."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor:\nJoshua Jackson\n\n\nOffice:\n315b\n\n\nOffice hours:\nThur after class and by appt\nThis course covers modern methods of handling longitudinal, repeated measures. The class will introduce the rationale of measuring change and stability over time to study phenomena, as well as how within-person designs can increase statistical power and precision compared to more traditional designs. Most the course will use multi-level models and latent (growth) curve models to specify patterns of change across time. Additional topics include: visualization, measurement invariance, time-to- event models and power. PREREQ: Use of R will be required, Familiarity with MLM and/or Structural Equation Models.\n\n\nClass textbooks\nHoffman, Lesa. (2015). Longitudinal Analysis: Modeling Within-Person Fluctuation and Change. (reffered to as LA)\nLittle, T. D. (2013). Longitudinal structural equation modeling. Guilford press. (referred to as LSEM)\nI have PDFs available if you do not want to purchase the textbook.\nI find both of these textbooks quite readable, but if you are struggling I have additional recommendations for intro MLM and SEM that may fill in the gaps. Additionally, if there are topics that you are interested in and want to know more about please let me know as there are a number of textbooks devoted to a single topic.\n\n\nGrading\nGrading consists of 3 aspects:\n\nSemi-weekly “pop” quiz. Quizzes consist of 1-3 questions based on the previous lecture and readings.\nHomework take homes. Homework will be (typically) due 1 week later.\nFinal project. Everyone needs to do a longitudinal data analysis from start to finish. Preferably you have your own data, but if you do not then let me know and I can give you some. More details on this later.\n\nGrading breakdown is 1/3 for each of these components.\n\n\nSchedule\nThis will likely change based on our pace and what we want to cover more/less in depth\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nReadings\n\n\n\n\n1\n8/30\nMotivation and data\nLA Ch 1 (2 optional)\n\n\n2\n9/6\nLongitudinal MLM\nLA Ch 3, 5\n\n\n3\n9/13\nLongitudinal MLM\n\n\n\n4\n9/20\nLongitudinal SEM\nLSEM 1,3\n\n\n5\n9/27\nLongitudinal SEM\nLSEM 8\n\n\n6\n10/4\nMeasurement invariance and treating time\nLSEM 7\n\n\n7\n10/11\nCurves that bend\nLA 6\n\n\n8\n10/18\nCurves that bend\n\n\n\n9\n10/25\nPredictors of change\nLA 7\n\n\n10\n11/1\nPredictors of change\n\n\n\n11\n11/8\nIntensive longitudinal designs\nLA 8\n\n\n12\n11/15\nMultivariate change\nLA 9\n\n\n13\n11/22\nMultivariate change\n\n\n\n14\n11/29\n2 measurement points\n\n\n\n15\n12/6\nMixture Models\n\n\n\n\nOther topics: Longitudinal mediation (and multilevel mediation), time to event analyses, power, …?"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "readings",
    "section": "",
    "text": "Extra readings will go here"
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "homeworks",
    "section": "",
    "text": "Longitudinal MLM homework\nLongitudinal SEM homework"
  },
  {
    "objectID": "mlm-2.html#standard-regression",
    "href": "mlm-2.html#standard-regression",
    "title": "MLM",
    "section": "Standard regression",
    "text": "Standard regression\n\\[{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\\epsilon_{i}\\]\n\\[\\hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+...\\]\nParameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.\nEach person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i."
  },
  {
    "objectID": "mlm-2.html#handling-multiple-dvs",
    "href": "mlm-2.html#handling-multiple-dvs",
    "title": "MLM",
    "section": "Handling multiple DVs?",
    "text": "Handling multiple DVs?\nBut what if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple ____ ?\nTwo options: 1. Collapse and average across."
  },
  {
    "objectID": "mlm-2.html#example",
    "href": "mlm-2.html#example",
    "title": "MLM",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "mlm-2.html#could-aggragate-across-group",
    "href": "mlm-2.html#could-aggragate-across-group",
    "title": "MLM",
    "section": "could aggragate across group",
    "text": "could aggragate across group"
  },
  {
    "objectID": "mlm-2.html#aggregation-obscures-hypotheses",
    "href": "mlm-2.html#aggregation-obscures-hypotheses",
    "title": "MLM",
    "section": "Aggregation obscures hypotheses",
    "text": "Aggregation obscures hypotheses\n\nBetween person H1: Do students who study more get better grades?\nWithin person H2: When a student studies, do they get better grades?\nH1 and H2 are independent from one another! Aggregation collapses the two. When you have nested data with many DVs (longitudinal but also almost any experimental situation) it is important to not aggregate."
  },
  {
    "objectID": "mlm-2.html#stroop-example",
    "href": "mlm-2.html#stroop-example",
    "title": "MLM",
    "section": "Stroop example",
    "text": "Stroop example\nWe calculate stroop scores by looking at repeated trials of congruent vs not congruent. This is dummy coded such that the \\(\\beta_{1}\\) reflects the average stroop effect. How much slower are people in incongruent trials?\n\\[Y_{i} = \\beta_{0} + \\beta_{1}X_{1} + \\varepsilon_i\\]"
  },
  {
    "objectID": "mlm-2.html#ways-to-think-about-mlms",
    "href": "mlm-2.html#ways-to-think-about-mlms",
    "title": "MLM",
    "section": "4 ways to think about MLMs",
    "text": "4 ways to think about MLMs\n\nDifferent levels of analysis (average/person specific or between/within)\n\nRegressions within regressions (ie coefficients as outcomes)\n\nVariance decomposition\n\nLearning from other data through pooling/shrinkage"
  },
  {
    "objectID": "mlm-2.html#get-new-example-here.",
    "href": "mlm-2.html#get-new-example-here.",
    "title": "MLM",
    "section": "GET NEW EXAMPLE HERE.",
    "text": "GET NEW EXAMPLE HERE."
  },
  {
    "objectID": "mlm-2.html#regressions-within-regressions",
    "href": "mlm-2.html#regressions-within-regressions",
    "title": "MLM",
    "section": "regressions within regressions",
    "text": "regressions within regressions\nHelps to take multilevel and split it into the different levels.\nLevel 1 is the smallest unit of analysis (students, waves, trials, family members)\nLevel 2 variables are what level 1 variables are “nested” in (people, schools, counties, families, dyads)\nWe are going to use level one components to run a regression, all the while level 1 is also estimating a regression"
  },
  {
    "objectID": "mlm-2.html#diferent-levels-and-regressions-within-regressions",
    "href": "mlm-2.html#diferent-levels-and-regressions-within-regressions",
    "title": "MLM",
    "section": "diferent levels and regressions within regressions",
    "text": "diferent levels and regressions within regressions\nTo sum up the first two ways to think about regression, we take a relationship that could be simplified by aggregation, but instead we ask regression questions at different levels of analysis.\nAt level 1 we can ask lower-unit questions e.g., if trials are nested within person, what predicts lengthier trials?\nLongitudinally, if level 1 is observations, then we can ask level 1/observation level questions like if you’re with people do you then feel happier"
  },
  {
    "objectID": "mlm-2.html#variance-decomposition",
    "href": "mlm-2.html#variance-decomposition",
    "title": "MLM",
    "section": "variance decomposition",
    "text": "variance decomposition\nFor standard regression, we think of error as existing in one big bucket called \\(\\varepsilon\\) . Everything that we do not know goes into that bucket, from measurement error to unmeasured important factors.\nFor MLMs we will be breaking up ( \\(\\varepsilon\\) ) into multiple buckets. These useful “buckets” (Us) are what we refer to as random/varying effects.\n\\[Y_{trials, i} = \\beta_{0i} + \\beta_{1i}X_{trial,i} + \\varepsilon_{trial}\\] \\[\\beta_{0} = \\gamma_{00} + U_{0i}\\] \\[\\beta_{1} = \\gamma_{10} +\\gamma_{11}Z_i+ U_{1i}\\]"
  },
  {
    "objectID": "mlm-2.html#shrinkagepartial-pooling",
    "href": "mlm-2.html#shrinkagepartial-pooling",
    "title": "MLM",
    "section": "shrinkage/partial pooling",
    "text": "shrinkage/partial pooling\n\nWe treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions.\nWe do this in standard regression where we make predictions based on values from the whole dataset (not just binned Xs). A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data!"
  },
  {
    "objectID": "mlm-2.html#complete-partial-and-no-pooling",
    "href": "mlm-2.html#complete-partial-and-no-pooling",
    "title": "MLM",
    "section": "Complete, partial and no pooling",
    "text": "Complete, partial and no pooling\n\nComplete assumes everyone is the same, with \\(U_{0i}\\) being zero for everyone.\nNo pooling is if we calculate every person’s effect with a regression, subtracting out he grand mean average.\nPartial pooling is in the middle, a weighted average between the two. For those with fewer trials there is less information for a particular individual, thus the complete pooling estimate will be given more weight. If someone has a lot of data, there weighted average is closer to no pooling.\nPartial pooling prevents both over and under fitting of your data, leading to increased out of sample predictions."
  },
  {
    "objectID": "mlm-2.html#basic-longitudinal-models",
    "href": "mlm-2.html#basic-longitudinal-models",
    "title": "MLM",
    "section": "Basic Longitudinal Models",
    "text": "Basic Longitudinal Models\nTo keep with the book, we are going to discuss DVs that take on different values at each timepoint t, for individual i \\({Y}_{ti}\\) Other naming schemes are equivalent such as the same \\({Y}_{ij}\\) where i’s are nested in j groups."
  },
  {
    "objectID": "mlm-2.html#empty-model",
    "href": "mlm-2.html#empty-model",
    "title": "MLM",
    "section": "Empty model",
    "text": "Empty model\nLevel 1 \\[{Y}_{ti} = \\beta_{0i}  + \\varepsilon_{ti}\\]\nLevel 2 \\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\\[{e}_{ti} \\sim \\mathcal{N}(0, \\sigma^{2})\\]\n\\[{U}_{0i} \\sim \\mathcal{N}(0, \\tau_{00}^{2})\\]"
  },
  {
    "objectID": "mlm-2.html#what-does-this-look-like",
    "href": "mlm-2.html#what-does-this-look-like",
    "title": "MLM",
    "section": "What does this look like?",
    "text": "What does this look like?"
  },
  {
    "objectID": "mlm-2.html#combined-equation",
    "href": "mlm-2.html#combined-equation",
    "title": "MLM",
    "section": "combined equation",
    "text": "combined equation\n\\[{Y}_{ti} = \\gamma_{00} + U_{0i}  + \\varepsilon_{ti}\\]\nAkin to ANOVA if we treat \\(U_{0i}\\) as between subjects variance & \\(\\varepsilon_{ti}\\) as within subjects variance.\n\n\\(\\gamma_{00}\\) is fixed or constant across people\n\\(U_{0i}\\) is random or varies across people"
  },
  {
    "objectID": "mlm-2.html#icc",
    "href": "mlm-2.html#icc",
    "title": "MLM",
    "section": "ICC",
    "text": "ICC\nBetween version variance over total variance. If the ICC is greater than zero, we are breaking standard regression assumptions as this is an index of dependency.\n\\[\\frac{U_{0i}}{U_{0i}+ \\varepsilon_{ti}}\\]\nICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals)."
  },
  {
    "objectID": "mlm-2.html#add-example",
    "href": "mlm-2.html#add-example",
    "title": "MLM",
    "section": "ADD EXAMPLE",
    "text": "ADD EXAMPLE"
  },
  {
    "objectID": "mlm-2.html#what-do-the-random-effects-look-like",
    "href": "mlm-2.html#what-do-the-random-effects-look-like",
    "title": "MLM",
    "section": "what do the random effects look like?",
    "text": "what do the random effects look like?\n\n\nCode\nlibrary(modelbased)\nrandom <- estimate_grouplevel(mod.1) \nhead(random)\n\n\nGroup | Level |   Parameter | Coefficient |   SE |        95% CI\n----------------------------------------------------------------\nID    |   901 | (Intercept) |        0.14 | 0.24 | [-0.33, 0.61]\nID    |  2301 | (Intercept) |        0.33 | 0.24 | [-0.14, 0.80]\nID    |  2302 | (Intercept) |        0.27 | 0.24 | [-0.20, 0.74]\nID    |  4701 | (Intercept) |       -0.04 | 0.27 | [-0.57, 0.48]\nID    |  4901 | (Intercept) |        0.01 | 0.24 | [-0.46, 0.48]\nID    |  5201 | (Intercept) |        0.59 | 0.24 | [ 0.12, 1.06]"
  },
  {
    "objectID": "mlm-2.html#base-r-dealing-with-random-effects",
    "href": "mlm-2.html#base-r-dealing-with-random-effects",
    "title": "MLM",
    "section": "base r dealing with random effects",
    "text": "base r dealing with random effects\ncoef = fixef + raneff\n\nglimpse(ranef(mod.1))\n\nList of 1\n $ ID:'data.frame': 8565 obs. of  1 variable:\n  ..$ (Intercept): num [1:8565] 0.1388 0.3318 0.2675 -0.0448 0.0101 ...\n  ..- attr(*, \"postVar\")= num [1, 1, 1:8565] 0.0581 0.0581 0.0581 0.0719 0.0581 ...\n - attr(*, \"class\")= chr \"ranef.mer\"\n\n\n\nglimpse(coef(mod.1))\n\nList of 1\n $ ID:'data.frame': 8565 obs. of  1 variable:\n  ..$ (Intercept): num [1:8565] 4.9 5.09 5.03 4.72 4.77 ...\n - attr(*, \"class\")= chr \"coef.mer\"\n\n\n\nfixef(mod.1)\n\n(Intercept) \n   4.760256"
  },
  {
    "objectID": "mlm-2.html#residuals",
    "href": "mlm-2.html#residuals",
    "title": "MLM",
    "section": "Residuals",
    "text": "Residuals\nTo get residuals and fitted scores\n\n\nCode\nlibrary(broom.mixed)\nexample.aug<- augment(mod.1, data = b5_long)\n\nexample.aug\n\n\n# A tibble: 22,081 × 21\n   Procedural__SID year    DOB   Sex     A     C     E     N     O ID    .fitted\n             <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <fct>   <dbl>\n 1             901 2005   1951     2  4.67  5     5     4     4    901      4.90\n 2             901 2009   1951     2  4.33  5     5     4.67  4    901      4.90\n 3             901 2013   1951     2  4.67  5     5     4.33  3.33 901      4.90\n 4            2301 2005   1946     1  5.67  5.67  5     4.33  4.33 2301     5.09\n 5            2301 2009   1946     1  5.67  5     4     5.33  5    2301     5.09\n 6            2301 2013   1946     1  4.67  5.33  5.33  3.67  5    2301     5.09\n 7            2302 2005   1946     2  4.67  4.67  4.67  3.67  5    2302     5.03\n 8            2302 2009   1946     2  4     6     5.33  3     4.33 2302     5.03\n 9            2302 2013   1946     2  3.33  5     4.67  3     5.67 2302     5.03\n10            4701 2005   1919     2  4     4.33  4.67  3.67  3    4701     4.72\n# … with 22,071 more rows, and 10 more variables: .resid <dbl>, .hat <dbl>,\n#   .cooksd <dbl>, .fixed <dbl>, .mu <dbl>, .offset <dbl>, .sqrtXwt <dbl>,\n#   .sqrtrwt <dbl>, .weights <dbl>, .wtres <dbl>\n\n\nCode\n# .fitted    = predicted values\n# .resid    = residuals/errors\n# .fixed     = predicted values with no random effects"
  },
  {
    "objectID": "mlm-2.html#predicted-scores",
    "href": "mlm-2.html#predicted-scores",
    "title": "MLM",
    "section": "Predicted scores",
    "text": "Predicted scores\nPredictors are super important for evaluating our model as well as graphing. Lots of packages have these capabilities. My favorites are tidybayes (for brms), marginaleffects (for all), modelbased (for all) and insight both from easystats.\nThese extend the flexibility of predict (base) and similar functions from emmeans"
  },
  {
    "objectID": "mlm-2.html#a-predictor-in-level-1",
    "href": "mlm-2.html#a-predictor-in-level-1",
    "title": "MLM",
    "section": "A predictor in level 1",
    "text": "A predictor in level 1\nLevel 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. People will be our cluster and observations are level 1.\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nNotice on the subscript of X that these predictors vary across cluster (i) and within the cluster (t) So if your clustering (i) is people, then t refers to different observations."
  },
  {
    "objectID": "mlm-2.html#person-predictions",
    "href": "mlm-2.html#person-predictions",
    "title": "MLM",
    "section": "person predictions",
    "text": "person predictions\nCan think of a persons score divided up into a fixed component as well as the random component.\n\\[{\\beta}_{1.26} = \\gamma_{10} \\pm U_{26}\\] Also call BLUPs or empirical bayes estimates"
  },
  {
    "objectID": "mlm-2.html#longidutidnal-example",
    "href": "mlm-2.html#longidutidnal-example",
    "title": "MLM",
    "section": "LONGIDUTIDNAL EXAMPLE",
    "text": "LONGIDUTIDNAL EXAMPLE\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: SMN7 ~ 1 + year + (1 | ID)\n   Data: example\n\nREML criterion at convergence: -675.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2308 -0.4868 -0.0377  0.4542  3.2337 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.001815 0.04261 \n Residual             0.001300 0.03606 \nNumber of obs: 216, groups:  ID, 88\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.104041   0.005733  18.147\nyear        0.001331   0.001755   0.758\n\nCorrelation of Fixed Effects:\n     (Intr)\nyear -0.426"
  },
  {
    "objectID": "mlm-2.html#error-structure",
    "href": "mlm-2.html#error-structure",
    "title": "MLM",
    "section": "Error Structure",
    "text": "Error Structure\nThe residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.\nG matrix (books term) \\[\\begin{pmatrix} {U}_{0j} \\\\ {U}_{1j} \\end{pmatrix}\n\\sim \\mathcal{N} \\begin{pmatrix}\n  0,      \\tau_{00}^{2} & \\tau_{01}\\\\\n  0,  \\tau_{01} & \\tau_{10}^{2}\n\\end{pmatrix}\\]\n\\[{e}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})\\] The \\({e}_{ij}\\) is structured as a NxN matrix, where n reflect number of waves\nNote that it is possible to impose a different error structure depending on your needs."
  },
  {
    "objectID": "mlm-2.html#decomposing-variance-for-random-intercept-model",
    "href": "mlm-2.html#decomposing-variance-for-random-intercept-model",
    "title": "MLM",
    "section": "Decomposing variance for random intercept model",
    "text": "Decomposing variance for random intercept model\n\\[\\text{Total variance CS} = \\begin{pmatrix}\n       \\tau_{00}^{2} + \\sigma^{2}& \\tau_{00}^{2} & \\tau_{00}^{2}\\\\\n       \\tau_{00}^{2} &  \\tau_{00}^{2} + \\sigma^{2} &  \\tau_{00}^{2}\\\\\n       \\tau_{00}^{2} & \\tau_{00}^{2} &   \\tau_{00}^{2} + \\sigma^{2}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "mlm-2.html#error-assumptions",
    "href": "mlm-2.html#error-assumptions",
    "title": "MLM",
    "section": "Error assumptions",
    "text": "Error assumptions\nLevel 1 residuals are independent for Level 1 units across people\nAND\nLevel 1 residuals are independent of random effects\nAND\nLevel 1 residuals are the same magnitude across people\nWe can modify a standard assumption: Level 1 residuals are independent within a person through different variance/covariance structures"
  },
  {
    "objectID": "mlm-2.html#centering",
    "href": "mlm-2.html#centering",
    "title": "MLM",
    "section": "centering",
    "text": "centering\nBecause mlms are regressions, and because mlms involve interactions, it is important to consider how your predictors zero point is defined.\nHow do you want your intercept interpreted? How do you want lower order terms in an interaction interpreted?\nWe will use these extensively to help disentangle within and between person variance."
  },
  {
    "objectID": "mlm-2.html#uncentered",
    "href": "mlm-2.html#uncentered",
    "title": "MLM",
    "section": "Uncentered",
    "text": "Uncentered\nThe default will give you predicted score of intercept when all predictors are zero.\nBecause most models will have a random intercept, it is important to keep in mind interpretations as we will be looking at variations around this value."
  },
  {
    "objectID": "mlm-2.html#grand-mean-centered",
    "href": "mlm-2.html#grand-mean-centered",
    "title": "MLM",
    "section": "Grand mean Centered",
    "text": "Grand mean Centered\nZero now represents that grand mean of the sample. Calculated by taking \\(x_{ti} - \\bar{x}\\)\nUseful as this is often our the default in other methods. Changes meaning of intercept but not slope.\nA related way to center is group grand mean centering where you take the mean of your grouping variables rather than the grand mean."
  },
  {
    "objectID": "mlm-2.html#group-mean-centering-person-centering",
    "href": "mlm-2.html#group-mean-centering-person-centering",
    "title": "MLM",
    "section": "group mean centering (person centering)",
    "text": "group mean centering (person centering)\nCalculated by taking \\(x_{ti} - \\bar{x_i}\\)\nCan change meaning of intercept and slope. Intercept is now a person’s average level rather than the samples average level (grand mean) and level when predictors = 0 (no centering)\nSlope at level 1 is the expected change relative to a person’s average."
  },
  {
    "objectID": "mlm-2.html#estimation",
    "href": "mlm-2.html#estimation",
    "title": "MLM",
    "section": "Estimation",
    "text": "Estimation\nWe need to identify: 1. the estimates of each parameter 2. some measure of precision of that estimate (SEs) 3. an index of overall model fit (deviance/-2LL/aic/bic)\nWe will use maximum likelihood (and variants of) as well as MCMC (Bayesian) for estimation.\nModel comparison is usually done through a likelihood ratio test distributed as a chi square."
  },
  {
    "objectID": "mlm-2.html#ml-vs-reml",
    "href": "mlm-2.html#ml-vs-reml",
    "title": "MLM",
    "section": "ML vs REML",
    "text": "ML vs REML\nREML = Restricted maximum likelihood\nSimilar to sample vs population estimates of SD where we do or don’t divide by n-1, ML downward biased random effect estimates.\nREML maximizes the likelihood of the residuals, so models with different fixed effects are not on the same scale and are not comparable. As a result, you cannot compare fixed models with likleihood metrics (aic) with REML. You can compare variance differences."
  },
  {
    "objectID": "mlm-2.html#transitioning-to-longitudinal-applications",
    "href": "mlm-2.html#transitioning-to-longitudinal-applications",
    "title": "MLM",
    "section": "transitioning to longitudinal applications",
    "text": "transitioning to longitudinal applications\nWe are going to fit a simple longitudinal model: a growth model. Growth model is just a fancy term for including TIME as our level 1 predictor where we are now creating lines for each person."
  },
  {
    "objectID": "mlm-2.html#within-person-empty-model",
    "href": "mlm-2.html#within-person-empty-model",
    "title": "MLM",
    "section": "within person empty model",
    "text": "within person empty model\n\\[{Y}_{ti} = \\gamma_{00} + U_{0i}  + \\varepsilon_{ti}\\]\nThis model is helpful in producing the simplest longitudinal model, one where it states: there is an average value \\(\\gamma_{00}\\) that people differ along \\(U_{0i}\\) . Because time is not in the model it assumes people do not change. \\(\\varepsilon_{ti}\\) reflects variation around each person’s predicted score ( \\(\\gamma_{00} + U_{0i}\\) )."
  },
  {
    "objectID": "mlm-2.html#adding-time",
    "href": "mlm-2.html#adding-time",
    "title": "MLM",
    "section": "Adding time",
    "text": "Adding time\nLevel 1:\n\\[{Y}_{it} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10}\\]\nAny time we have a level 1 variable we can model it at level 2. If we want to assume it is constant across people, we do not include a random effect."
  },
  {
    "objectID": "mlm-2.html#random-slope",
    "href": "mlm-2.html#random-slope",
    "title": "MLM",
    "section": "random slope",
    "text": "random slope\nLevel 1: \\[{Y}_{it} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10} + U_{1i}\\]\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{10} (X_{1i})+ U_{0i}  + U_{1i}(X_{1i}) + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "mlm-2.html#correlations-among-random-effects",
    "href": "mlm-2.html#correlations-among-random-effects",
    "title": "MLM",
    "section": "correlations among random effects",
    "text": "correlations among random effects\n\\[\\begin{pmatrix} {U}_{0i} \\\\ {U}_{1i} \\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} 0, & \\tau_{00}^{2} & \\tau_{01}\\\\ 0, & \\tau_{01} & \\tau_{10}^{2} \\end{pmatrix}\\]\nThe variances and the covariation (correlations) can be of substantive interest. What do each of these terms reflect? What if one of the terms was zero, what would that mean?"
  },
  {
    "objectID": "mlm-2.html#residual",
    "href": "mlm-2.html#residual",
    "title": "MLM",
    "section": "residual",
    "text": "residual\n\\[ {\\varepsilon}_{ti} \\sim \\mathcal{N}(0, \\sigma^{2})  \\] Much like in normal regression models we often use \\(\\sigma^{2}\\) as a means to describe the fit of the model"
  },
  {
    "objectID": "mlm-2.html#model-comparisons",
    "href": "mlm-2.html#model-comparisons",
    "title": "MLM",
    "section": "model comparisons",
    "text": "model comparisons\nIn setting up the basic growth model we have a series of questions to address:\n\nDo we need to add a time component?\nIf so, do we need to allow that to vary across people?\nif so, do we want to allow the intercept to correlate with the slope?\n\nUsually 1 & 2 are explicitly tested whereas 3 is more theoretical"
  },
  {
    "objectID": "mlm-2.html#centering-redux",
    "href": "mlm-2.html#centering-redux",
    "title": "MLM",
    "section": "centering redux",
    "text": "centering redux\nThe correlation among random intercept and slopes is directly related to centering of variables. The two standard choices for time is to center at the mean of time or at the start of time. Both have their pros and cons."
  },
  {
    "objectID": "mlm-2.html#gsoep-example",
    "href": "mlm-2.html#gsoep-example",
    "title": "MLM",
    "section": "GSOEP EXAMPLE",
    "text": "GSOEP EXAMPLE\n\n\n\n\nstr(b5_soep_long)\n\ntibble [151,186 × 6] (S3: tbl_df/tbl/data.frame)\n $ Procedural__SID: num [1:151186] 901 901 901 901 901 901 901 901 901 901 ...\n $ trait          : chr [1:151186] \"A\" \"A\" \"A\" \"C\" ...\n $ year           : chr [1:151186] \"2005\" \"2009\" \"2013\" \"2005\" ...\n $ value          : num [1:151186] 4.67 4.33 4.67 5 5 ...\n $ DOB            : num [1:151186] 1951 1951 1951 1951 1951 ...\n $ Sex            : num [1:151186] 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\nCode\nb5_long <- b5_soep_long %>% \n  pivot_wider(names_from = trait, values_from = value) %>% \n  mutate(ID = Procedural__SID) %>% \n  mutate(ID = as.factor(ID))\n\n\n\n\n\n\nstr(b5_long)\n\ntibble [22,098 × 10] (S3: tbl_df/tbl/data.frame)\n $ Procedural__SID: num [1:22098] 901 901 901 2301 2301 ...\n $ year           : chr [1:22098] \"2005\" \"2009\" \"2013\" \"2005\" ...\n $ DOB            : num [1:22098] 1951 1951 1951 1946 1946 ...\n $ Sex            : num [1:22098] 2 2 2 1 1 1 2 2 2 2 ...\n $ A              : num [1:22098] 4.67 4.33 4.67 5.67 5.67 ...\n $ C              : num [1:22098] 5 5 5 5.67 5 ...\n $ E              : num [1:22098] 5 5 5 5 4 ...\n $ N              : num [1:22098] 4 4.67 4.33 4.33 5.33 ...\n $ O              : num [1:22098] 4 4 3.33 4.33 5 ...\n $ ID             : Factor w/ 16719 levels \"901\",\"1202\",\"2301\",..: 1 1 1 3 3 3 4 4 4 7 ..."
  },
  {
    "objectID": "mlm-2.html#slope-example",
    "href": "mlm-2.html#slope-example",
    "title": "MLM",
    "section": "slope example",
    "text": "slope example\n\nmod.2 <- lmer(C ~ 1 + year + (1 | ID), data=b5_long)\n\nsummary(mod.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year + (1 | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42793.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2995 -0.4572  0.0548  0.4152  4.7573 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.1378   0.3712  \n Residual             0.3009   0.5485  \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.752544   0.007566 628.165\nyear2009    0.009782   0.008756   1.117\nyear2013    0.014584   0.009773   1.492\n\nCorrelation of Fixed Effects:\n         (Intr) yr2009\nyear2009 -0.621       \nyear2013 -0.565  0.488"
  },
  {
    "objectID": "mlm-2.html#visualizing-results-quickly",
    "href": "mlm-2.html#visualizing-results-quickly",
    "title": "MLM",
    "section": "Visualizing results (quickly)",
    "text": "Visualizing results (quickly)\nThere are many ways to do this. The parameters package paired with the see package, both from the easystats package, are useful in this regard.\n\nlibrary(parameters)\nlibrary(see)\n\nresult <- model_parameters(mod.4)"
  },
  {
    "objectID": "mlm-2.html#including-a-random-slope",
    "href": "mlm-2.html#including-a-random-slope",
    "title": "MLM",
    "section": "Including a random slope",
    "text": "Including a random slope\nlevel 1 \\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nlevel 2 \\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10}+ U_{1i}\\] combined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{10}(X_{ti})+ U_{0i} + U_{1i}(X_{ti}) + \\varepsilon_{ti}\\]\n\\[{Y}_{ti} = (\\gamma_{00} + U_{0i}) + (\\gamma_{10}+  U_{1i})X_{ti} + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "mlm-2.html#adding-a-random-slope",
    "href": "mlm-2.html#adding-a-random-slope",
    "title": "MLM",
    "section": "Adding a random slope",
    "text": "Adding a random slope\nLevel 1: \\[{Y}_{it} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\n\\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10} + U_{1i}\\]\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{10} (X_{1i})+ U_{0i}  + U_{1i}(X_{1i}) + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "mlm-2.html#as-starting",
    "href": "mlm-2.html#as-starting",
    "title": "MLM",
    "section": "0 as starting",
    "text": "0 as starting\n\nb5_long<- b5_long %>%  \n  mutate(year.c = (year - 2005))\n  \n  \nmod.6 <- lmer(C ~ 1 + year.c + (1 + year.c  | ID), data=b5_long)\n\nsummary(mod.6)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year.c + (1 + year.c | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42714\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0083 -0.4685  0.0461  0.4376  4.8649 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n ID       (Intercept) 0.1050567 0.32412      \n          year.c      0.0002853 0.01689  0.75\n Residual             0.2966451 0.54465      \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.753235   0.006765  702.59\nyear.c      0.001783   0.001230    1.45\n\nCorrelation of Fixed Effects:\n       (Intr)\nyear.c -0.591\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00437407 (tol = 0.002, component 1)"
  },
  {
    "objectID": "mlm-2.html#as-mid",
    "href": "mlm-2.html#as-mid",
    "title": "MLM",
    "section": "0 as mid",
    "text": "0 as mid\n\nb5_long<- b5_long %>%  \n  mutate(year.cM = (year - 2009))\n  \n  \nmod.7 <- lmer(C ~ 1 + year.cM + (1 + year.cM | ID), data=b5_long)\n\nsummary(mod.7)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year.cM + (1 + year.cM | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42714\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0085 -0.4685  0.0461  0.4376  4.8649 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n ID       (Intercept) 0.1426960 0.37775      \n          year.cM     0.0002854 0.01689  0.83\n Residual             0.2966298 0.54464      \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.760368   0.005535  860.07\nyear.cM     0.001783   0.001230    1.45\n\nCorrelation of Fixed Effects:\n        (Intr)\nyear.cM 0.166 \noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00601838 (tol = 0.002, component 1)"
  },
  {
    "objectID": "mlm-2.html#testing-significance",
    "href": "mlm-2.html#testing-significance",
    "title": "MLM",
    "section": "Testing significance",
    "text": "Testing significance\nMethods for testing single parameters From worst to best:\n\nWald Z-tests. Easy to compute. However, they are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.\nWald t-tests\nLikelihood ratio test.\n\nMarkov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals"
  },
  {
    "objectID": "mlm-2.html#likelhiood-ratio-test",
    "href": "mlm-2.html#likelhiood-ratio-test",
    "title": "MLM",
    "section": "Likelhiood ratio test",
    "text": "Likelhiood ratio test\nHow much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).\nLog Likelihood (LL) is derived from ML estimation. Larger the LL the better the fit. Deviance compares two LLs. Current model and a saturated model (that fits data perfectly).\nDeviance = -2[LL current - LL saturated]\nLL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out.\nDeviance = -2LL current model."
  },
  {
    "objectID": "mlm-2.html#likelhiood-ratio-test-1",
    "href": "mlm-2.html#likelhiood-ratio-test-1",
    "title": "MLM",
    "section": "Likelhiood ratio test",
    "text": "Likelhiood ratio test\nComparing 2 models is called a likelihood ration test. Need to have: 1. same data 2. nested models (think of constraining a parameter to zero)\nDistributed as chi-square with df equal to constraint differences between models.\n\n\nData: b5_long\nModels:\nmod.1: C ~ 1 + (1 | ID)\nmod.2: C ~ 1 + year + (1 | ID)\n      npar   AIC   BIC logLik deviance  Chisq Df Pr(>Chisq)\nmod.1    3 42778 42802 -21386    42772                     \nmod.2    5 42780 42820 -21385    42770 2.4256  2     0.2974"
  },
  {
    "objectID": "mlm-2.html#other-types-of-models",
    "href": "mlm-2.html#other-types-of-models",
    "title": "MLM",
    "section": "Other types of models",
    "text": "Other types of models\nDepending on your DV, you might not want to have a Gaussian sampling distribution. Instead you may want something like a Poisson or a negative binomial if you are using some sort of count data. You can do this somewhat with lme4. However, the BRMS package – which uses Bayesian estimation – has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. Maybe we will fit some of these later in the semester."
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "homework",
    "section": "",
    "text": "Note that we are looking at depression across 5 waves during college. Waves are signified by a letter at the start of the variable name, starting with A, then B, then C, etcetera. Not all waves collected depression during college. The scale is called the CED, and is an already calculated composite measure.\nPlease email your answers to me (j.jackson@wustl.edu) that includes a) an Rmd file, and b) a pdf/html of your output.\n\nHow many observations are there? How many subjects? How many subjects have more than 3 observations? Take the wide depression dataset and turn it long format to run subsequent models.\nRun separate linear models on all of the participants subjects (a basic regression) with time as a predictor. What is the average intercept, the average slope? How would you interpret each of these?\nNow run a mlm model with only a random intercept. What is the ICC? Interpret.\nIntroduce time as a fixed term to the mlm model. Interpret your findings. What are there differences (and why do they exist?) in this model vs the models you ran in #2\nRun an additional model with a random slope. How does this change compare to the previous fixed slope model? Should you keep the random slope or not?\nInterpret the correlation between the slope and the intercept.\nCreate a density plot of the random effects from your final model.\nPlot the predicted fixed effects along with the random effects (Eg individual slopes) on the same plot."
  },
  {
    "objectID": "sem-3.html#longitudinal-structural-equation-modeling",
    "href": "sem-3.html#longitudinal-structural-equation-modeling",
    "title": "sem",
    "section": "Longitudinal Structural Equation Modeling",
    "text": "Longitudinal Structural Equation Modeling\n\n\n\nSEM is the broader umbrella from the GLM. With it we are able to do two interesting this:\n\nFit a latent measurement model (e.g., CFA)\nFit a structural model (e.g,. path analysis)\n\nThese two components allow us to address more difficult research questions involving but not limited to: multiple DVs, mediators, varying effects across time, unmeasured variables, constraints, and measurement invariance."
  },
  {
    "objectID": "sem-3.html#sem-terminology",
    "href": "sem-3.html#sem-terminology",
    "title": "sem",
    "section": "SEM terminology",
    "text": "SEM terminology\n\nIndicators or items or manifest variables\n\nRepresented by squares in path diagrams\n\nLatent or unobserved\n\nRepresented by circles"
  },
  {
    "objectID": "sem-3.html#path-diagrams",
    "href": "sem-3.html#path-diagrams",
    "title": "sem",
    "section": "Path diagrams",
    "text": "Path diagrams\nCircles = latent variables\nBoxes = observed indicator variables\nTwo headed arrows = correlations/covariances/variances\nSingle head arrows = regressions\nTriangle = means"
  },
  {
    "objectID": "sem-3.html#classical-test-theory-interpretation",
    "href": "sem-3.html#classical-test-theory-interpretation",
    "title": "sem",
    "section": "Classical test theory interpretation",
    "text": "Classical test theory interpretation\n\nLatent construct = what the indicators share in common\nIndicators represent the sum of True Score variance + Item specific variance + Random error\nVariance of the latent variable represents the amount of common information. Variance represents that meaningful differences btwn people.\nThe residual errors (disturbances) represent the amount of information unique to each indicator. A combination of error and item-specific variance.\nThe extent of the connection between the latent variable and the indicators is represented as a factor loading."
  },
  {
    "objectID": "sem-3.html#generizability-interpretation-of-latent-variables",
    "href": "sem-3.html#generizability-interpretation-of-latent-variables",
    "title": "sem",
    "section": "Generizability interpretation of latent variables",
    "text": "Generizability interpretation of latent variables\nTrue score variance can be thought of as consisting as a combination of:\n\nConstruct variance- this is the truest true score variance.\nMethod variance- see Campbell and Fiske or sludge factor of Meehl.\nOccasion/time specific- important for longitudinal, aging, and cohort analyses–and for this class.\n\n\nFor longitudinal models, occasion specific variance can lead to biased estimates. We want to separate the occasion variance from the overall construct variance."
  },
  {
    "objectID": "sem-3.html#formative-indicators",
    "href": "sem-3.html#formative-indicators",
    "title": "sem",
    "section": "Formative indicators",
    "text": "Formative indicators\n\nThese pretty pictures imply that the latent variables “cause” the indicators. This is the standard view and are referred to as reflexive indicators.\nThere is another approach, formative indicators, where indicators “cause” the latent variable. It is not real, only a combination of variables.\nAn example of this is SES. SES does not ‘exist’ but is a socially constructed idea."
  },
  {
    "objectID": "sem-3.html#measurement-error",
    "href": "sem-3.html#measurement-error",
    "title": "sem",
    "section": "Measurement error",
    "text": "Measurement error\n\nA major advantage of SEM is that each latent variable does not contain measurement error. It is as is if we measured our variable with an alpha = 1.\nGets us closer to the population model, which could yield higher R2 and better parameter estimates.\nCaptures what is shared among the indicators. The measurement error associated with each indicator is uncorrelated with the latent variable. Compare with composite approach.\n“Theoretically error free”. The latent variable is not only filled with true score variance (see above re: occasion and method variance). Unless you have multiple methods and occasions it is hard to parse them apart."
  },
  {
    "objectID": "sem-3.html#regarding-means",
    "href": "sem-3.html#regarding-means",
    "title": "sem",
    "section": "Regarding means",
    "text": "Regarding means\n\nSEM is also known as covariance structure analysis. You can do SEM using only variance-covariance matrices. I.e means are not necessary.\n\nThis is cool because you can technically reproduce the analyses of a paper if they give you a correlation matrix of study variables.\nGiven we are interested in change across time,we will be interested in means. Latent variables by themselves do not have any inherent metric, it is up to us to choose the scale they are on. We can standardize them, use the original metric, and more!"
  },
  {
    "objectID": "sem-3.html#path-model",
    "href": "sem-3.html#path-model",
    "title": "sem",
    "section": "Path model",
    "text": "Path model\n\nThe path model component can be in addition to a measurement model or separate from them.\nYou have already worked with path models as a simple regression is a path model, so is a standard mediation.\nYou can make the path models more complex than these though, by specifying relationships among many variables."
  },
  {
    "objectID": "sem-3.html#path-model-with-measurement-model",
    "href": "sem-3.html#path-model-with-measurement-model",
    "title": "sem",
    "section": "path model with measurement model",
    "text": "path model with measurement model\n\n\nCode\nmod.2 <- 'X =~ x1 + x2 + x3\nM =~ x4 + x5 + x6\nY =~ x7 + x8 + x9\n\nY ~ M + X\nM ~ X'\n\nfit.2 <- cfa(mod.2, data=HolzingerSwineford1939)\nsemPaths(fit.2)"
  },
  {
    "objectID": "sem-3.html#estimating-an-sem-model",
    "href": "sem-3.html#estimating-an-sem-model",
    "title": "sem",
    "section": "Estimating an SEM model",
    "text": "Estimating an SEM model\n\nCompare our model implied associations to actual associations.\nIn addition to setting the measurement model and paths we may want to put apriori constraining parameters (variances/covariances/regressions) to reflect how we think variables are related.\nThen we use or ML algorithm to get our model implied covariances/means as close as possible to the observed covariances/means."
  },
  {
    "objectID": "sem-3.html#setting-the-scale-and-defining-latent-variables",
    "href": "sem-3.html#setting-the-scale-and-defining-latent-variables",
    "title": "sem",
    "section": "Setting the scale and defining latent variables",
    "text": "Setting the scale and defining latent variables\nWe are trying to measure clouds. Need to define the scale of a latent variable because there is no inherent scale of measurement. Largely irrelevant as to what scale is chosen just as centering or standardizing yield no substantive changes. Instead, scaling serves to establish a point of reference so as to interpret other parameters.\n3 options:\n\nFixed factor. Here you fix the variance of the latent variable to 1 (standardized).\nMarker variable. Here you fix one factor loading to 1. All other loadings are relative to this loading. The variance of the latent variable can thus be anything. This is often the default of software programs.\nEffect coding. Here you constrain loading to average to 1. This will be helpful for us as we can then put the scale of measurement into our original metric. For longitudinal models this is helpful in terms of how to interpret the amount of change.\n\nKnowns - unknowns = df. Note that df in this case df will not directly relate to sample size, so it is a little different than typical degree of freedom concepts."
  },
  {
    "objectID": "sem-3.html#identification",
    "href": "sem-3.html#identification",
    "title": "sem",
    "section": "Identification",
    "text": "Identification\n\nWe have multiple parameters we are trying to estimate (Paths, means, variances, residuals. Cannot have more unknowns than knowns.\nIf you are asking too much you canconstrain parameters to be the same, which reduces the number of parameters.\nCompare the number of knowns (variances and covariances) to the unknowns (model parameters). For example, a three indicator latent variable has 7 unknowns. 3 Loadings, 3 error variances and the variance of the latent variable\nThe covariance matrix has 6 data points. Thus we need to add in one more known, in this case a fixed factor or a marker variable."
  },
  {
    "objectID": "sem-3.html#types-of-identification",
    "href": "sem-3.html#types-of-identification",
    "title": "sem",
    "section": "Types of identification",
    "text": "Types of identification\n\nJust identified is where the number of knowns equal unknowns. Also known as saturated model.\n\n\nWhen you evaluate the fit of the model these will be perfect. So while these will estimate, we cannot examine whether or not our model is a good representation of the world, as we are simply recreating the observed covariance matrix (data).\nKnowns - unknowns = df. Note that df in this case df will not directly relate to sample size, so it is a little different than typical degree of freedom concepts."
  },
  {
    "objectID": "sem-3.html#fit-indices",
    "href": "sem-3.html#fit-indices",
    "title": "sem",
    "section": "Fit Indices",
    "text": "Fit Indices\n\nresiduals. Good to check.\nmodification indices. Check to see if missing parameters that residuals may suggest you didn’t include or should include.\nchi-square. (Statistical fit) Implied versus observed data, tests to see if model are exact fit with data.\nRMSEA or SRMR (Absolute fit). Judges distance from perfect fit. Above .10 poor fit Below .08 acceptable\nCFI, TFI (Relative fit models). Compares relative to a null model. Null models have no covariance among observed and latent variables. Range from 0-1. Indicate % of improvement from the null model to a saturated i.e. just identified model. Usually >.9 is okay. Some care about > .95"
  },
  {
    "objectID": "sem-3.html#parcels",
    "href": "sem-3.html#parcels",
    "title": "sem",
    "section": "Parcels",
    "text": "Parcels\n\nIt is often necessary to simplify your model. Parcels combine indicators into a composite.\nBenefits in terms of the assumptions of the indicator variables (multivariate normal).\nYou can combine items however you want into 3 or 4 parcels. You may balance highly loading with less highly loading items (item to construct technique) or you may pair pos and neg keyed items together.\nSome dislike parcels because you are assuming each indicator is exchangeable."
  },
  {
    "objectID": "sem-3.html#setting-the-scale",
    "href": "sem-3.html#setting-the-scale",
    "title": "sem",
    "section": "Setting the scale",
    "text": "Setting the scale\n\nWe are trying to measure clouds. Need to define the scale of a latent variable because there is no inherent scale of measurement.\nLargely irrelevant as to what scale is chosen just as centering or standardizing yield no substantive changes.\nInstead, scaling serves to establish a point of reference so as to interpret other parameters."
  },
  {
    "objectID": "sem-3.html#types-of-identification-1",
    "href": "sem-3.html#types-of-identification-1",
    "title": "sem",
    "section": "Types of identification",
    "text": "Types of identification\n\nOver identified is when you have more knowns than unknowns. This is good as we can fit a model that is more parsimonious than our data. Moreover, we can examine fit stats.\nUnder identified is when you have problems and have more unknowns than knowns. this is because there is more than one solution available and the algorithm cannot decide e.g,. 2 + X = Y. If we add a constraint or a known value then it becomes manageable 2 + X = 12"
  },
  {
    "objectID": "sem-3.html#types-of-longitudinal-sem-models",
    "href": "sem-3.html#types-of-longitudinal-sem-models",
    "title": "sem",
    "section": "Types of longitudinal SEM models",
    "text": "Types of longitudinal SEM models\n\nGrowth models\nLongitudinal CFA\nCross lag panel model\nLongitudinal mediation\nGrowth models + cross lags\nLatent change/difference score models\nMixture or class based longitudinal models"
  },
  {
    "objectID": "sem-3.html#lavaan",
    "href": "sem-3.html#lavaan",
    "title": "sem",
    "section": "lavaan",
    "text": "lavaan\n\nlibrary(lavaan)\nwide <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/longitudinal.csv\")\n\nsummary(wide)\n\n    PosAFF11        PosAFF21         PosAFF31        NegAFF11      \n Min.   :1.365   Min.   :0.4152   Min.   :1.140   Min.   :-0.8584  \n 1st Qu.:2.739   1st Qu.:2.6343   1st Qu.:2.797   1st Qu.: 1.1035  \n Median :3.209   Median :3.1143   Median :3.204   Median : 1.5075  \n Mean   :3.212   Mean   :3.1050   Mean   :3.248   Mean   : 1.5220  \n 3rd Qu.:3.688   3rd Qu.:3.6216   3rd Qu.:3.775   3rd Qu.: 1.9815  \n Max.   :5.804   Max.   :6.1970   Max.   :6.048   Max.   : 3.2403  \n    NegAFF21          NegAFF31          PosAFF12        PosAFF22     \n Min.   :-0.3991   Min.   :-0.5606   Min.   :1.528   Min.   :0.6575  \n 1st Qu.: 1.0229   1st Qu.: 1.0100   1st Qu.:2.852   1st Qu.:2.6571  \n Median : 1.3718   Median : 1.4335   Median :3.215   Median :3.1206  \n Mean   : 1.3971   Mean   : 1.3981   Mean   :3.253   Mean   :3.1256  \n 3rd Qu.: 1.7566   3rd Qu.: 1.8101   3rd Qu.:3.637   3rd Qu.:3.5467  \n Max.   : 2.9844   Max.   : 2.7674   Max.   :5.413   Max.   :5.4420  \n    PosAFF32         NegAFF12         NegAFF22         NegAFF32       \n Min.   :0.7369   Min.   :0.1797   Min.   :0.1784   Min.   :-0.03494  \n 1st Qu.:2.8484   1st Qu.:1.1464   1st Qu.:0.9963   1st Qu.: 1.02027  \n Median :3.2692   Median :1.3818   Median :1.3172   Median : 1.31692  \n Mean   :3.2737   Mean   :1.4115   Mean   :1.3237   Mean   : 1.30002  \n 3rd Qu.:3.7170   3rd Qu.:1.7251   3rd Qu.:1.6382   3rd Qu.: 1.56441  \n Max.   :5.9676   Max.   :2.5033   Max.   :2.5587   Max.   : 2.44236  \n    PosAFF13        PosAFF23         PosAFF33        NegAFF13       \n Min.   :1.307   Min.   :0.8057   Min.   :1.629   Min.   :-0.01837  \n 1st Qu.:2.979   1st Qu.:2.7147   1st Qu.:2.858   1st Qu.: 1.15739  \n Median :3.299   Median :3.0832   Median :3.325   Median : 1.43937  \n Mean   :3.302   Mean   :3.0945   Mean   :3.280   Mean   : 1.43015  \n 3rd Qu.:3.683   3rd Qu.:3.5296   3rd Qu.:3.698   3rd Qu.: 1.73650  \n Max.   :4.712   Max.   :4.8007   Max.   :5.014   Max.   : 2.75085  \n    NegAFF23        NegAFF33     \n Min.   :0.147   Min.   :0.3145  \n 1st Qu.:1.009   1st Qu.:1.0261  \n Median :1.294   Median :1.3154  \n Mean   :1.281   Mean   :1.2974  \n 3rd Qu.:1.560   3rd Qu.:1.5583  \n Max.   :2.447   Max.   :2.6385"
  },
  {
    "objectID": "sem-3.html#wide-data-for-sem-models.",
    "href": "sem-3.html#wide-data-for-sem-models.",
    "title": "sem",
    "section": "Wide data for SEM models.",
    "text": "Wide data for SEM models.\n\nhead(wide)\n\n  PosAFF11 PosAFF21 PosAFF31  NegAFF11  NegAFF21 NegAFF31 PosAFF12 PosAFF22\n1 3.407196 3.161366 2.691232 2.4207020 2.3531140 2.338767 3.608672 2.385396\n2 2.594691 2.682974 2.676173 1.6652403 0.8228416 1.083938 3.103346 2.666753\n3 3.168698 2.854090 3.277873 1.5646086 1.6909398 1.502032 2.327999 3.061529\n4 4.600160 4.698997 4.751315 0.9352668 0.7944903 0.205178 4.254750 3.792440\n5 1.911294 2.184561 1.661057 1.6076395 1.5046016 1.209850 3.971308 4.674580\n6 3.472354 4.016982 4.021543 1.9754108 1.1348223 1.729405 3.845904 4.186825\n  PosAFF32 NegAFF12  NegAFF22  NegAFF32 PosAFF13 PosAFF23 PosAFF33 NegAFF13\n1 3.029975 2.283468 2.3173734 2.3932485 3.561401 3.506353 3.862975 2.090484\n2 3.251786 1.651941 1.7019345 1.3462416 2.893546 2.804488 2.637762 1.257376\n3 3.272173 1.749022 1.6237575 1.5763265 2.705572 2.310391 2.841122 1.699849\n4 3.934392 1.279030 0.8741891 1.1822674 3.212826 2.775791 3.239004 1.824778\n5 4.093508 1.019751 1.0376248 0.7776716 3.487932 2.673107 3.093565 1.316975\n6 4.316398 1.189548 1.1866801 1.3158520 3.236481 3.256453 4.023383 1.053895\n   NegAFF23  NegAFF33\n1 1.3276333 1.6680488\n2 1.6585273 1.4298172\n3 1.7373582 1.8542867\n4 1.3016450 1.5356285\n5 0.7120284 0.7083461\n6 1.4449087 0.9600753\n\n\nAnyone have some comments on naming conventions for this dataset?"
  },
  {
    "objectID": "sem-3.html#growth-models",
    "href": "sem-3.html#growth-models",
    "title": "sem",
    "section": "Growth models",
    "text": "Growth models\n\nGrowth models in an SEM framework is very similar to the MLM framework.\nThe major differences is how time is treated with time variables being the same for everyone, with a variable associated with it (categorical time).\nWhereas previously we had a time variable, now we indirectly include time into our model by specifying when variables were assessed. This has the consequence of necessitating a wide format\nOther than time, the idea behind the growth model is exactly the same."
  },
  {
    "objectID": "sem-3.html#coding-time",
    "href": "sem-3.html#coding-time",
    "title": "sem",
    "section": "Coding time",
    "text": "Coding time\nLet’s use the long dataset from chapter 4 of Singer and Willet. It is a three wave longitudinal study of adolescents. We are looking at alcohol use during the previous year, measured from 0 - 7. COA is a variable indicating the child’s parents are alcoholics.\n\n\nCode\nalcohol <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/alcohol1_pp.csv\")\nhead(alcohol)\n\n\n  X id age coa male age_14   alcuse      peer      cpeer  ccoa\n1 1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2 2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3 3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4 4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5 5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6 6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549"
  },
  {
    "objectID": "sem-3.html#compare-with-mlm",
    "href": "sem-3.html#compare-with-mlm",
    "title": "sem",
    "section": "Compare with MLM",
    "text": "Compare with MLM\n\n\nCode\nlibrary(lme4)\nfit1.mlm.c <- lmer(alcuse ~  age_14 + (age_14 | id), data = alcohol)\nsummary(fit1.mlm.c)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ age_14 + (age_14 | id)\n   Data: alcohol\n\nREML criterion at convergence: 643.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 0.6355   0.7972        \n          age_14      0.1552   0.3939   -0.23\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.10573   6.160\nage_14       0.27065    0.06284   4.307\n\nCorrelation of Fixed Effects:\n       (Intr)\nage_14 -0.441"
  },
  {
    "objectID": "sem-3.html#rescale-time",
    "href": "sem-3.html#rescale-time",
    "title": "sem",
    "section": "rescale time",
    "text": "rescale time\n\n\nCode\nmodel.2 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16'\nfit.2 <- growth(model.2, data=alcohol.wide)\nsummary(fit.2)\n\n\nlavaan 0.6-11 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.636\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.425\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         0.500                           \n    alcuse_16         1.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~~                                                \n    s                -0.374    0.203   -1.841    0.066\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n    i                 0.634    0.103    6.163    0.000\n    s                 0.555    0.124    4.481    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.064    0.147    0.436    0.663\n   .alcuse_15         0.420    0.094    4.463    0.000\n   .alcuse_16         0.280    0.180    1.556    0.120\n    i                 0.807    0.193    4.177    0.000\n    s                 0.936    0.334    2.803    0.005"
  },
  {
    "objectID": "sem-3.html#constraining-slope-to-be-fixed-only",
    "href": "sem-3.html#constraining-slope-to-be-fixed-only",
    "title": "sem",
    "section": "constraining slope to be fixed only",
    "text": "constraining slope to be fixed only\nAs with MLM we have options to handle the inclusion of random effects.\n\n\nCode\nmodel.3 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16\n               s ~~0*s'\nfit.3<- growth(model.3, data=alcohol.wide)\nsummary(fit.3)\n\n\nlavaan 0.6-11 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 7.508\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         0.500                           \n    alcuse_16         1.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~~                                                \n    s                 0.083    0.114    0.731    0.465\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n    i                 0.652    0.104    6.273    0.000\n    s                 0.556    0.115    4.849    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    s                 0.000                           \n   .alcuse_14         0.416    0.100    4.172    0.000\n   .alcuse_15         0.331    0.084    3.923    0.000\n   .alcuse_16         0.692    0.138    5.003    0.000\n    i                 0.541    0.147    3.676    0.000"
  },
  {
    "objectID": "sem-3.html#what-is-measurement-invariance",
    "href": "sem-3.html#what-is-measurement-invariance",
    "title": "sem",
    "section": "What is measurement invariance?",
    "text": "What is measurement invariance?\n\nTo meaningfully look at means, we need to have the means mean the same thing.\nOtherwise, change could reflect people responding to the indicators differently. For example, a common item on an extraversion scale is “Do you like to go to parties?” This is likely interpreted differently by a 20 year old compared to a 70 year old.\nMaturation is the easiest way to see measurement differences, but it also happens when you want to compare groups. This assumption is typically never critically examined."
  },
  {
    "objectID": "sem-3.html#types-of-mi",
    "href": "sem-3.html#types-of-mi",
    "title": "sem",
    "section": "types of MI",
    "text": "types of MI\n\nConfigural (pattern). Asks: does your measure have the same factor structure? Typically always true with a decent measure of your construct. Can be tested through test statistics and eye-balling. Serves as default model to compare with our more stringent models.\nWeak (metric/loading). Can be easily met. Not meeting this shows big problems, unless you are working with a really large dataset (where there is large power to find differences)."
  },
  {
    "objectID": "sem-3.html#configural-baseline",
    "href": "sem-3.html#configural-baseline",
    "title": "sem",
    "section": "configural (baseline)",
    "text": "configural (baseline)\n\n\nCode\nconfig <- '\n## define latent variables\nPos1 =~ PosAFF11 + PosAFF21 + PosAFF31\nPos2 =~ PosAFF12 + PosAFF22 + PosAFF32\nPos3 =~ PosAFF13 + PosAFF23 + PosAFF33\n\n\n\n\n'\n\nconfig <- cfa(config, data=wide, meanstructure=TRUE, std.lv=TRUE)\n\nsummary(config, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6-11 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n                                                      \n  Number of observations                           368\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                22.648\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.541\n\nModel Test Baseline Model:\n\n  Test statistic                              2688.088\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2047.648\n  Loglikelihood unrestricted model (H1)      -2036.324\n                                                      \n  Akaike (AIC)                                4155.296\n  Bayesian (BIC)                              4272.539\n  Sample-size adjusted Bayesian (BIC)         4177.360\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.040\n  P-value RMSEA <= 0.05                          0.991\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.013\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PosAFF11          0.656    0.030   22.006    0.000    0.656    0.906\n    PosAFF21          0.652    0.031   20.887    0.000    0.652    0.876\n    PosAFF31          0.683    0.031   22.152    0.000    0.683    0.909\n  Pos2 =~                                                               \n    PosAFF12          0.554    0.026   21.138    0.000    0.554    0.881\n    PosAFF22          0.644    0.030   21.660    0.000    0.644    0.894\n    PosAFF32          0.642    0.028   23.340    0.000    0.642    0.936\n  Pos3 =~                                                               \n    PosAFF13          0.508    0.024   21.006    0.000    0.508    0.886\n    PosAFF23          0.547    0.027   20.375    0.000    0.547    0.869\n    PosAFF33          0.536    0.026   20.707    0.000    0.536    0.878\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.475    0.044   10.733    0.000    0.475    0.475\n    Pos3              0.406    0.048    8.407    0.000    0.406    0.406\n  Pos2 ~~                                                               \n    Pos3              0.458    0.046   10.023    0.000    0.458    0.458\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11          3.212    0.038   85.115    0.000    3.212    4.437\n   .PosAFF21          3.105    0.039   80.021    0.000    3.105    4.171\n   .PosAFF31          3.248    0.039   82.957    0.000    3.248    4.324\n   .PosAFF12          3.253    0.033   99.137    0.000    3.253    5.168\n   .PosAFF22          3.126    0.038   83.295    0.000    3.126    4.342\n   .PosAFF32          3.274    0.036   91.525    0.000    3.274    4.771\n   .PosAFF13          3.302    0.030  110.492    0.000    3.302    5.760\n   .PosAFF23          3.094    0.033   94.296    0.000    3.094    4.916\n   .PosAFF33          3.280    0.032  102.971    0.000    3.280    5.368\n    Pos1              0.000                               0.000    0.000\n    Pos2              0.000                               0.000    0.000\n    Pos3              0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11          0.094    0.011    8.285    0.000    0.094    0.180\n   .PosAFF21          0.129    0.013    9.818    0.000    0.129    0.232\n   .PosAFF31          0.098    0.012    8.052    0.000    0.098    0.173\n   .PosAFF12          0.089    0.009   10.022    0.000    0.089    0.224\n   .PosAFF22          0.104    0.011    9.375    0.000    0.104    0.200\n   .PosAFF32          0.059    0.009    6.557    0.000    0.059    0.125\n   .PosAFF13          0.070    0.008    8.357    0.000    0.070    0.214\n   .PosAFF23          0.097    0.011    9.204    0.000    0.097    0.245\n   .PosAFF33          0.085    0.010    8.774    0.000    0.085    0.229\n    Pos1              1.000                               1.000    1.000\n    Pos2              1.000                               1.000    1.000\n    Pos3              1.000                               1.000    1.000\n\n\nNotice that we didnt do anything different here except for fitting a mean structure and ask for fit.measures in the output."
  },
  {
    "objectID": "sem-3.html#weak-constrain-loadings",
    "href": "sem-3.html#weak-constrain-loadings",
    "title": "sem",
    "section": "Weak (constrain loadings)",
    "text": "Weak (constrain loadings)\n\nIf the values are numbers, constraints will force the parameters to be that number. If they are letter, then it names the parameter and forces all of those with the same naming scheme to be equivalent.\nWeak measurement invariance test is to constrain the item loadings to be equivalent across waves.\nHere NA is used to allow the latent variance at time 2 and 3 to be estimated, rather than to be automatically constrained to 1 (bc we used a fixed factor approach)"
  },
  {
    "objectID": "sem-3.html#strong-constrain-loadings-and-intercepts",
    "href": "sem-3.html#strong-constrain-loadings-and-intercepts",
    "title": "sem",
    "section": "Strong (constrain loadings and intercepts)",
    "text": "Strong (constrain loadings and intercepts)\n\nStrong introduces means to the equation.They were there before but now we constrain the means of each item to be the same across time\nWhy are we doing that if we want to test whether people change? Change will be reflected latently.\n\n\n\nCode\nstrong <- '\n## define latent variables\nPos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31\nPos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32\nPos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33\n\n\n## constrain intercepts across time\nPosAFF11 ~ t1*1\nPosAFF21 ~ t2*1\nPosAFF31 ~ t3*1\n\n\nPosAFF12 ~ t1*1\nPosAFF22 ~ t2*1\nPosAFF32 ~ t3*1\n\n\nPosAFF13 ~ t1*1\nPosAFF23 ~ t2*1\nPosAFF33 ~ t3*1\n\n'\n\nstrong <- cfa(strong, data=wide, meanstructure=TRUE, std.lv=TRUE)\n\nsummary(strong, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6-11 ended normally after 66 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n  Number of equality constraints                    12\n                                                      \n  Number of observations                           368\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                62.650\n  Degrees of freedom                                36\n  P-value (Chi-square)                           0.004\n\nModel Test Baseline Model:\n\n  Test statistic                              2688.088\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.990\n  Tucker-Lewis Index (TLI)                       0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2067.649\n  Loglikelihood unrestricted model (H1)      -2036.324\n                                                      \n  Akaike (AIC)                                4171.298\n  Bayesian (BIC)                              4241.643\n  Sample-size adjusted Bayesian (BIC)         4184.536\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.045\n  90 Percent confidence interval - lower         0.025\n  90 Percent confidence interval - upper         0.063\n  P-value RMSEA <= 0.05                          0.656\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.098\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PosAFF11  (L1)    0.572    0.017   33.166    0.000    0.572    0.871\n    PosAFF21  (L2)    0.616    0.019   32.764    0.000    0.616    0.868\n    PosAFF31  (L3)    0.624    0.018   34.247    0.000    0.624    0.895\n  Pos2 =~                                                               \n    PosAFF12  (L1)    0.572    0.017   33.166    0.000    0.572    0.890\n    PosAFF22  (L2)    0.616    0.019   32.764    0.000    0.616    0.883\n    PosAFF32  (L3)    0.624    0.018   34.247    0.000    0.624    0.930\n  Pos3 =~                                                               \n    PosAFF13  (L1)    0.572    0.017   33.166    0.000    0.572    0.905\n    PosAFF23  (L2)    0.616    0.019   32.764    0.000    0.616    0.891\n    PosAFF33  (L3)    0.624    0.018   34.247    0.000    0.624    0.909\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.460    0.045   10.130    0.000    0.460    0.460\n    Pos3              0.397    0.048    8.193    0.000    0.397    0.397\n  Pos2 ~~                                                               \n    Pos3              0.472    0.044   10.672    0.000    0.472    0.472\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11  (t1)    3.259    0.025  129.148    0.000    3.259    4.959\n   .PosAFF21  (t2)    3.106    0.027  113.614    0.000    3.106    4.372\n   .PosAFF31  (t3)    3.267    0.027  120.338    0.000    3.267    4.680\n   .PosAFF12  (t1)    3.259    0.025  129.148    0.000    3.259    5.070\n   .PosAFF22  (t2)    3.106    0.027  113.614    0.000    3.106    4.452\n   .PosAFF32  (t3)    3.267    0.027  120.338    0.000    3.267    4.864\n   .PosAFF13  (t1)    3.259    0.025  129.148    0.000    3.259    5.153\n   .PosAFF23  (t2)    3.106    0.027  113.614    0.000    3.106    4.490\n   .PosAFF33  (t3)    3.267    0.027  120.338    0.000    3.267    4.755\n    Pos1              0.000                               0.000    0.000\n    Pos2              0.000                               0.000    0.000\n    Pos3              0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11          0.104    0.011    9.480    0.000    0.104    0.242\n   .PosAFF21          0.125    0.013    9.601    0.000    0.125    0.247\n   .PosAFF31          0.097    0.012    8.346    0.000    0.097    0.200\n   .PosAFF12          0.086    0.009    9.681    0.000    0.086    0.208\n   .PosAFF22          0.107    0.011    9.979    0.000    0.107    0.220\n   .PosAFF32          0.061    0.008    7.215    0.000    0.061    0.136\n   .PosAFF13          0.073    0.008    8.842    0.000    0.073    0.182\n   .PosAFF23          0.099    0.010    9.564    0.000    0.099    0.206\n   .PosAFF33          0.082    0.010    8.597    0.000    0.082    0.174\n    Pos1              1.000                               1.000    1.000\n    Pos2              1.000                               1.000    1.000\n    Pos3              1.000                               1.000    1.000"
  },
  {
    "objectID": "sem-3.html#strict-loadings-intercept-residual-variances",
    "href": "sem-3.html#strict-loadings-intercept-residual-variances",
    "title": "sem",
    "section": "Strict (loadings, intercept, residual variances)",
    "text": "Strict (loadings, intercept, residual variances)\nThe next step is to constrain residual variances to be the same. This step isn’t necessary to for correctly interpretting growth models.\n\n\nCode\nstrict <- '\n## define latent variables\nPos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31\nPos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32\nPos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33\n\n\n## equality of residuals \nPosAFF11 ~~ r*PosAFF11 \nPosAFF12 ~~ r*PosAFF12\nPosAFF13 ~~ r*PosAFF13\n\nPosAFF21 ~~ r*PosAFF21 \nPosAFF22 ~~ r*PosAFF22\nPosAFF23 ~~ r*PosAFF23\n\nPosAFF31 ~~ r*PosAFF31 \nPosAFF32 ~~ r*PosAFF32\nPosAFF33 ~~ r*PosAFF33\n\n\n## constrain intercepts across time\nPosAFF11 ~ t1*1\nPosAFF21 ~ t2*1\nPosAFF31 ~ t3*1\n\n\nPosAFF12 ~ t1*1\nPosAFF22 ~ t2*1\nPosAFF32 ~ t3*1\n\n\nPosAFF13 ~ t1*1\nPosAFF23 ~ t2*1\nPosAFF33 ~ t3*1\n\n'\n\nstrict <- cfa(strict, data=wide, meanstructure=TRUE, std.lv=TRUE)\n\nsummary(strict, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n  Number of equality constraints                    20\n                                                      \n  Number of observations                           368\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                93.802\n  Degrees of freedom                                44\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2688.088\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.985\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2083.225\n  Loglikelihood unrestricted model (H1)      -2036.324\n                                                      \n  Akaike (AIC)                                4186.450\n  Bayesian (BIC)                              4225.531\n  Sample-size adjusted Bayesian (BIC)         4193.804\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055\n  90 Percent confidence interval - lower         0.040\n  90 Percent confidence interval - upper         0.071\n  P-value RMSEA <= 0.05                          0.266\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.105\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PosAFF11  (L1)    0.573    0.017   33.193    0.000    0.573    0.883\n    PosAFF21  (L2)    0.626    0.018   34.047    0.000    0.626    0.899\n    PosAFF31  (L3)    0.617    0.018   33.923    0.000    0.617    0.897\n  Pos2 =~                                                               \n    PosAFF12  (L1)    0.573    0.017   33.193    0.000    0.573    0.883\n    PosAFF22  (L2)    0.626    0.018   34.047    0.000    0.626    0.899\n    PosAFF32  (L3)    0.617    0.018   33.923    0.000    0.617    0.897\n  Pos3 =~                                                               \n    PosAFF13  (L1)    0.573    0.017   33.193    0.000    0.573    0.883\n    PosAFF23  (L2)    0.626    0.018   34.047    0.000    0.626    0.899\n    PosAFF33  (L3)    0.617    0.018   33.923    0.000    0.617    0.897\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.457    0.045   10.065    0.000    0.457    0.457\n    Pos3              0.397    0.048    8.214    0.000    0.397    0.397\n  Pos2 ~~                                                               \n    Pos3              0.480    0.044   10.857    0.000    0.480    0.480\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11  (t1)    3.255    0.025  128.237    0.000    3.255    5.021\n   .PosAFF21  (t2)    3.108    0.027  113.239    0.000    3.108    4.467\n   .PosAFF31  (t3)    3.267    0.027  120.462    0.000    3.267    4.747\n   .PosAFF12  (t1)    3.255    0.025  128.237    0.000    3.255    5.021\n   .PosAFF22  (t2)    3.108    0.027  113.239    0.000    3.108    4.467\n   .PosAFF32  (t3)    3.267    0.027  120.462    0.000    3.267    4.747\n   .PosAFF13  (t1)    3.255    0.025  128.237    0.000    3.255    5.021\n   .PosAFF23  (t2)    3.108    0.027  113.239    0.000    3.108    4.467\n   .PosAFF33  (t3)    3.267    0.027  120.462    0.000    3.267    4.747\n    Pos1              0.000                               0.000    0.000\n    Pos2              0.000                               0.000    0.000\n    Pos3              0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .PosAFF11   (r)    0.092    0.003   33.226    0.000    0.092    0.220\n   .PosAFF12   (r)    0.092    0.003   33.226    0.000    0.092    0.220\n   .PosAFF13   (r)    0.092    0.003   33.226    0.000    0.092    0.220\n   .PosAFF21   (r)    0.092    0.003   33.226    0.000    0.092    0.191\n   .PosAFF22   (r)    0.092    0.003   33.226    0.000    0.092    0.191\n   .PosAFF23   (r)    0.092    0.003   33.226    0.000    0.092    0.191\n   .PosAFF31   (r)    0.092    0.003   33.226    0.000    0.092    0.195\n   .PosAFF32   (r)    0.092    0.003   33.226    0.000    0.092    0.195\n   .PosAFF33   (r)    0.092    0.003   33.226    0.000    0.092    0.195\n    Pos1              1.000                               1.000    1.000\n    Pos2              1.000                               1.000    1.000\n    Pos3              1.000                               1.000    1.000"
  },
  {
    "objectID": "sem-3.html#other-mi-types",
    "href": "sem-3.html#other-mi-types",
    "title": "sem",
    "section": "Other MI types",
    "text": "Other MI types\nNote that there are other types of MI that we could investigate, depending on what we are interested in. We could look at equality of latent means and variances, as well as regressions, if they were in the model."
  },
  {
    "objectID": "sem-3.html#comparing-the-models",
    "href": "sem-3.html#comparing-the-models",
    "title": "sem",
    "section": "Comparing the models",
    "text": "Comparing the models\nUsually done through chi-square difference test. You want NS, i.e. the constrains do not lead to worse fit. But this is a very sensitive test, especially with larger samples. Better to look at changes in CFI. If delta is .01 or greater than maybe it shows misfit.\n\n##Compare configural and weak model\nanova(config, weak)\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nconfig 24 4155.3 4272.5 22.648                                  \nweak   30 4170.1 4263.9 49.459      26.81       6  0.0001572 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "sem-3.html#section",
    "href": "sem-3.html#section",
    "title": "sem",
    "section": "",
    "text": "Code\n##Compare weak and strong model\nanova(weak, strong)\n\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)  \nweak   19 4160.3 4297.1 17.684                                \nstrong 23 4164.8 4285.9 30.144     12.461       4    0.01424 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nfitmeasures(weak)['cfi']\n\n\ncfi \n  1 \n\n\nCode\nfitmeasures(strong)['cfi']\n\n\n      cfi \n0.9973062 \n\n\nCode\nfitmeasures(strict)['cfi']\n\n\n      cfi \n0.9888801"
  },
  {
    "objectID": "sem-3.html#longitudinal-cfa",
    "href": "sem-3.html#longitudinal-cfa",
    "title": "sem",
    "section": "Longitudinal CFA",
    "text": "Longitudinal CFA\n\nCan be simply thought of as does this construct relate to itself across time? And, to the extent that it does not, is that due to changes in how the construct is measured over time?\nSubstantively, these analyses get at the relative rank order of people across assessment points. What is nice is that unlike using composites, this looks at latent correlations across time.\n\n-Also nice is that different measures can be used eg a cognitive ability exam for children which naturally differs in content across age groups."
  },
  {
    "objectID": "sem-3.html#unstandardized-estimates",
    "href": "sem-3.html#unstandardized-estimates",
    "title": "sem",
    "section": "unstandardized estimates",
    "text": "unstandardized estimates\n\nNotice the difference in loadings\nNotice the difference in covariances\n\nThis uses the marker variable approach. The above uses the fixed factor method.\n\nfit.long.cfa.us <- cfa(long.cfa, data=wide, std.lv=FALSE)\nsummary(fit.long.cfa.us)\n\nlavaan 0.6-11 ended normally after 144 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        69\n                                                      \n  Number of observations                           368\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               119.443\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.114\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  Pos1 =~                                             \n    PosAFF11          1.000                           \n    PosAFF21          0.996    0.041   24.232    0.000\n    PosAFF31          1.048    0.040   26.223    0.000\n  Pos2 =~                                             \n    PosAFF12          1.000                           \n    PosAFF22          1.148    0.047   24.463    0.000\n    PosAFF32          1.159    0.043   27.012    0.000\n  Pos3 =~                                             \n    PosAFF13          1.000                           \n    PosAFF23          1.072    0.049   22.027    0.000\n    PosAFF33          1.058    0.047   22.511    0.000\n  Neg1 =~                                             \n    NegAFF11          1.000                           \n    NegAFF21          0.851    0.040   21.308    0.000\n    NegAFF31          0.985    0.042   23.372    0.000\n  Neg2 =~                                             \n    NegAFF12          1.000                           \n    NegAFF22          1.027    0.049   21.002    0.000\n    NegAFF32          1.007    0.049   20.719    0.000\n  Neg3 =~                                             \n    NegAFF13          1.000                           \n    NegAFF23          0.940    0.055   17.134    0.000\n    NegAFF33          0.948    0.056   16.997    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .PosAFF11 ~~                                         \n   .PosAFF12          0.004    0.007    0.578    0.563\n   .PosAFF13          0.000    0.007    0.037    0.971\n .PosAFF12 ~~                                         \n   .PosAFF13          0.004    0.006    0.674    0.500\n .PosAFF21 ~~                                         \n   .PosAFF22          0.008    0.008    1.020    0.308\n   .PosAFF23          0.008    0.008    0.991    0.322\n .PosAFF22 ~~                                         \n   .PosAFF23          0.011    0.007    1.470    0.142\n .PosAFF31 ~~                                         \n   .PosAFF32          0.004    0.007    0.616    0.538\n   .PosAFF33          0.016    0.007    2.182    0.029\n .PosAFF32 ~~                                         \n   .PosAFF33          0.004    0.006    0.690    0.490\n .NegAFF11 ~~                                         \n   .NegAFF12          0.005    0.005    0.966    0.334\n   .NegAFF13          0.006    0.006    1.036    0.300\n .NegAFF12 ~~                                         \n   .NegAFF13          0.007    0.005    1.528    0.126\n .NegAFF21 ~~                                         \n   .NegAFF22          0.015    0.004    3.605    0.000\n   .NegAFF23          0.011    0.005    2.387    0.017\n .NegAFF22 ~~                                         \n   .NegAFF23          0.010    0.003    3.145    0.002\n .NegAFF31 ~~                                         \n   .NegAFF32         -0.006    0.004   -1.607    0.108\n   .NegAFF33         -0.008    0.004   -1.778    0.075\n .NegAFF32 ~~                                         \n   .NegAFF33         -0.001    0.003   -0.481    0.630\n  Pos1 ~~                                             \n    Pos2              0.172    0.023    7.457    0.000\n    Pos3              0.132    0.021    6.446    0.000\n    Neg1             -0.160    0.023   -6.937    0.000\n    Neg2             -0.071    0.014   -4.955    0.000\n    Neg3             -0.040    0.014   -2.866    0.004\n  Pos2 ~~                                             \n    Pos3              0.127    0.018    7.088    0.000\n    Neg1             -0.056    0.018   -3.116    0.002\n    Neg2             -0.110    0.014   -8.074    0.000\n    Neg3             -0.040    0.012   -3.351    0.001\n  Pos3 ~~                                             \n    Neg1             -0.021    0.016   -1.293    0.196\n    Neg2             -0.031    0.011   -2.860    0.004\n    Neg3             -0.054    0.011   -4.737    0.000\n  Neg1 ~~                                             \n    Neg2              0.108    0.014    7.765    0.000\n    Neg3              0.072    0.013    5.512    0.000\n  Neg2 ~~                                             \n    Neg3              0.058    0.009    6.483    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .PosAFF11          0.096    0.011    8.497    0.000\n   .PosAFF21          0.130    0.013    9.956    0.000\n   .PosAFF31          0.095    0.012    7.944    0.000\n   .PosAFF12          0.087    0.009   10.044    0.000\n   .PosAFF22          0.110    0.011    9.883    0.000\n   .PosAFF32          0.055    0.009    6.437    0.000\n   .PosAFF13          0.070    0.008    8.319    0.000\n   .PosAFF23          0.098    0.011    9.317    0.000\n   .PosAFF33          0.085    0.010    8.716    0.000\n   .NegAFF11          0.104    0.011    9.546    0.000\n   .NegAFF21          0.091    0.009   10.363    0.000\n   .NegAFF31          0.056    0.009    6.475    0.000\n   .NegAFF12          0.062    0.006   10.835    0.000\n   .NegAFF22          0.037    0.004    8.445    0.000\n   .NegAFF32          0.033    0.004    7.917    0.000\n   .NegAFF13          0.084    0.008   10.660    0.000\n   .NegAFF23          0.043    0.005    8.170    0.000\n   .NegAFF33          0.038    0.005    7.372    0.000\n    Pos1              0.427    0.039   10.968    0.000\n    Pos2              0.309    0.029   10.628    0.000\n    Pos3              0.258    0.025   10.514    0.000\n    Neg1              0.317    0.031   10.233    0.000\n    Neg2              0.133    0.014    9.494    0.000\n    Neg3              0.132    0.015    8.564    0.000"
  },
  {
    "objectID": "sem-3.html#introducing-means.",
    "href": "sem-3.html#introducing-means.",
    "title": "sem",
    "section": "introducing means.",
    "text": "introducing means.\n\nNotice how we can also estimate means of the variables, though we don’t necessarily need to. These are now under the “intercepts” heading.\n\n\nfit.long.cfa.us.mean <- cfa(long.cfa, meanstructure=TRUE, data=wide, std.lv=FALSE)\nsummary(fit.long.cfa.us.mean)\n\nlavaan 0.6-11 ended normally after 144 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        87\n                                                      \n  Number of observations                           368\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               119.443\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.114\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  Pos1 =~                                             \n    PosAFF11          1.000                           \n    PosAFF21          0.996    0.041   24.232    0.000\n    PosAFF31          1.048    0.040   26.223    0.000\n  Pos2 =~                                             \n    PosAFF12          1.000                           \n    PosAFF22          1.148    0.047   24.463    0.000\n    PosAFF32          1.159    0.043   27.012    0.000\n  Pos3 =~                                             \n    PosAFF13          1.000                           \n    PosAFF23          1.072    0.049   22.027    0.000\n    PosAFF33          1.058    0.047   22.511    0.000\n  Neg1 =~                                             \n    NegAFF11          1.000                           \n    NegAFF21          0.851    0.040   21.308    0.000\n    NegAFF31          0.985    0.042   23.372    0.000\n  Neg2 =~                                             \n    NegAFF12          1.000                           \n    NegAFF22          1.027    0.049   21.002    0.000\n    NegAFF32          1.007    0.049   20.719    0.000\n  Neg3 =~                                             \n    NegAFF13          1.000                           \n    NegAFF23          0.940    0.055   17.134    0.000\n    NegAFF33          0.948    0.056   16.997    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .PosAFF11 ~~                                         \n   .PosAFF12          0.004    0.007    0.578    0.563\n   .PosAFF13          0.000    0.007    0.037    0.971\n .PosAFF12 ~~                                         \n   .PosAFF13          0.004    0.006    0.674    0.500\n .PosAFF21 ~~                                         \n   .PosAFF22          0.008    0.008    1.020    0.308\n   .PosAFF23          0.008    0.008    0.991    0.322\n .PosAFF22 ~~                                         \n   .PosAFF23          0.011    0.007    1.470    0.142\n .PosAFF31 ~~                                         \n   .PosAFF32          0.004    0.007    0.616    0.538\n   .PosAFF33          0.016    0.007    2.182    0.029\n .PosAFF32 ~~                                         \n   .PosAFF33          0.004    0.006    0.690    0.490\n .NegAFF11 ~~                                         \n   .NegAFF12          0.005    0.005    0.966    0.334\n   .NegAFF13          0.006    0.006    1.036    0.300\n .NegAFF12 ~~                                         \n   .NegAFF13          0.007    0.005    1.528    0.126\n .NegAFF21 ~~                                         \n   .NegAFF22          0.015    0.004    3.605    0.000\n   .NegAFF23          0.011    0.005    2.387    0.017\n .NegAFF22 ~~                                         \n   .NegAFF23          0.010    0.003    3.145    0.002\n .NegAFF31 ~~                                         \n   .NegAFF32         -0.006    0.004   -1.607    0.108\n   .NegAFF33         -0.008    0.004   -1.778    0.075\n .NegAFF32 ~~                                         \n   .NegAFF33         -0.001    0.003   -0.481    0.630\n  Pos1 ~~                                             \n    Pos2              0.172    0.023    7.457    0.000\n    Pos3              0.132    0.021    6.446    0.000\n    Neg1             -0.160    0.023   -6.937    0.000\n    Neg2             -0.071    0.014   -4.955    0.000\n    Neg3             -0.040    0.014   -2.866    0.004\n  Pos2 ~~                                             \n    Pos3              0.127    0.018    7.088    0.000\n    Neg1             -0.056    0.018   -3.116    0.002\n    Neg2             -0.110    0.014   -8.074    0.000\n    Neg3             -0.040    0.012   -3.351    0.001\n  Pos3 ~~                                             \n    Neg1             -0.021    0.016   -1.293    0.196\n    Neg2             -0.031    0.011   -2.860    0.004\n    Neg3             -0.054    0.011   -4.737    0.000\n  Neg1 ~~                                             \n    Neg2              0.108    0.014    7.765    0.000\n    Neg3              0.072    0.013    5.512    0.000\n  Neg2 ~~                                             \n    Neg3              0.058    0.009    6.483    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .PosAFF11          3.212    0.038   85.141    0.000\n   .PosAFF21          3.105    0.039   80.039    0.000\n   .PosAFF31          3.248    0.039   82.975    0.000\n   .PosAFF12          3.253    0.033   99.077    0.000\n   .PosAFF22          3.126    0.037   83.350    0.000\n   .PosAFF32          3.274    0.036   91.552    0.000\n   .PosAFF13          3.302    0.030  110.512    0.000\n   .PosAFF23          3.094    0.033   94.436    0.000\n   .PosAFF33          3.280    0.032  102.909    0.000\n   .NegAFF11          1.522    0.034   45.016    0.000\n   .NegAFF21          1.397    0.030   47.359    0.000\n   .NegAFF31          1.398    0.031   44.488    0.000\n   .NegAFF12          1.412    0.023   61.269    0.000\n   .NegAFF22          1.324    0.022   60.181    0.000\n   .NegAFF32          1.300    0.021   60.718    0.000\n   .NegAFF13          1.430    0.024   59.112    0.000\n   .NegAFF23          1.281    0.021   61.551    0.000\n   .NegAFF33          1.297    0.021   62.890    0.000\n    Pos1              0.000                           \n    Pos2              0.000                           \n    Pos3              0.000                           \n    Neg1              0.000                           \n    Neg2              0.000                           \n    Neg3              0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .PosAFF11          0.096    0.011    8.497    0.000\n   .PosAFF21          0.130    0.013    9.956    0.000\n   .PosAFF31          0.095    0.012    7.944    0.000\n   .PosAFF12          0.087    0.009   10.044    0.000\n   .PosAFF22          0.110    0.011    9.883    0.000\n   .PosAFF32          0.055    0.009    6.437    0.000\n   .PosAFF13          0.070    0.008    8.319    0.000\n   .PosAFF23          0.098    0.011    9.317    0.000\n   .PosAFF33          0.085    0.010    8.716    0.000\n   .NegAFF11          0.104    0.011    9.546    0.000\n   .NegAFF21          0.091    0.009   10.363    0.000\n   .NegAFF31          0.056    0.009    6.475    0.000\n   .NegAFF12          0.062    0.006   10.835    0.000\n   .NegAFF22          0.037    0.004    8.445    0.000\n   .NegAFF32          0.033    0.004    7.917    0.000\n   .NegAFF13          0.084    0.008   10.660    0.000\n   .NegAFF23          0.043    0.005    8.170    0.000\n   .NegAFF33          0.038    0.005    7.372    0.000\n    Pos1              0.427    0.039   10.968    0.000\n    Pos2              0.309    0.029   10.628    0.000\n    Pos3              0.258    0.025   10.514    0.000\n    Neg1              0.317    0.031   10.233    0.000\n    Neg2              0.133    0.014    9.494    0.000\n    Neg3              0.132    0.015    8.564    0.000"
  },
  {
    "objectID": "sem-3.html#second-order-growth-model",
    "href": "sem-3.html#second-order-growth-model",
    "title": "sem",
    "section": "Second order growth model",
    "text": "Second order growth model\nRepeated measures are latent. Why would we want to do this? At least two reasons.\n\nWe can take advantage of the benefits of latent variables ie no measurement error.\nwe can impose constraints for measurement invariance across time (MI)."
  },
  {
    "objectID": "sem-3.html#modeling-residuals",
    "href": "sem-3.html#modeling-residuals",
    "title": "sem",
    "section": "modeling residuals",
    "text": "modeling residuals\nConstrain to be equal. Compare with mlm residual\n\n\nCode\nmodel.4 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16\n            \nalcuse_14 ~~ a*alcuse_14\nalcuse_15 ~~ a*alcuse_15\nalcuse_16 ~~ a*alcuse_16'\n\nfit.4 <- growth(model.4, data=alcohol.wide)\nsummary(fit.4)\n\n\nlavaan 0.6-11 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n  Number of equality constraints                     2\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 4.238\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.237\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         0.500                           \n    alcuse_16         1.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~~                                                \n    s                -0.137    0.140   -0.977    0.329\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n    i                 0.651    0.105    6.198    0.000\n    s                 0.541    0.125    4.334    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14  (a)    0.337    0.053    6.403    0.000\n   .alcuse_15  (a)    0.337    0.053    6.403    0.000\n   .alcuse_16  (a)    0.337    0.053    6.403    0.000\n    i                 0.624    0.148    4.217    0.000\n    s                 0.605    0.226    2.678    0.007"
  },
  {
    "objectID": "sem-3.html#plotting",
    "href": "sem-3.html#plotting",
    "title": "sem",
    "section": "Plotting",
    "text": "Plotting\n\nlavPredict(fit.1,type=\"lv\")\n\n          i      s\n [1,] 1.673  0.151\n [2,] 0.016  0.420\n [3,] 1.037  0.892\n [4,] 0.155  0.808\n [5,] 0.012  0.096\n [6,] 2.857  0.058\n [7,] 1.700 -0.140\n [8,] 0.012  0.096\n [9,] 0.094  1.294\n[10,] 0.960  0.066\n[11,] 0.024  1.120\n[12,] 0.895  0.228\n[13,] 0.020  0.744\n[14,] 2.747  0.069\n[15,] 1.691  0.314\n[16,] 0.990  0.231\n[17,] 0.080  0.171\n[18,] 1.886 -0.309\n[19,] 0.012  0.096\n[20,] 3.167 -0.733\n[21,] 1.013  0.358\n[22,] 2.686 -0.582\n[23,] 0.963  0.303\n[24,] 0.080  0.171\n[25,] 2.719 -0.260\n[26,] 0.085  0.629\n[27,] 2.647 -0.631\n[28,] 0.012  0.096\n[29,] 0.016  0.420\n[30,] 1.031  0.378\n[31,] 1.776  0.529\n[32,] 1.630 -0.217\n[33,] 0.085  0.629\n[34,] 0.084  0.495\n[35,] 0.956 -0.258\n[36,] 0.964  0.390\n[37,] 1.093  0.904\n[38,] 0.138  0.874\n[39,] 0.012  0.096\n[40,] 0.012  0.096\n[41,] 1.259  0.137\n[42,] 0.012  0.096\n[43,] 0.012  0.096\n[44,] 0.120  1.174\n[45,] 2.135  0.276\n[46,] 0.017  0.554\n[47,] 0.080  0.171\n[48,] 0.012  0.096\n[49,] 0.012  0.096\n[50,] 0.012  0.096\n[51,] 0.012  0.096\n[52,] 0.080  0.171\n[53,] 0.012  0.096\n[54,] 0.016  0.420\n[55,] 0.012  0.096\n[56,] 0.118  1.060\n[57,] 0.012  0.096\n[58,] 0.994  0.567\n[59,] 0.022  0.953\n[60,] 0.085  0.629\n[61,] 1.924  0.053\n[62,] 0.012  0.096\n[63,] 0.012  0.096\n[64,] 0.157  0.971\n[65,] 0.085  0.629\n[66,] 1.498  0.721\n[67,] 0.888 -0.334\n[68,] 0.080  0.171\n[69,] 0.012  0.096\n[70,] 1.832 -0.688\n[71,] 0.012  0.096\n[72,] 0.012  0.096\n[73,] 0.012  0.096\n[74,] 0.138  0.951\n[75,] 0.012  0.096\n[76,] 0.012  0.096\n[77,] 0.119  1.119\n[78,] 0.012  0.096\n[79,] 1.046  0.293\n[80,] 0.963  0.303\n[81,] 0.012  0.096\n[82,] 0.112  0.526"
  },
  {
    "objectID": "bends-4.html#trajectories-that-bend",
    "href": "bends-4.html#trajectories-that-bend",
    "title": "bends",
    "section": "Trajectories that bend",
    "text": "Trajectories that bend\n\n\n\n\nThus far we have been sticking with monotonically increasing trajectories. This is a good assumption given the amount of data often found, along with the simplicity.\nOften we want to see if trajectories are not straight. Development is not simple so our lines should not be.\nNeed effective strategies for line that bend that also balance tradeoffs with interprettability and overfitting"
  },
  {
    "objectID": "bends-4.html#polynomial-and-splines",
    "href": "bends-4.html#polynomial-and-splines",
    "title": "bends",
    "section": "Polynomial and Splines",
    "text": "Polynomial and Splines\nPolynomials (quadratic) level 1:\n\\[{Y}_{ij} = \\beta_{0j}  + \\beta_{1j}(Time_{ij} - \\bar{X)} + \\beta_{2j}(Time_{ij} - \\bar{X)}^2 + \\varepsilon_{ij}\\]\nLevel 2:\n\\[{\\beta}_{0j} = \\gamma_{00} +   U_{0j}\\]\n\\[{\\beta}_{1j} = \\gamma_{10} +  U_{1j}\\]\n\\[{\\beta}_{2j} = \\gamma_{20} +  U_{2j}\\]"
  },
  {
    "objectID": "bends-4.html#mlm-poly-example",
    "href": "bends-4.html#mlm-poly-example",
    "title": "bends",
    "section": "MLM poly example",
    "text": "MLM poly example\n\n\nCode\npersonality <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/Subject_personality.csv\")\n\nggplot(personality,\n   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()"
  },
  {
    "objectID": "bends-4.html#sem-poly-example",
    "href": "bends-4.html#sem-poly-example",
    "title": "bends",
    "section": "SEM poly example",
    "text": "SEM poly example\n\n\nCode\n#use alcohol data from before\nalcohol <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/alcohol1_pp.csv\")\n\nalcohol.wide <- alcohol %>% \n  dplyr::select(-X, -age_14, -ccoa) %>% \n  pivot_wider(names_from = \"age\", \n              names_prefix = \"alcuse_\",\n              values_from  = alcuse) \nalcohol.wide\n\n\n# A tibble: 82 × 8\n      id   coa  male  peer  cpeer alcuse_14 alcuse_15 alcuse_16\n   <int> <int> <int> <dbl>  <dbl>     <dbl>     <dbl>     <dbl>\n 1     1     1     0 1.26   0.247      1.73      2         2   \n 2     2     1     1 0.894 -0.124      0         0         1   \n 3     3     1     1 0.894 -0.124      1         2         3.32\n 4     4     1     1 1.79   0.771      0         2         1.73\n 5     5     1     0 0.894 -0.124      0         0         0   \n 6     6     1     1 1.55   0.531      3         3         3.16\n 7     7     1     0 1.55   0.531      1.73      2.45      1   \n 8     8     1     1 0     -1.02       0         0         0   \n 9     9     1     1 0     -1.02       0         1         3.46\n10    10     1     0 2      0.982      1         1         1   \n# … with 72 more rows"
  },
  {
    "objectID": "bends-4.html#sem-latent-basis",
    "href": "bends-4.html#sem-latent-basis",
    "title": "bends",
    "section": "SEM latent basis",
    "text": "SEM latent basis\n\n#| code-fold: true\nmodel.7 <- '  \n\ni =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 \ns =~ 0*extra_1 + extra_2 + extra_3 + extra_4 + 4*extra_5 \n\n'\n\np7 <- growth(model.7, data = p.wide, missing = \"ML\")"
  },
  {
    "objectID": "bends-4.html#piecewise",
    "href": "bends-4.html#piecewise",
    "title": "bends",
    "section": "Piecewise",
    "text": "Piecewise\n\nFit more than 1 trajectory\nBest to use when we have a reason for a qualitative difference at a time point. For example, before your health event you may have a different trajectory than after\nTime modeled as dummy variables that represent different segments\nThe point of separation is called a knot. You can have as many as you want and these can be pre-specified or let the data specify"
  },
  {
    "objectID": "bends-4.html#splines-polynomial-polynomial-piecewise",
    "href": "bends-4.html#splines-polynomial-polynomial-piecewise",
    "title": "bends",
    "section": "splines + polynomial = polynomial piecewise",
    "text": "splines + polynomial = polynomial piecewise\n\\[{Y}_{ij} = \\beta_{0j}  + \\beta_{1j}Time1_{ij} +  \\beta_{2j}Time1_{ij}^2 + \\beta_{3j}Time2_{ij} + \\varepsilon_{ij}\\]\nLevel 2:\n\\[{\\beta}_{0j} = \\gamma_{00} +  U_{0j}\\]\n\\[{\\beta}_{1j} = \\gamma_{10} +  U_{1j}\\]\n\\[{\\beta}_{2j} = \\gamma_{20} +  U_{2j}\\]\n\\[{\\beta}_{3j} = \\gamma_{30} +  U_{3j}\\]"
  },
  {
    "objectID": "sem-3.html#types-of-longitudinal-models",
    "href": "sem-3.html#types-of-longitudinal-models",
    "title": "sem",
    "section": "Types of longitudinal models",
    "text": "Types of longitudinal models\n\nGrowth models\nLongitudinal CFA\nCross lag panel model\nLongitudinal mediation\nGrowth models + cross lags\nLatent change/difference score models\nMixture or class based longitudinal models"
  },
  {
    "objectID": "sem-3.html#effect-coding",
    "href": "sem-3.html#effect-coding",
    "title": "sem",
    "section": "effect coding",
    "text": "effect coding\n\n\nCode\nsec.order <- '\n## define latent variables\nPos1 =~ NA*PosAFF11 + L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31\nPos2 =~ NA*PosAFF12 + L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32\nPos3 =~ NA*PosAFF13 + L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33\n\n## intercepts\nPosAFF11 ~ t1*1\nPosAFF21 ~ t2*1\nPosAFF31 ~ t3*1\n\nPosAFF12 ~ t1*1\nPosAFF22 ~ t2*1\nPosAFF32 ~ t3*1\n\nPosAFF13 ~ t1*1\nPosAFF23 ~ t2*1\nPosAFF33 ~ t3*1\n\n\n## correlated residuals across time\nPosAFF11 ~~ PosAFF12 + PosAFF13\nPosAFF12 ~~ PosAFF13\nPosAFF21 ~~ PosAFF22 + PosAFF23\nPosAFF22 ~~ PosAFF23\nPosAFF31 ~~ PosAFF32 + PosAFF33\nPosAFF32 ~~ PosAFF33\n\n\n## latent variable intercepts\nPos1 ~ 0*1\nPos2  ~ 0*1\nPos3  ~ 0*1\n\n#model constraints for effect coding\n## loadings must average to 1\nL1 == 3 - L2 - L3\n## means must average to 0\nt1 == 0 - t2 - t3\n\ni =~ 1*Pos1 + 1*Pos2 + 1*Pos3 \ns =~ 0*Pos1 + 1*Pos2 + 2*Pos3 '\n\n\nfit.sec.order <- growth(sec.order, data=wide, missing = \"ML\")"
  },
  {
    "objectID": "bends-4.html#mlm-example",
    "href": "bends-4.html#mlm-example",
    "title": "bends",
    "section": "mlm example",
    "text": "mlm example\nlevel 1:\n\\[{Y}_{ij} = \\beta_{0j}  + \\beta_{1j}Time1_{ij} + \\beta_{2j}Time2_{ij} + \\varepsilon_{ij}\\]\nLevel 2:\n\\[{\\beta}_{0j} = \\gamma_{00} +  U_{0j}\\]\n\\[{\\beta}_{1j} = \\gamma_{10} +  U_{1j}\\]\n\\[{\\beta}_{2j} = \\gamma_{20} +  U_{2j}\\]"
  },
  {
    "objectID": "bends-4.html#sem-example",
    "href": "bends-4.html#sem-example",
    "title": "bends",
    "section": "SEM example",
    "text": "SEM example\n0 1 2 2 2\n0 0 0 1 2\n\ntwo.rate <- 'i =~ 1*extra_1 + 1*extra_2 + 1*extra_3 + 1*extra_4 + 1*extra_5 \ns1 =~ 0*extra_1 + 1*extra_2 + 2*extra_3 + 2*extra_4 + 2*extra_5 \ns2 =~ 0*extra_1 + 0*extra_2 + 0*extra_3 + 1*extra_4 + 2*extra_5  \n'\n\np8 <- growth(two.rate, data = p.wide, missing = \"ML\")"
  },
  {
    "objectID": "hw2.html",
    "href": "hw2.html",
    "title": "hw2",
    "section": "",
    "text": "There are 7 waves of data, each with a different file (hw2-t1, hw2-t2, etc). There are five different types of repeated measures variables, all from “the Big Five” of personality. Each of these five constructs have 9 items associated with them. See codebook for more details.\nYour first task is to combine the datasets to create a longitudinal dataset. Then create composites of each of the 5 constructs (extraversion, agreeableness, conscientiousness, neuroticism and openness). Be sure to be on the lookout for reverse scored items.\n\nChoose one of the Big Five traits and run a growth model in SEM where the indicators are manifest. Interpret the intercept and the variance terms\nTry a new time coding: Use a time coding that defines the intercept as the final time point. How does this differ form the previous model – what is the same/different?\nRun a latent basis coding where you allow the data to inform how time should be coded. Describe how this differs from the initial latent growth curve model.\nRun an MLM growth curve. How are the SEM and the MLM different? The same? What constraints could you put on the SEM model to more closely mimic the MLM model?\nPlot the growth curve from an SEM model of your choosing\nTest whether Measurement Invariance (MI) holds for a construct of your choosing. First try each item as an indicator. Then try parcels. Do the parcels improve the fit?\nImpose MI and run a second order growth curve model. How does this model differ from models where the repeated measure is manifest (estimates, fit statistics, etcetera)?"
  },
  {
    "objectID": "bends-4.html#gams",
    "href": "bends-4.html#gams",
    "title": "bends",
    "section": "GAMs",
    "text": "GAMs\n\nstandard longitudinal models are simple to understand (lines!), but fail to\n\nGAMs offer a middle ground: they can be fit to complex, nonlinear relationships and make good predictions in these cases, but we are still able to do inferential statistics and understand and explain the underlying structure of our models and why they make predictions that they do"
  },
  {
    "objectID": "conditional-5.html#conditional-models",
    "href": "conditional-5.html#conditional-models",
    "title": "conditional",
    "section": "Conditional models",
    "text": "Conditional models\n\nWe are now going to introduce predictors to our growth models beyond time. These predictors are similar to predictors in standard regression – dummy for nominal, interactions change lower order terms, etcetera.\nThis is all just regression, so the same interpretation is the same. It is now only harder because we have multiple levels and lots of potential for interactions"
  },
  {
    "objectID": "conditional-5.html#level-2-group-predictors",
    "href": "conditional-5.html#level-2-group-predictors",
    "title": "conditional",
    "section": "Level 2 group predictors",
    "text": "Level 2 group predictors\n\nDo groups differ in their initial status? Level 2, person variable that is dummy coded. Note that group here only is measured once, it is a between person variable.\n\nlevel 1: \\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}G_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} + U_{1i}\\]"
  },
  {
    "objectID": "conditional-5.html#interpretation-of-conditional-fixed-effects",
    "href": "conditional-5.html#interpretation-of-conditional-fixed-effects",
    "title": "conditional",
    "section": "Interpretation of conditional fixed effects",
    "text": "Interpretation of conditional fixed effects\nNotice we have a new gamma term, \\(\\gamma_{01}\\). How do we interpret this new fixed effect, especially in the presence of other fixed effects?\n\\(\\gamma_{00}\\) is the intercept and can be considered the value when G = 0 and time = 0, whereas the \\(\\gamma_{01}\\) is the difference in initial values between groups.\nThe value for group = 1? \\(\\gamma_{00} + \\gamma_{01}\\)"
  },
  {
    "objectID": "conditional-5.html#interpretation-of-random-effects",
    "href": "conditional-5.html#interpretation-of-random-effects",
    "title": "conditional",
    "section": "Interpretation of random effects",
    "text": "Interpretation of random effects\n\nOne thing to keep in mind is that we are now changing the meaning of the random effect. Now that we have a predictor in the model, the \\(U_{0j}\\) is the person specific deviation from the group predicted intercept, not the grand mean intercept.\n\nLevel 1:\n\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}G_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} + U_{1i}\\]"
  },
  {
    "objectID": "conditional-5.html#does-your-covariance-structure-change",
    "href": "conditional-5.html#does-your-covariance-structure-change",
    "title": "conditional",
    "section": "Does your covariance structure change?",
    "text": "Does your covariance structure change?\nLevel 2 covariance matrix \\[\\begin{pmatrix} {U}_{0j} \\\\ {U}_{1j} \\end{pmatrix}\n\\sim \\mathcal{N} \\begin{pmatrix}\n  0,     & \\tau_{00}^{2} & \\tau_{01}\\\\\n  0, & \\tau_{01} & \\tau_{10}^{2}\n\\end{pmatrix}\\]\nLevel 1 residual variance will also be different\n\\[ {R}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})  \\]"
  },
  {
    "objectID": "conditional-5.html#equations-for-each-group",
    "href": "conditional-5.html#equations-for-each-group",
    "title": "conditional",
    "section": "Equations for each group",
    "text": "Equations for each group\n\\[{Y}_{ti} = (\\gamma_{00} + \\gamma_{01}G_{i} + U_{0i} ) + [(\\gamma_{10}+ U_{1i})(Time_{ti})] + \\varepsilon_{ti}\\]\nUnderstanding how to re-write the equation will help for calculating estimated scores for your predictors in addition to being able to interpret the coefficients.\nEstimated value for an individual in group = 0\n\n\\[\\hat{Y}_{ti} = (\\gamma_{00} + U_{0i} ) + [(\\gamma_{10}+ U_{1i})(Time_{ti})]\\] group = 1 individual \\[\\hat{Y}_{ti} = (\\gamma_{00} + \\gamma_{01}+ U_{0i} ) + [(\\gamma_{10}+ U_{1i})(Time_{ti})]\\]"
  },
  {
    "objectID": "conditional-5.html#slope-and-intercept-group-predictors",
    "href": "conditional-5.html#slope-and-intercept-group-predictors",
    "title": "conditional",
    "section": "Slope and Intercept Group Predictors",
    "text": "Slope and Intercept Group Predictors\nPredicting the intercept only can only answer static questions, not about change.\nLevel 1:\n\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}G_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  \\gamma_{11}G_{i} +U_{1i}\\]\nSimilar to before, the interpretation of \\(U_{1i}\\) changes. The term is now what is left over after accounting for group differences in the mean slope."
  },
  {
    "objectID": "conditional-5.html#cross-level-interactions",
    "href": "conditional-5.html#cross-level-interactions",
    "title": "conditional",
    "section": "cross level interactions",
    "text": "cross level interactions\nLevel 1:\n\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}G_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  \\gamma_{11}G_{i} +U_{1i}\\]\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{01}G_{i}+  \\gamma_{10} (Time_{ti}) + \\gamma_{11}(G_{i}*Time_{ti}) + \\] \\[ U_{0i} + U_{1i}(Time_{ti}) + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "conditional-5.html#interpretation-of-lower-order-terms",
    "href": "conditional-5.html#interpretation-of-lower-order-terms",
    "title": "conditional",
    "section": "interpretation of lower order terms",
    "text": "interpretation of lower order terms\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{01}G_{i}+  \\gamma_{10} (Time_{ti}) + \\gamma_{11}(G_{i}*Time_{ti}) + \\] \\[ U_{0i} + U_{1i}(Time_{ti}) + \\varepsilon_{ti}\\]\nAs with regression, the lower order terms are now conditional on the higher order interaction.\n\\(\\gamma_{10}\\), the fixed effect representing our slope is now the simple slope for group = 0\n\\(\\gamma_{01}\\), the fixed effect for group is the difference between groups when time = 0, e.g. the intercept\n\\(\\gamma_{00}\\) is the intercept, it is the value for when our predictors are zero."
  },
  {
    "objectID": "conditional-5.html#residual-structure",
    "href": "conditional-5.html#residual-structure",
    "title": "conditional",
    "section": "residual structure",
    "text": "residual structure\nLevel 2 covariance matrix\n\\[\\begin{pmatrix} {U}_{0j} \\\\ {U}_{1j} \\end{pmatrix} \\sim\\mathcal{N} \\begin{pmatrix} 0, \\tau_{00}^{2} & \\tau_{01}\\\\  0,  \\tau_{01} & \\tau_{10}^{2} \\end{pmatrix}\\]\nHow does your variance-covariance matrix change?\nLevel 1 residual variance \\[{R}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})\\]\nHow does your residual change relative to a model without group effects? Can you graph conceptually what this now captures?"
  },
  {
    "objectID": "conditional-5.html#predictive-equation",
    "href": "conditional-5.html#predictive-equation",
    "title": "conditional",
    "section": "predictive equation",
    "text": "predictive equation\nThinking about the equation as a predictive engine will help us later with graphing\nAlternative combined \\[{Y}_{ti} = [\\gamma_{00} + U_{0i} +\\gamma_{01}G_{i}] + [(\\gamma_{10}  + \\gamma_{11}G_{i}+  U_{1i})(Time_{ti})] + \\varepsilon_{ti}\\]\nThis is just rearranged so you can see that different groups have different intercepts and slopes\n\\[\\hat{Y}_{ti} = [\\gamma_{00} +\\gamma_{01}G_{i}] + [(\\gamma_{10}  + \\gamma_{11}G_{i})(Time_{ti})]\\]\nNotice how when G = 0, the equation simplifies:\n\\[\\hat{Y}_{ti} = \\gamma_{00} + \\gamma_{10} (Time_{ti})\\]"
  },
  {
    "objectID": "conditional-5.html#l2-continuous-predictors",
    "href": "conditional-5.html#l2-continuous-predictors",
    "title": "conditional",
    "section": "L2 Continuous predictors",
    "text": "L2 Continuous predictors\nA continuous L2 predictor is similar to the group predictors.\nLevel 1:\n\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}C_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  \\gamma_{11}C_{i} +U_{1i}\\]\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{01}C_{i}+  \\gamma_{10} (Time_{ti}) + \\gamma_{11}(C_{i}*Time_{ti}) +  \\] \\[ U_{0i} + U_{1j}(Time_{ti}) + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "conditional-5.html#continuous-conditional-interpretation",
    "href": "conditional-5.html#continuous-conditional-interpretation",
    "title": "conditional",
    "section": "Continuous conditional interpretation",
    "text": "Continuous conditional interpretation\n\\[{Y}_{ti} = \\gamma_{00} + \\gamma_{01}C_{i}+  \\gamma_{10} (Time_{ti}) + \\gamma_{11}(C_{i}*Time_{ti}) +  \\] \\[ U_{0i} + U_{1j}(Time_{ti}) + \\varepsilon_{ti}\\]\nThe \\(\\gamma_{11}\\) interaction coefficient indexes the difference in slopes at different levels of C\nThe \\(\\gamma_{10}\\) is now the effect of time when the continuous predictor is zero.\nThe \\(\\gamma_{01}\\) is now the effect of C when time is zero.\n\\(\\gamma_{00}\\) is the intercept, it is the value for when our predictors are zero.\n\\(U_{0i}\\) Is the random effect for intercept after accounting for C.\n\\(U_{1i}\\) Is the random effect for the slope after accounting for C."
  },
  {
    "objectID": "conditional-5.html#equations-for-plotting",
    "href": "conditional-5.html#equations-for-plotting",
    "title": "conditional",
    "section": "Equations for plotting",
    "text": "Equations for plotting\nThe same logic for plotting continuous variables except this time we have to choose the values for simple slopes\n\\[\\hat{Y}_{ti} = [\\gamma_{00} + \\gamma_{01}C_{i}]+  [\\gamma_{10} + \\gamma_{11}C_{i}]*Time_{ti}\\]\nAssuming C is standardized: -1sd \\[\\hat{Y}_{ti} = [\\gamma_{00} +(\\gamma_{01}*-1)] + [\\gamma_{10}  + (\\gamma_{11}*-1)]*Time_{ti}\\]\nMean \\[\\hat{Y}_{ti} = \\gamma_{00} + \\gamma_{10} * (Time_{ti})\\]\n+1sd \\[\\hat{Y}_{ti} = [\\gamma_{00} +\\gamma_{01}] + [\\gamma_{10}  + \\gamma_{11}]*Time_{ti}\\]"
  },
  {
    "objectID": "conditional-5.html#individual-level-trajectories",
    "href": "conditional-5.html#individual-level-trajectories",
    "title": "conditional",
    "section": "individual level trajectories",
    "text": "individual level trajectories\nWhat do individual level trajectories look like?\n\\[\\hat{Y}_{ti} = [\\gamma_{00} + \\gamma_{01}C_{i}+  U_{0i}] + \\] \\[ [\\gamma_{10}  + (\\gamma_{11}*C_{i}) + U_{1i}]  * Time_{ti}\\]\nIf you know someones random effects, and you know the fixed effects, you can create predicted values for any level of time."
  },
  {
    "objectID": "conditional-5.html#plotting",
    "href": "conditional-5.html#plotting",
    "title": "conditional",
    "section": "Plotting",
    "text": "Plotting\n\n\nCode\nlibrary(tidyverse)\nalcohol <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2022/main/alcohol1_pp.csv\")\nhead(alcohol)\n\n\n  X id age coa male age_14   alcuse      peer      cpeer  ccoa\n1 1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2 2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3 3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4 4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5 5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6 6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549"
  },
  {
    "objectID": "conditional-5.html#plotting-1",
    "href": "conditional-5.html#plotting-1",
    "title": "conditional",
    "section": "Plotting",
    "text": "Plotting\n\nlibrary(report)\nalcohol %>% \n   report_sample()\n\n# Descriptive Statistics\n\nVariable         |        Summary\n---------------------------------\nMean X (SD)      | 123.50 (71.16)\nMean id (SD)     |  41.50 (23.72)\nMean age (SD)    |   15.00 (0.82)\nMean coa (SD)    |    0.45 (0.50)\nMean male (SD)   |    0.51 (0.50)\nMean age_14 (SD) |    1.00 (0.82)\nMean alcuse (SD) |    0.92 (1.06)\nMean peer (SD)   |    1.02 (0.73)\nMean cpeer (SD)  |   -0.00 (0.73)\nMean ccoa (SD)   |    0.00 (0.50)"
  },
  {
    "objectID": "conditional-5.html#example",
    "href": "conditional-5.html#example",
    "title": "conditional",
    "section": "Example",
    "text": "Example\n\n\nCode\nfit4 <- lmer(alcuse ~  age_14 + peer + (age_14 | id), data = alcohol)\nsummary(fit4)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ age_14 + peer + (age_14 | id)\n   Data: alcohol\n\nREML criterion at convergence: 615.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5192 -0.3643 -0.1291  0.4198  2.4918 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 0.3396   0.5827        \n          age_14      0.1552   0.3939   -0.08\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.03236    0.14113  -0.229\nage_14       0.27065    0.06284   4.307\npeer         0.67186    0.10920   6.152\n\nCorrelation of Fixed Effects:\n       (Intr) age_14\nage_14 -0.256       \npeer   -0.787  0.000"
  },
  {
    "objectID": "conditional-5.html#multiple-predictors",
    "href": "conditional-5.html#multiple-predictors",
    "title": "conditional",
    "section": "Multiple predictors",
    "text": "Multiple predictors\nInterpretations with multiple predictors in regression extend to MLMs. For example, health across time, examining the effects of an intervention, while controlling for initial exercise status?:\nlevel 1: \\[{Health}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\varepsilon_{ti}\\]\nLevel 2: \\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}Exercise_{i} +  \\gamma_{02}Intervention_{i} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} + \\gamma_{11}Intervention_{i} + U_{1i}\\]"
  },
  {
    "objectID": "conditional-5.html#centering-redux",
    "href": "conditional-5.html#centering-redux",
    "title": "conditional",
    "section": "Centering redux",
    "text": "Centering redux\n\nIf we just leave the exercise variable by itself there is often a combination of within and between person information. The two sources are “smushed” together.\nI could exercise more than the average person (between person) but my research question is about what happens when I exercise a lot, does that correspond to health. Because my excercise level varies there is within person information.\nIt is not clear whether exercise is associated with health BECAUSE people who exercise tend to be healthier (a between person question that could be addressed with cross sectional data) or is it because when someone exercises they feel healthier.\nHow is \\(\\gamma_{20}Exercise_{ti}\\) interpreted?"
  },
  {
    "objectID": "conditional-5.html#time-considerations",
    "href": "conditional-5.html#time-considerations",
    "title": "conditional",
    "section": "Time considerations",
    "text": "Time considerations\n\nWe typically center time around each person’s initial time to make the intercept more interpretable. However, this can cause correlations between an intercept and a slope.\nIf high, the correlation can be problematic in terms of estimation. Often we center time in the middle of the repeated assessments to minimize this association.\nDoing so is especially important if you want to use some variable to predict intercept and slope (or use intercept/slope to predict some variable)."
  },
  {
    "objectID": "conditional-5.html#level-2-centering",
    "href": "conditional-5.html#level-2-centering",
    "title": "conditional",
    "section": "Level 2 centering",
    "text": "Level 2 centering\n\nBecause level 2 is involved with cross-level interactions, it is always helpful to at least consider centering. For level 2, the centering options are much easier, as one can generally go with grand mean centering.\nAs everyone has only 1 value to contribute to, the calculation and the interpretation is more straightforward. It is thus also equivalent to group grand-mean centering."
  },
  {
    "objectID": "conditional-5.html#level-1-predictors",
    "href": "conditional-5.html#level-1-predictors",
    "title": "conditional",
    "section": "Level 1 predictors",
    "text": "Level 1 predictors\n\nThese are predictors that repeat.\nSome variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income).\nThese variables can be treated as another predictor with the effect of “controlling” for some level 1 variable as well as a focal variable."
  },
  {
    "objectID": "conditional-5.html#sem",
    "href": "conditional-5.html#sem",
    "title": "conditional",
    "section": "SEM",
    "text": "SEM\n\nalcohol.wide <- alcohol %>% \n  dplyr::select(-X, -age_14, -ccoa) %>% \n  pivot_wider(names_from = \"age\", \n              names_prefix = \"alcuse_\",\n              values_from  = alcuse)"
  },
  {
    "objectID": "conditional-5.html#sem-covariatespredictors",
    "href": "conditional-5.html#sem-covariatespredictors",
    "title": "conditional",
    "section": "SEM covariates/predictors",
    "text": "SEM covariates/predictors\nThe strategy for including predictors is similar to working with a level 2 mlm equation. Do you want to predict the intercept or the slope? No need to explicitly model an interaction as that is implied.\n\n\nCode\nlibrary(lavaan)\nsem.1 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16\n               \n# regressions\n    \n    i ~ male \n    s ~ male\n'\nfit.1 <- growth(sem.1, data = alcohol.wide)\nsummary(fit.1)\n\n\nlavaan 0.6-11 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 4.213\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.122\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         1.000                           \n    alcuse_16         2.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~                                                 \n    male             -0.110    0.206   -0.534    0.594\n  s ~                                                 \n    male              0.272    0.120    2.261    0.024\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .i ~~                                                \n   .s                -0.173    0.098   -1.762    0.078\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n   .i                 0.692    0.147    4.701    0.000\n   .s                 0.136    0.086    1.575    0.115\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.080    0.147    0.546    0.585\n   .alcuse_15         0.432    0.095    4.537    0.000\n   .alcuse_16         0.216    0.175    1.236    0.216\n   .i                 0.789    0.191    4.127    0.000\n   .s                 0.225    0.082    2.749    0.006"
  },
  {
    "objectID": "conditional-5.html#centered-predictors",
    "href": "conditional-5.html#centered-predictors",
    "title": "conditional",
    "section": "centered predictors",
    "text": "centered predictors\n\n\nCode\nsem.3 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16\n               \n# regressions\n    \n    i ~ cpeer\n    s ~  cpeer\n'\nfit.3 <- growth(sem.3, data = alcohol.wide)\nsummary(fit.3)\n\n\nlavaan 0.6-11 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 2.406\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.300\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         1.000                           \n    alcuse_16         2.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~                                                 \n    cpeer             0.736    0.117    6.288    0.000\n  s ~                                                 \n    cpeer            -0.139    0.084   -1.656    0.098\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .i ~~                                                \n   .s                -0.075    0.082   -0.912    0.362\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n   .i                 0.642    0.085    7.562    0.000\n   .s                 0.276    0.061    4.531    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.175    0.124    1.416    0.157\n   .alcuse_15         0.375    0.083    4.529    0.000\n   .alcuse_16         0.349    0.175    1.990    0.047\n   .i                 0.430    0.138    3.114    0.002\n   .s                 0.177    0.076    2.312    0.021"
  },
  {
    "objectID": "conditional-5.html#multiple-predictors-1",
    "href": "conditional-5.html#multiple-predictors-1",
    "title": "conditional",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\n\nCode\nsem.4 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 \n            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16\n               \n# regressions\n    \n    i ~ cpeer + male\n    s ~ cpeer + male\n'\nfit.4 <- growth(sem.4, data = alcohol.wide)\nsummary(fit.4)\n\n\nlavaan 0.6-11 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 5.134\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.162\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    alcuse_14         1.000                           \n    alcuse_15         1.000                           \n    alcuse_16         1.000                           \n  s =~                                                \n    alcuse_14         0.000                           \n    alcuse_15         1.000                           \n    alcuse_16         2.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~                                                 \n    cpeer             0.750    0.120    6.265    0.000\n    male              0.105    0.174    0.603    0.546\n  s ~                                                 \n    cpeer            -0.107    0.084   -1.271    0.204\n    male              0.241    0.122    1.977    0.048\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .i ~~                                                \n   .s                -0.090    0.080   -1.132    0.257\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.000                           \n   .alcuse_15         0.000                           \n   .alcuse_16         0.000                           \n   .i                 0.587    0.123    4.783    0.000\n   .s                 0.150    0.086    1.742    0.081\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .alcuse_14         0.162    0.123    1.316    0.188\n   .alcuse_15         0.397    0.084    4.712    0.000\n   .alcuse_16         0.274    0.166    1.649    0.099\n   .i                 0.439    0.139    3.166    0.002\n   .s                 0.183    0.074    2.480    0.013"
  },
  {
    "objectID": "conditional-5.html#centering-mlm-redux",
    "href": "conditional-5.html#centering-mlm-redux",
    "title": "conditional",
    "section": "Centering mlm redux",
    "text": "Centering mlm redux\nChanging the scale of your predictors changes the interpretation of your model.\n\nOriginal metric (no centering)\nGroup-mean centering (our group/nesting is person so this is also called person centering). This will be more appropriate when we talk about level 1 predictors.\nGroup grand-mean centering (centering around person avg)\nGrand-mean centering (this is taking the average across every obs)\nCentering on a value of theoretical or applied interest"
  },
  {
    "objectID": "conditional-5.html#what-if-time-is-not-focal",
    "href": "conditional-5.html#what-if-time-is-not-focal",
    "title": "conditional",
    "section": "What if time is not focal?",
    "text": "What if time is not focal?\nUsually time is not an important factor for studies that last for weeks or less. Instead we often we want to look at repeated associations or correlations across time. What happens to the time variable? Two options:\n\nIgnore it. A time variable is asking whether the variable we care about is “associated” with time. Maybe that isn’t important to us, theoretically.\nUse it to control to increases in your DV across time, but mostly disregard. Time could serve to control for the messiness of daily life, even if it is not a focal variable."
  },
  {
    "objectID": "conditional-5.html#introducing-a-random-slope-for-a-tvc",
    "href": "conditional-5.html#introducing-a-random-slope-for-a-tvc",
    "title": "conditional",
    "section": "Introducing a random slope for a TVC",
    "text": "Introducing a random slope for a TVC\nPerson specific residuals make the interpretation of parameters a little more difficult as the model says that the gap between exercise and not exercise is the same for everyone. Should we allow it to be this way?\nlevel 1:\n\\[{Health}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\beta_{2i}Exercise_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} +   U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  U_{1i}\\]\n\\[{\\beta}_{2i} = \\gamma_{20} +  U_{2i}\\]"
  },
  {
    "objectID": "conditional-5.html#where-does-the-variance-go",
    "href": "conditional-5.html#where-does-the-variance-go",
    "title": "conditional",
    "section": "Where does the variance go?",
    "text": "Where does the variance go?\nCompared to time invariant (level 2) predictors, tvc/level 1 predictors are likely to explain variance level 1 and level 2 variance terms. This is because level 1 predictors can include both level 1 and level 2 information.\nTypically level 2 predictors tend to only reduce level 2 variance. It is possible, however, that including a level 1 predictor will increase the variance in level 2 variance components."
  },
  {
    "objectID": "conditional-5.html#interactions-among-level-1-variables",
    "href": "conditional-5.html#interactions-among-level-1-variables",
    "title": "conditional",
    "section": "Interactions among level 1 variables",
    "text": "Interactions among level 1 variables\nCouldn’t exercise levels influence the slope of health? The previous models constrained the slopes to be the same, saying that people differ on level when exercising vs not but not on rate of change.\n\\[{Health}_{ti} =  [\\gamma_{00} +   \\gamma_{10}Time_{ti}   + \\gamma_{20}Exercise_{ti} + \\gamma_{30}TimeXExercise_{ti}]\\]\n\\[+ [ U_{0i}  + U_{1i}Time_{ti}+ \\varepsilon_{ti}]\\]\n\nHow could you visualize this model?\nHow do you interpret each of the terms (knowing what you know about interactions)?\nHow would all of this change if our level 1 variable was continuous?"
  },
  {
    "objectID": "conditional-5.html#person-mean-centering",
    "href": "conditional-5.html#person-mean-centering",
    "title": "conditional",
    "section": "person mean centering",
    "text": "person mean centering\n\nTypically for level 1 we will want to within person-mean center to help with the issue of “smushed” variance. Especially when you are working with level 1 interactions, centering is important to interpret your lower order terms.\nHowever, this gets rid of all mean level information for a person. The question at hand is not whether you exercise more or less it is compared to your typical levels, what happens when you exercise more or less.\nIf you are including a level 1 person centered variable in the model, note that 1) the average level of exercise is not controlled for and 2) the variation around the level will likely be related to the persons mean score. In other words, the within and between person variance of exercise is not neatly decomposed."
  },
  {
    "objectID": "conditional-5.html#multiple-level-1-predictors",
    "href": "conditional-5.html#multiple-level-1-predictors",
    "title": "conditional",
    "section": "Multiple level 1 predictors",
    "text": "Multiple level 1 predictors\n\\[{Health}_{ti} =  [\\gamma_{00} +   \\gamma_{10}Time_{ti}   + \\gamma_{20}(Exercise_{ti}-\\overline{Exercise_{i}}) + \\gamma_{30}Mood_{ti}\\]\n\\[+\\gamma_{40}(Mood*(Exercise_{ti}-\\overline{Exercise_{i}}))_{ti} ] + [ U_{0i}  + U_{1i}Time_{ti}+ \\varepsilon_{ti}]\\]\nHow would \\(\\gamma_{20}\\) and \\(\\gamma_{40}\\) change in interpretation if exercise was not person centered?"
  },
  {
    "objectID": "conditional-5.html#what-happens-if-we-want-to-add-a-level-2-predictor",
    "href": "conditional-5.html#what-happens-if-we-want-to-add-a-level-2-predictor",
    "title": "conditional",
    "section": "What happens if we want to add a level 2 predictor?",
    "text": "What happens if we want to add a level 2 predictor?\n\\[{Health}_{ti} =  [\\gamma_{00} + \\gamma_{10}Time_{ti} + \\gamma_{20}(Exercise_{ti}-\\overline{Exercise_{i}}) +\\gamma_{30}Mood_{ti}+\\gamma_{31}(Mood*\\overline{Exercise_{i}})_{ti}\\]\n\\[+\\gamma_{40}(Mood*(Exercise_{ti}-\\overline{Exercise_{i}}))_{ti} ] + [ U_{0i}  + U_{1i}Time_{ti}+ \\varepsilon_{ti}]\\]"
  },
  {
    "objectID": "conditional-5.html#lagged-models",
    "href": "conditional-5.html#lagged-models",
    "title": "conditional",
    "section": "Lagged models",
    "text": "Lagged models\nWhat if I want to predict something in the future?\nLevel 1\n\\[{Health}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\beta_{2i}(Exercise_{(t-1)i}-\\overline{Exercise_{i}}) + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}\\overline{Exercise_{i}} + U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} + \\gamma_{11}\\overline{Exercise_{i}} + U_{1i}\\]\n\\[{\\beta}_{2i} = \\gamma_{20}\\]"
  },
  {
    "objectID": "conditional-5.html#contextual-models",
    "href": "conditional-5.html#contextual-models",
    "title": "conditional",
    "section": "Contextual models",
    "text": "Contextual models\nGrand mean centering our level 1 variables is another option. We have already seen that when we use time as level 1 predictor. The between and within person variance is now “smushed” together when we do not person center.\nLevel 1:\n\\[{Health}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\beta_{2i}(Exercise_{ti}-\\overline{Exercise_{..}}) + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}(\\overline{Exercise_{i}}-\\overline{Exercise_{..}})+ U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  U_{1i}\\]\n\\[{\\beta}_{2i} = \\gamma_{20}\\]"
  },
  {
    "objectID": "conditional-5.html#categorical-tvc",
    "href": "conditional-5.html#categorical-tvc",
    "title": "conditional",
    "section": "categorical TVC",
    "text": "categorical TVC\nDepending on your goal it might be confusing/difficult to person mean center. The issue is that centering in this manner leads to values that do not exist e.g. -.5 when your TVC = 0, for example. This also leads to potentially strange/uninterpretable intercepts or other lower order terms.\nSuggested to keep raw tvc at level 1 but GMC at level 2.\nSometimes time variables can be like this. For a recent paper we coded a life event as a tvc, which effectively switched on and of when the person encountered a life event.\n\n\n\nID\nwave\nevent\n\n\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n1\n2\n1\n\n\n2\n0\n0\n\n\n2\n1\n1"
  },
  {
    "objectID": "conditional-5.html#sem-tvcs",
    "href": "conditional-5.html#sem-tvcs",
    "title": "conditional",
    "section": "SEM TVCs",
    "text": "SEM TVCs\nLevel 1 variables (tvs) are flexibly handled in SEM. Whereas previously a level 1 variable had a constant effect (ie you just had a single parameter) here you can model separate effects for different times. Or you can have a tvc for one timepoint but not another. Doing this in an MLM framework is awkward at best."
  },
  {
    "objectID": "conditional-5.html#seperating-between-and-within-person-effects",
    "href": "conditional-5.html#seperating-between-and-within-person-effects",
    "title": "conditional",
    "section": "Seperating between and within person effects",
    "text": "Seperating between and within person effects\nWe will create a new variable out of the existing level 1 variable, a person mean. This makes the effects of the person level uncorrelated with the effects of the between person level\n\\[{Health}_{ti} = \\beta_{0i}  + \\beta_{1i}Time_{ti} + \\beta_{2i}(Exercise_{ti}-\\overline{Exercise_{i}}) + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + \\gamma_{01}\\overline{Exercise_{i}} + U_{0i}\\]\n\\[{\\beta}_{1i} = \\gamma_{10} +  U_{1i}\\]\n\\[{\\beta}_{2i} = \\gamma_{20}\\]"
  },
  {
    "objectID": "conditional-5.html#level-1-intensive-vs-long-term",
    "href": "conditional-5.html#level-1-intensive-vs-long-term",
    "title": "conditional",
    "section": "Level 1 intensive vs long term",
    "text": "Level 1 intensive vs long term\n\nThe question of whether to center or not at level 1 partially depends on the type of data we are analyzing.\nFor intensive data, we will want to center for two related reasons: 1. makes the coefficient about within person deviations and 2. helps us separate within and between person effects\nFor longer term data, level 1 is often used as a covariate rather than a focal coefficient. As a result, centering becomes less important."
  }
]