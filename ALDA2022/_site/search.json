[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Longitudinal Data Analyses",
    "section": "",
    "text": "Please check the homepage regularly for class updates and announcements."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro-1.html#how-to-think-longitudinal-y",
    "href": "intro-1.html#how-to-think-longitudinal-y",
    "title": "Thinking longitudinally",
    "section": "how to think longitudinal-y",
    "text": "how to think longitudinal-y\n\n\n\nlines/ trajectories\nvariance decomposition"
  },
  {
    "objectID": "intro-1.html#goals-for-today",
    "href": "intro-1.html#goals-for-today",
    "title": "Thinking longitudinally",
    "section": "Goals for today:",
    "text": "Goals for today:\n\n\n\n\nGet a feeling for how to think/talk about longitudinal/repeated measures data\nIntroduce some important terms\nBegin to develop a framework for analysis"
  },
  {
    "objectID": "intro-1.html#how-do-we-define-change",
    "href": "intro-1.html#how-do-we-define-change",
    "title": "Thinking longitudinally",
    "section": "How do we define “change”?",
    "text": "How do we define “change”?\n\nTypes of change (most common):\nDifferential / rank order consistency/rank order stability (correlations)\nMean level/ absolute change (mean differences)"
  },
  {
    "objectID": "intro-1.html#how-do-we-defining-change",
    "href": "intro-1.html#how-do-we-defining-change",
    "title": "Thinking longitudinally",
    "section": "How do we defining “change”?",
    "text": "How do we defining “change”?\n\nBecause there are many types of change, we will view change in terms of the model.\n(Usually) it is clearer to refer to the type of change in terms of an equation or pictorially. Putting a word onto it usually causes confusion, which is why there are a lot of redundant terms in the literature.\nModels may be able to tell us about two different types of change (within person vs between person change)"
  },
  {
    "objectID": "intro-1.html#prerequisites",
    "href": "intro-1.html#prerequisites",
    "title": "Thinking longitudinally",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOrdinal or greater scale of measurement us easiest. Dichotomous is hard.\nConstruct has the same meaning across measurement occasions. Usually the same items. Called measurement invariance. Complicates developmental work.\n2 or more measurement occasions. More is better! Though often 3 - 10 is practically fine for some models. With 30+ occasions you have “intensive” longitudinal data which presents new models and opportunities."
  },
  {
    "objectID": "intro-1.html#defining-time-metric",
    "href": "intro-1.html#defining-time-metric",
    "title": "Thinking longitudinally",
    "section": "Defining time metric",
    "text": "Defining time metric\n\nTime is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable.\nWhat is the process that is changing someone? Age? Time in study? Year? Wave?\nIs it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. If you are running an intervention you may want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level.\n\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#defining-a-time-metric",
    "href": "intro-1.html#defining-a-time-metric",
    "title": "Thinking longitudinally",
    "section": "Defining a time metric",
    "text": "Defining a time metric\n\nTime is the most important part of a longitudinal analyses. The key to interpreting your output is to know how you handled your time variable.\nWhat is the process that is changing someone? Age? Time in study? Year? Wave?\nIs it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. If you are running an intervention you may want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level."
  },
  {
    "objectID": "intro-1.html#example",
    "href": "intro-1.html#example",
    "title": "Thinking longitudinally",
    "section": "Example",
    "text": "Example\n\nDataPlot\n\n\n\n\n# A tibble: 225 × 51\n      ID group  time    CON CON_SAL CON_SMN7   DAN DAN_CON DAN_SAL DAN_SMN7\n   <dbl> <chr> <dbl>  <dbl>   <dbl>    <dbl> <dbl>   <dbl>   <dbl>    <dbl>\n 1     6 PD        4 0.193   0.0708   0.0088 0.162  0.116   0.0739   0.0041\n 2     6 PD        5 0.195   0.0862   0.004  0.168  0.117   0.0812   0.0204\n 3     6 PD        6 0.181   0.0916  -0.0037 0.215  0.164   0.113    0.0155\n 4    29 PD        4 0.159   0.0438  -0.0253 0.175  0.0161  0.0621   0.0121\n 5    29 PD        5 0.0881  0.0446  -0.0288 0.136  0.0377  0.0277   0.0444\n 6    34 CTRL      4 0.137   0.0113  -0.0792 0.166  0.0045 -0.0075   0.0432\n 7    34 CTRL      6 0.0746  0.0009  -0.0089 0.140  0.0635 -0.024    0.0483\n 8    36 CTRL      1 0.139   0.0438  -0.0466 0.152  0.0279  0.0137   0.0513\n 9    36 CTRL      4 0.180   0.0772  -0.0648 0.205 -0.0116 -0.0477   0.0415\n10    37 PD        3 0.226   0.0412  -0.0565 0.219  0.0971  0.0195   0.0644\n# … with 215 more rows, and 41 more variables: DMN6 <dbl>, DMN6_CON <dbl>,\n#   DMN6_DAN <dbl>, DMN6_SAL <dbl>, DMN6_SMN7 <dbl>, SAL <dbl>, SAL_SMN7 <dbl>,\n#   SMN7 <dbl>, wave <dbl>, date <date>, Exclude <chr>, RSNdata <dbl>,\n#   RSNexclude <chr>, RSNexcludeDevDem <chr>, CogDate_0 <date>,\n#   CCIRtrio_MR_date_0 <date>, Dur_PDsx_0 <dbl>, PIBpos18 <chr>,\n#   Neuro_Dx <chr>, NeuroCDR_0 <chr>, NeuroCDR_1 <chr>, NeuroCDR_2 <chr>,\n#   NeuroCDR_3 <chr>, NeuroCDR_4 <chr>, NeuroCDR_5 <chr>, NeuroCDR_6 <chr>, …\n\n\n\n\n\nggplot(example,\n   aes(x = year, y = SMN7, group = ID)) + geom_point()"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Motivation and handling data\nLongitudinal MLM"
  },
  {
    "objectID": "intro-1.html#individual-level",
    "href": "intro-1.html#individual-level",
    "title": "Thinking longitudinally",
    "section": "Individual level",
    "text": "Individual level\n\nggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)"
  },
  {
    "objectID": "intro-1.html#theoretical-model-of-change",
    "href": "intro-1.html#theoretical-model-of-change",
    "title": "Thinking longitudinally",
    "section": "Theoretical model of change",
    "text": "Theoretical model of change\n\nThe shape we want to model - linear, quadradic, cyclical, etc.\nIs shape related to calendar time, age, or maybe artificial time such as grade"
  },
  {
    "objectID": "intro-1.html#section",
    "href": "intro-1.html#section",
    "title": "Thinking longitudinally",
    "section": "",
    "text": "# A tibble: 20 × 6\n# Groups:   ID [10]\n      ID term        estimate std.error statistic  p.value\n   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1    67 (Intercept)  0.0921    0.0161      5.72    0.0292\n 2    67 week         0.00662   0.00657     1.01    0.420 \n 3    75 (Intercept)  0.126   NaN         NaN     NaN     \n 4    75 week         0.00771 NaN         NaN     NaN     \n 5    87 (Intercept)  0.0787  NaN         NaN     NaN     \n 6    87 week        -0.0227  NaN         NaN     NaN     \n 7    99 (Intercept)  0.111     0.0236      4.69    0.0426\n 8    99 week         0.00545   0.0122      0.446   0.699 \n 9   101 (Intercept)  0.111     0.0424      2.62    0.232 \n10   101 week         0.0421    0.0217      1.94    0.303 \n11   103 (Intercept)  0.0887  NaN         NaN     NaN     \n12   103 week        -0.0168  NaN         NaN     NaN     \n13   105 (Intercept)  0.0465    0.00658     7.06    0.0896\n14   105 week         0.00122   0.00467     0.261   0.838 \n15   142 (Intercept)  0.197   NaN         NaN     NaN     \n16   142 week         0.0130  NaN         NaN     NaN     \n17   149 (Intercept)  0.0801  NaN         NaN     NaN     \n18   149 week         0.00497 NaN         NaN     NaN     \n19   152 (Intercept)  0.0921  NaN         NaN     NaN     \n20   152 week        -0.0172  NaN         NaN     NaN     \n\n\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#how-do-we-define-change-1",
    "href": "intro-1.html#how-do-we-define-change-1",
    "title": "Thinking longitudinally",
    "section": "How do we define “change”?",
    "text": "How do we define “change”?\n\nBecause there are many types of change, we will view change in terms of the model.\n(Usually) it is clearer to refer to the type of change in terms of an equation or pictorially. Putting a word onto it usually causes confusion, which is why there are a lot of redundant terms in the literature.\nModels may be able to tell us about two different types of change (within person vs between person change)"
  },
  {
    "objectID": "intro-1.html#questions-we-need-to-answer",
    "href": "intro-1.html#questions-we-need-to-answer",
    "title": "Thinking longitudinally",
    "section": "Questions we need to answer",
    "text": "Questions we need to answer\n\nWhat is the theoretical shape we want to model - linear, quadradic, cyclical, etc?\nIs shape related to calendar time, age, or maybe artificial time such as grade?"
  },
  {
    "objectID": "intro-1.html#temporal-design",
    "href": "intro-1.html#temporal-design",
    "title": "Thinking longitudinally",
    "section": "Temporal design",
    "text": "Temporal design\n\nI.e., timing, frequency, and spacing of assessments.\nHow longitudinal data are collected will impact our ability to model the theoretical shape.\nBecause of the difficulty of collecting longitudinal data, a lot of longitudinal data are under specified."
  },
  {
    "objectID": "intro-1.html#statistical-model",
    "href": "intro-1.html#statistical-model",
    "title": "Thinking longitudinally",
    "section": "Statistical model",
    "text": "Statistical model\n\nWith a theoretical model of change in mind, and a good temporal design, we can then choose our statistical model.\nThis matching of theory with design with a model is similar to all of stats. We will be using three general purpose models that are related, but have pros and cons in different areas: MLM, SEM, and GAMs"
  },
  {
    "objectID": "intro-1.html#simple-to-begin-with",
    "href": "intro-1.html#simple-to-begin-with",
    "title": "Thinking longitudinally",
    "section": "Simple to begin with",
    "text": "Simple to begin with\nBefore we get too fancy, lets just run some regressions."
  },
  {
    "objectID": "intro-1.html#individual-regression-output",
    "href": "intro-1.html#individual-regression-output",
    "title": "Thinking longitudinally",
    "section": "Individual regression output",
    "text": "Individual regression output\n\n\nCode\nlibrary(broom)\nregressions <- example2 %>% \n  group_by(ID) %>% \n  do(tidy(lm(SMN7 ~ week, data=.)))\n\nregressions\n\n\n# A tibble: 20 × 6\n# Groups:   ID [10]\n      ID term        estimate std.error statistic  p.value\n   <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1    67 (Intercept)  0.0921    0.0161      5.72    0.0292\n 2    67 week         0.00662   0.00657     1.01    0.420 \n 3    75 (Intercept)  0.126   NaN         NaN     NaN     \n 4    75 week         0.00771 NaN         NaN     NaN     \n 5    87 (Intercept)  0.0787  NaN         NaN     NaN     \n 6    87 week        -0.0227  NaN         NaN     NaN     \n 7    99 (Intercept)  0.111     0.0236      4.69    0.0426\n 8    99 week         0.00545   0.0122      0.446   0.699 \n 9   101 (Intercept)  0.111     0.0424      2.62    0.232 \n10   101 week         0.0421    0.0217      1.94    0.303 \n11   103 (Intercept)  0.0887  NaN         NaN     NaN     \n12   103 week        -0.0168  NaN         NaN     NaN     \n13   105 (Intercept)  0.0465    0.00658     7.06    0.0896\n14   105 week         0.00122   0.00467     0.261   0.838 \n15   142 (Intercept)  0.197   NaN         NaN     NaN     \n16   142 week         0.0130  NaN         NaN     NaN     \n17   149 (Intercept)  0.0801  NaN         NaN     NaN     \n18   149 week         0.00497 NaN         NaN     NaN     \n19   152 (Intercept)  0.0921  NaN         NaN     NaN     \n20   152 week        -0.0172  NaN         NaN     NaN"
  },
  {
    "objectID": "intro-1.html#if-we-simply-average-these-we-get",
    "href": "intro-1.html#if-we-simply-average-these-we-get",
    "title": "Thinking longitudinally",
    "section": "If we simply average these we get",
    "text": "If we simply average these we get\nPopulation level estimates ::: {.cell}\nregressions %>% \n  group_by(term) %>% \n  summarise(avg.reg = mean(estimate))\n\n# A tibble: 2 × 2\n  term        avg.reg\n  <chr>         <dbl>\n1 (Intercept) 0.102  \n2 week        0.00244\n\n:::\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#average-each-regression",
    "href": "intro-1.html#average-each-regression",
    "title": "Thinking longitudinally",
    "section": "Average each regression",
    "text": "Average each regression\n\nregressions %>% \n  group_by(term) %>% \n  summarise(avg.reg = mean(estimate))\n\n# A tibble: 2 × 2\n  term        avg.reg\n  <chr>         <dbl>\n1 (Intercept) 0.102  \n2 week        0.00244\n\n\nThis is not that far off from what MLM gives us."
  },
  {
    "objectID": "intro-1.html#spaghetti-plot",
    "href": "intro-1.html#spaghetti-plot",
    "title": "Thinking longitudinally",
    "section": "Spaghetti Plot",
    "text": "Spaghetti Plot\n\n\nCode\nggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = \"lm\", se = FALSE) +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = \"black\"), method = \"lm\", size = 2) + guides(fill=\"none\")+ theme(legend.position=\"none\")"
  },
  {
    "objectID": "intro-1.html#thinking-longitudinally",
    "href": "intro-1.html#thinking-longitudinally",
    "title": "Thinking longitudinally",
    "section": "Thinking longitudinally",
    "text": "Thinking longitudinally\nAlmost all of the questions we have can be simplified down to: lines/trajectories.\n\nPerson level trajectories index change for a person\nAverage person trajectory is the average trajectory\nPredictors of change are just a correlation with the trajectory and the predictor"
  },
  {
    "objectID": "intro-1.html#lines-regardless-of-the-stat-model",
    "href": "intro-1.html#lines-regardless-of-the-stat-model",
    "title": "Thinking longitudinally",
    "section": "Lines, regardless of the stat model",
    "text": "Lines, regardless of the stat model\n\nMLM and SEM (and even GAMs) can be equivalent\nWe will start with MLM/HLM as it is simple extension of standard regression models. Best suited to run models when the time of measurement differs from person to person (compared to equal intervals). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person"
  },
  {
    "objectID": "intro-1.html#why-not-rm-anova",
    "href": "intro-1.html#why-not-rm-anova",
    "title": "Thinking longitudinally",
    "section": "Why not RM ANOVA?",
    "text": "Why not RM ANOVA?\n\nCannot handle missing data\nAssumes rate of change is the same for all individuals.\nTime is categorical.\nAccounting for correlation across time uses up many parameters (df penalty).\nCannot handle some types of predictors\nSpecial case of MLM, might as well learn/use flexible model"
  },
  {
    "objectID": "intro-1.html#how-to-think-longitudinal-y-1",
    "href": "intro-1.html#how-to-think-longitudinal-y-1",
    "title": "Thinking longitudinally",
    "section": "how to think longitudinal-y",
    "text": "how to think longitudinal-y\n\nlines/ trajectories\nvariance decomposition"
  },
  {
    "objectID": "intro-1.html#modeling-dependency",
    "href": "intro-1.html#modeling-dependency",
    "title": "Thinking longitudinally",
    "section": "Modeling dependency",
    "text": "Modeling dependency\nWe have multiple DVs per person with longitudinal data. If we ignored the person aspect, the residuals would likely be related, violating standard regression assumption. MLM accounts for residuals for outcomes from the same person through modeling different “levels”\nWith longitudinal data we have people nested within observations.\nLevel 1: observation level (observation specific variance)\nLevel 2: person level (person specific variance)"
  },
  {
    "objectID": "intro-1.html#person-specific-variance",
    "href": "intro-1.html#person-specific-variance",
    "title": "Thinking longitudinally",
    "section": "Person specific variance",
    "text": "Person specific variance\nSome people start at different levels and some people change at different rates\n\n\nCode\nggplot(example, aes(x = year, y = SMN7, group = ID, colour = ID)) + stat_smooth(method = \"lm\", se = FALSE, alpha = .5) + theme(legend.position=\"none\")"
  },
  {
    "objectID": "intro-1.html#observation-level-variance",
    "href": "intro-1.html#observation-level-variance",
    "title": "Thinking longitudinally",
    "section": "Observation level variance",
    "text": "Observation level variance\nAfter account for a person starting level and their slope, there is still residual variance left over.\n\n\nCode\nob.var <- example %>% \n  filter(ID %in% c(\"67\",\"82\", \"110\")) \n\nexample3 <-\n  left_join(ob.var, example)  \n  \nggplot(example3,\n   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method=\"lm\", se = FALSE) + facet_wrap( ~ID)"
  },
  {
    "objectID": "intro-1.html#thinking-about-variation",
    "href": "intro-1.html#thinking-about-variation",
    "title": "Thinking longitudinally",
    "section": "Thinking about variation",
    "text": "Thinking about variation\nA goal of longitudinal data analysis (and all other data analysis) is to explain this variation. We will fit models that includes predictors and model constraints (e.g. are people similar or different) to see how it impacts variation.\nTo the extent that we can put variance into different “piles” (eg people change at different rates, a random slope) we will have more explained variance and less unexplained variance."
  },
  {
    "objectID": "intro-1.html#speaking-of-variation",
    "href": "intro-1.html#speaking-of-variation",
    "title": "Thinking longitudinally",
    "section": "Speaking of variation",
    "text": "Speaking of variation\n\nBetween-Person (BP) Variation/Level-2/INTER-individual differences/Time-Invariant\nBP = More/less than other people\nWithin-Person (WP) Variation/Level-1/INTRA-individual Differences/Time-Varying\nWP = more/less than one’s average\nAny variable measured over time usually has both BP and WP variation"
  },
  {
    "objectID": "intro-1.html#within-person-focus",
    "href": "intro-1.html#within-person-focus",
    "title": "Thinking longitudinally",
    "section": "Within person focus",
    "text": "Within person focus\nWithin-Person Change: Systematic (lasting) change Magnitude or direction of change can be different across individuals. Can refer to between person (inter individual differences) in within person change (intra individual)\nWithin-Person Fluctuation: No systematic change Outcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual\n\n\nhttps://alda22.netlify.app"
  },
  {
    "objectID": "intro-1.html#change-vs-fluctuations",
    "href": "intro-1.html#change-vs-fluctuations",
    "title": "Thinking longitudinally",
    "section": "Change vs fluctuations",
    "text": "Change vs fluctuations\n\nFuzzy boundary, but:\nWithin-Person Change: Systematic (lasting) change. Can refer to between-person (inter-individual) differences in within-person change (intra-individual)\nWithin-Person Fluctuation: No systematic change Outcome just varies/fluctuates over time (e.g., emotion, stress). Time is just a way to get lots of data per individual"
  },
  {
    "objectID": "intro-1.html#what-are-data",
    "href": "intro-1.html#what-are-data",
    "title": "Thinking longitudinally",
    "section": "What Are Data?",
    "text": "What Are Data?\nData are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean .csv, .xls, .sav, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.\nWhy are we thinking about data? Because 80%, maybe more, of your time spent with “analysis” is spent getting data in order and setting up your model of interest."
  },
  {
    "objectID": "intro-1.html#wide-vs-long",
    "href": "intro-1.html#wide-vs-long",
    "title": "Thinking longitudinally",
    "section": "Wide vs long",
    "text": "Wide vs long\n\nmultivariate vs stacked\nperson vs person period\nuntidy vs tidy*\nLong is what MLM, ggplot2 and tidyverse packages expect whereas SEM and a lot of descriptives are calculated using wide dataframes."
  },
  {
    "objectID": "intro-1.html#tidyr",
    "href": "intro-1.html#tidyr",
    "title": "Thinking longitudinally",
    "section": "tidyr",
    "text": "tidyr\nFor longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows.\n\n\n# A tibble: 225 × 4\n      ID  wave group   DAN\n   <dbl> <dbl> <chr> <dbl>\n 1     6     1 PD    0.162\n 2     6     2 PD    0.168\n 3     6     3 PD    0.215\n 4    29     1 PD    0.175\n 5    29     2 PD    0.136\n 6    34     1 CTRL  0.166\n 7    34     2 CTRL  0.140\n 8    36     1 CTRL  0.152\n 9    36     2 CTRL  0.205\n10    37     1 PD    0.219\n# … with 215 more rows"
  },
  {
    "objectID": "intro-1.html#pivot_wider",
    "href": "intro-1.html#pivot_wider",
    "title": "Thinking longitudinally",
    "section": "pivot_wider",
    "text": "pivot_wider\nThe pivot_wider() function takes two arguments: names_from which is the variable whose values will be converted to column names and values_from whose values will be cell values.\n\nwide.ex <- long %>% \n  pivot_wider(names_from = wave, values_from = DAN) \nwide.ex\n\n# A tibble: 91 × 6\n      ID group    `1`    `2`    `3`   `4`\n   <dbl> <chr>  <dbl>  <dbl>  <dbl> <dbl>\n 1     6 PD    0.162  0.168   0.215    NA\n 2    29 PD    0.175  0.136  NA        NA\n 3    34 CTRL  0.166  0.140  NA        NA\n 4    36 CTRL  0.152  0.205  NA        NA\n 5    37 PD    0.219  0.158   0.259    NA\n 6    48 PD    0.130  0.270   0.248    NA\n 7    53 CTRL  0.211  0.152  NA        NA\n 8    54 PD    0.220  0.152   0.192    NA\n 9    58 PD    0.380  0.215   0.204    NA\n10    61 PD    0.0818 0.0628 NA        NA\n# … with 81 more rows"
  },
  {
    "objectID": "intro-1.html#pivot_longer",
    "href": "intro-1.html#pivot_longer",
    "title": "Thinking longitudinally",
    "section": "pivot_longer",
    "text": "pivot_longer\nThe pivot_longer function takes three arguments: - cols is a list of columns that are to be collapsed. The columns can be referenced by column number or column name. - names_to is the name of the new column which will combine all column names. This is up to you to decide what the name is. - values_to is the name of the new column which will combine all column values associated with each variable combination."
  },
  {
    "objectID": "intro-1.html#seperate-and-unite",
    "href": "intro-1.html#seperate-and-unite",
    "title": "Thinking longitudinally",
    "section": "Seperate and Unite",
    "text": "Seperate and Unite\n\nMany times datasets are, for a lack of a better term, messy.\nOne common way to represent longitudinal data is to name the variable with a wave signifier.\n\n\n\n# A tibble: 3 × 4\n     ID ext_1 ext_2 ext_3\n  <dbl> <dbl> <dbl> <dbl>\n1     1     4     4     4\n2     2     6     5     4\n3     3     4     5     6"
  },
  {
    "objectID": "intro-1.html#date-time-metrics",
    "href": "intro-1.html#date-time-metrics",
    "title": "Thinking longitudinally",
    "section": "Date time metrics",
    "text": "Date time metrics\n\nlibrary(lubridate)\n\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). These are called POSIXct in R.\n\ntoday()\n\n[1] \"2022-08-31\"\n\n\n\nnow()\n\n[1] \"2022-08-31 13:52:03 CDT\""
  },
  {
    "objectID": "intro-1.html#projects-and-rmarkdown",
    "href": "intro-1.html#projects-and-rmarkdown",
    "title": "Thinking longitudinally",
    "section": "Projects and Rmarkdown",
    "text": "Projects and Rmarkdown\nAs with any project, but especially for longitudinal data, one of the most important aspects of data analysis is A. not losing track of what you did and B. being organized.\n\nrstudio projects 2. git and 3. codebooks are helpful in accomplishing these two goals. We will talk about #1 and #3 but I also encourage you to read about git. These are not the only way to do these sorts of analyses but I feel that exposure to them is helpful, as often in the social sciences these sort of decisions are not discussed."
  },
  {
    "objectID": "intro-1.html#packages",
    "href": "intro-1.html#packages",
    "title": "Thinking longitudinally",
    "section": "Packages",
    "text": "Packages\nPackages seems like the most basic step, but it is actually very important. Depending on what gets loaded you might overwrite functions from other packages. (Note: I will often reload or not follow this advice within lectures for didactic reasons, choosing to put library calls above the code)\n\n# load packages\nlibrary(psych)\nlibrary(plyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "intro-1.html#codebook",
    "href": "intro-1.html#codebook",
    "title": "Thinking longitudinally",
    "section": "Codebook",
    "text": "Codebook\nThe second step is a codebook. Arguably, this is the first step because you should create the codebook long before you open R and load your data.\nWhy a codebook? Well, because you typically have a lot of variables and you will not be able to remember all the details that go into each one of them (rating scale, what the actual item was, was it coded someway, etc). This is especially true now that data are being collected online, which often provides placeholder variable names that then need to be processed somehow.\nThis codebook will serve as a means to document RAW code. It will also allow us to automate some tasks that are somewhat cumbersome, facilitate open data practices, and efficiently see what variables are available. Ultimately, we want to be able to show how we got from the start, with the messy raw data, to our analyses and results at the end? A codebook makes this easier."
  },
  {
    "objectID": "intro-1.html#data-1",
    "href": "intro-1.html#data-1",
    "title": "Thinking longitudinally",
    "section": "Data",
    "text": "Data\nFirst, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.\nThis code below shows how I would read in and rename a wide-format data set using the codebook I created.\n\nold.names <- codebook$old_name # get old column names\nnew.names <- codebook$new_name # get new column names\n\nsoep <- read.csv(\"https://raw.githubusercontent.com/josh-jackson/longitudinal-2021/master/soepdata.csv\")\n\n soep <-  soep %>% # read in data\n  dplyr::select(old.names) %>% # select the columns from our codebook\n  setNames(new.names) # rename columns with our new names\nsoep"
  },
  {
    "objectID": "intro-1.html#recode-variables",
    "href": "intro-1.html#recode-variables",
    "title": "Thinking longitudinally",
    "section": "Recode Variables",
    "text": "Recode Variables\nMany of the data we work with have observations that are missing for a variety of reasons. In R, we treat missing values as NA, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit NA values."
  },
  {
    "objectID": "intro-1.html#reverse-scoring",
    "href": "intro-1.html#reverse-scoring",
    "title": "Thinking longitudinally",
    "section": "Reverse-Scoring",
    "text": "Reverse-Scoring\nMany scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.\nThere are a few ways to do this in R. Below, I’ll demonstrate how to do so using the reverse.code() function in the psych package in R. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).\nBefore we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook."
  },
  {
    "objectID": "intro-1.html#create-composites",
    "href": "intro-1.html#create-composites",
    "title": "Thinking longitudinally",
    "section": "Create Composites",
    "text": "Create Composites\nNow that we have reverse coded our items, we can create composites.\nWe’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.\nThe “simplest” way, which is also the longest way because you’d have to do it for each scale, in each year is to use a function like rowMeans which I don’t recommend as that will be MANY MANY lines of code."
  },
  {
    "objectID": "intro-1.html#metric-variables",
    "href": "intro-1.html#metric-variables",
    "title": "Thinking longitudinally",
    "section": "metric variables",
    "text": "metric variables\n\n\nCode\nb5_soep_long_des <- b5_soep_long %>%\n  unite(tmp, trait, year, sep = \"_\") \nhead(b5_soep_long_des)\n\n\n# A tibble: 6 × 5\n  Procedural__SID tmp    value   DOB   Sex\n            <dbl> <chr>  <dbl> <dbl> <dbl>\n1             901 A_2005  4.67  1951     2\n2             901 A_2009  4.33  1951     2\n3             901 A_2013  4.67  1951     2\n4             901 C_2005  5     1951     2\n5             901 C_2009  5     1951     2\n6             901 C_2013  5     1951     2"
  },
  {
    "objectID": "intro-1.html#count-variables",
    "href": "intro-1.html#count-variables",
    "title": "Thinking longitudinally",
    "section": "count variables",
    "text": "count variables\nWe have life event variable in the dataset that is a count variable. It asks did someone experience a life event during the previous year. also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).\n\n\nCode\nevents_long  <-soep_long %>%\n  filter(type == \"Life Event\") \nhead(events_long )\n\n\n# A tibble: 6 × 12\n  Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex type  trait\n            <dbl>            <dbl>            <dbl>            <dbl> <chr> <chr>\n1             901               94             1951                2 Life… MomD…\n2             901               94             1951                2 Life… MomD…\n3            2301              230             1946                1 Life… Move…\n4            2301              230             1946                1 Life… Part…\n5            2301              230             1946                1 Life… Part…\n6            2305              230             1946                2 Life… Move…\n# … with 6 more variables: item <chr>, year <chr>, value <dbl>, reverse <int>,\n#   mini <int>, maxi <int>"
  },
  {
    "objectID": "intro-1.html#zero-order-correlations",
    "href": "intro-1.html#zero-order-correlations",
    "title": "Thinking longitudinally",
    "section": "Zero-Order Correlations",
    "text": "Zero-Order Correlations\nTo run the correlations, we will need to have our data in wide format\n\n\nCode\nb5_soep_long %>%\n  unite(tmp, trait, year, sep = \"_\") %>%\n  pivot_wider(names_from = tmp, values_from = value) %>% \n  select(-Procedural__SID) %>%\n  cor(., use = \"pairwise\") %>%\n  round(., 2)\n\n\n         DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009\nDOB     1.00  0.00   0.02   0.05   0.05   0.13   0.17   0.17  -0.03  -0.01\nSex     0.00  1.00  -0.01  -0.02   0.01  -0.06  -0.05  -0.05   0.10   0.09\nA_2005  0.02 -0.01   1.00   0.30   0.26   0.21   0.07   0.05   0.28   0.12\nA_2009  0.05 -0.02   0.30   1.00   0.32   0.08   0.21   0.06   0.15   0.28\nA_2013  0.05  0.01   0.26   0.32   1.00   0.05   0.07   0.19   0.13   0.15\nC_2005  0.13 -0.06   0.21   0.08   0.05   1.00   0.30   0.26   0.24   0.09\nC_2009  0.17 -0.05   0.07   0.21   0.07   0.30   1.00   0.37   0.09   0.25\nC_2013  0.17 -0.05   0.05   0.06   0.19   0.26   0.37   1.00   0.06   0.07\nE_2005 -0.03  0.10   0.28   0.15   0.13   0.24   0.09   0.06   1.00   0.39\nE_2009 -0.01  0.09   0.12   0.28   0.15   0.09   0.25   0.07   0.39   1.00\nE_2013 -0.08  0.12   0.11   0.15   0.28   0.08   0.11   0.22   0.38   0.42\nN_2005 -0.08  0.13   0.23   0.07   0.08   0.09  -0.02  -0.04   0.18   0.07\nN_2009 -0.04  0.14   0.10   0.23   0.10   0.01   0.10  -0.01   0.09   0.16\nN_2013 -0.05  0.13   0.09   0.07   0.21   0.02  -0.01   0.08   0.09   0.10\nO_2005  0.11  0.06   0.23   0.14   0.13   0.24   0.14   0.13   0.36   0.23\nO_2009  0.10  0.05   0.10   0.24   0.13   0.14   0.23   0.12   0.19   0.34\nO_2013  0.05  0.07   0.10   0.12   0.23   0.11   0.11   0.20   0.19   0.21\n       E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013\nDOB     -0.08  -0.08  -0.04  -0.05   0.11   0.10   0.05\nSex      0.12   0.13   0.14   0.13   0.06   0.05   0.07\nA_2005   0.11   0.23   0.10   0.09   0.23   0.10   0.10\nA_2009   0.15   0.07   0.23   0.07   0.14   0.24   0.12\nA_2013   0.28   0.08   0.10   0.21   0.13   0.13   0.23\nC_2005   0.08   0.09   0.01   0.02   0.24   0.14   0.11\nC_2009   0.11  -0.02   0.10  -0.01   0.14   0.23   0.11\nC_2013   0.22  -0.04  -0.01   0.08   0.13   0.12   0.20\nE_2005   0.38   0.18   0.09   0.09   0.36   0.19   0.19\nE_2009   0.42   0.07   0.16   0.10   0.23   0.34   0.21\nE_2013   1.00   0.08   0.07   0.17   0.20   0.21   0.35\nN_2005   0.08   1.00   0.34   0.32   0.14   0.03   0.05\nN_2009   0.07   0.34   1.00   0.39   0.05   0.13   0.03\nN_2013   0.17   0.32   0.39   1.00   0.04   0.06   0.15\nO_2005   0.20   0.14   0.05   0.04   1.00   0.58   0.55\nO_2009   0.21   0.03   0.13   0.06   0.58   1.00   0.61\nO_2013   0.35   0.05   0.03   0.15   0.55   0.61   1.00"
  },
  {
    "objectID": "intro-1.html#tidyr-pivot-functions",
    "href": "intro-1.html#tidyr-pivot-functions",
    "title": "Thinking longitudinally",
    "section": "tidyr pivot functions",
    "text": "tidyr pivot functions\nFor longitudinal/repeated measures data, each row is an observation. Each person will have multiple rows.\n\n\n# A tibble: 225 × 4\n      ID  wave group   DAN\n   <dbl> <dbl> <chr> <dbl>\n 1     6     1 PD    0.162\n 2     6     2 PD    0.168\n 3     6     3 PD    0.215\n 4    29     1 PD    0.175\n 5    29     2 PD    0.136\n 6    34     1 CTRL  0.166\n 7    34     2 CTRL  0.140\n 8    36     1 CTRL  0.152\n 9    36     2 CTRL  0.205\n10    37     1 PD    0.219\n# … with 215 more rows"
  },
  {
    "objectID": "intro-1.html#descriptives",
    "href": "intro-1.html#descriptives",
    "title": "Thinking longitudinally",
    "section": "Descriptives",
    "text": "Descriptives\nDescriptives of your data are incredibly important. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.\nThere are lots of ways to create great tables of descriptives.For now, we’ll use a wonderfully helpful function from the psych package called describe() in conjunction with a small amount of tidyr to reshape the data."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor:\nJoshua Jackson\n\n\nOffice:\n315b\n\n\nOffice hours:\nThur after class and by appt\nThis course covers modern methods of handling longitudinal, repeated measures. The class will introduce the rationale of measuring change and stability over time to study phenomena, as well as how within-person designs can increase statistical power and precision compared to more traditional designs. Most the course will use multi-level models and latent (growth) curve models to specify patterns of change across time. Additional topics include: visualization, measurement invariance, time-to- event models and power. PREREQ: Use of R will be required, Familiarity with MLM and/or Structural Equation Models.\n\n\nClass textbooks\nHoffman, Lesa. (2015). Longitudinal Analysis: Modeling Within-Person Fluctuation and Change. (reffered to as LA)\nLittle, T. D. (2013). Longitudinal structural equation modeling. Guilford press. (referred to as LSEM)\nI have PDFs available if you do not want to purchase the textbook.\nI find both of these textbooks quite readable, but if you are struggling I have additional recommendations for intro MLM and SEM that may fill in the gaps. Additionally, if there are topics that you are interested in and want to know more about please let me know as there are a number of textbooks devoted to a single topic.\n\n\nGrading\nGrading consists of 3 aspects:\n\nSemi-weekly “pop” quiz. Quizzes consist of 1-3 questions based on the previous lecture and readings.\nHomework take homes. Homework will be (typically) due 1 week later.\nFinal project. Everyone needs to do a longitudinal data analysis from start to finish. Preferably you have your own data, but if you do not then let me know and I can give you some. More details on this later.\n\nGrading breakdown is 1/3 for each of these components.\n\n\nSchedule\nThis will likely change based on our pace and what we want to cover more/less in depth\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nReadings\n\n\n\n\n1\n8/30\nMotivation and data\nLA Ch 1 (2 optional)\n\n\n2\n9/6\nLongitudinal MLM\nLA Ch 3, 5\n\n\n3\n9/13\nLongitudinal MLM\n\n\n\n4\n9/20\nLongitudinal SEM\nLSEM 1,3\n\n\n5\n9/27\nLongitudinal SEM\nLSEM 8\n\n\n6\n10/4\nMeasurement invariance and treating time\nLSEM 7\n\n\n7\n10/11\nCurves that bend\nLA 6\n\n\n8\n10/18\nCurves that bend\n\n\n\n9\n10/25\nPredictors of change\nLA 7\n\n\n10\n11/1\nPredictors of change\n\n\n\n11\n11/8\nIntensive longitudinal designs\nLA 8\n\n\n12\n11/15\nMultivariate change\nLA 9\n\n\n13\n11/22\nMultivariate change\n\n\n\n14\n11/29\n2 measurement points\n\n\n\n15\n12/6\nMixture Models\n\n\n\n\nOther topics: Longitudinal mediation (and multilevel mediation), time to event analyses, power, …?"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "readings",
    "section": "",
    "text": "Extra readings will go here"
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "homeworks",
    "section": "",
    "text": "Homework assignments will go here"
  },
  {
    "objectID": "mlm-2.html#standard-regression",
    "href": "mlm-2.html#standard-regression",
    "title": "MLM",
    "section": "Standard regression",
    "text": "Standard regression\n\\[{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\\epsilon_{i}\\]\n\\[\\hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+...\\]\nParameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.\nEach person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i."
  },
  {
    "objectID": "mlm-2.html#handling-multiple-dvs",
    "href": "mlm-2.html#handling-multiple-dvs",
    "title": "MLM",
    "section": "Handling multiple DVs?",
    "text": "Handling multiple DVs?\nBut what if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple ____ ?\nTwo options: 1. Collapse and average across."
  },
  {
    "objectID": "mlm-2.html#example",
    "href": "mlm-2.html#example",
    "title": "MLM",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "mlm-2.html#could-aggragate-across-group",
    "href": "mlm-2.html#could-aggragate-across-group",
    "title": "MLM",
    "section": "could aggragate across group",
    "text": "could aggragate across group"
  },
  {
    "objectID": "mlm-2.html#aggregation-obscures-hypotheses",
    "href": "mlm-2.html#aggregation-obscures-hypotheses",
    "title": "MLM",
    "section": "Aggregation obscures hypotheses",
    "text": "Aggregation obscures hypotheses\n\nBetween person H1: Do students who study more get better grades?\nWithin person H2: When a student studies, do they get better grades?\nH1 and H2 are independent from one another! Aggregation collapses the two. When you have nested data with many DVs (longitudinal but also almost any experimental situation) it is important to not aggregate."
  },
  {
    "objectID": "mlm-2.html#stroop-example",
    "href": "mlm-2.html#stroop-example",
    "title": "MLM",
    "section": "Stroop example",
    "text": "Stroop example\nWe calculate stroop scores by looking at repeated trials of congruent vs not congruent. This is dummy coded such that the \\(\\beta_{1}\\) reflects the average stroop effect. How much slower are people in incongruent trials?\n\\[Y_{i} = \\beta_{0} + \\beta_{1}X_{1} + \\varepsilon_i\\]"
  },
  {
    "objectID": "mlm-2.html#ways-to-think-about-mlms",
    "href": "mlm-2.html#ways-to-think-about-mlms",
    "title": "MLM",
    "section": "4 ways to think about MLMs",
    "text": "4 ways to think about MLMs\n\nDifferent levels of analysis (average/person specific or between/within)\n\nRegressions within regressions (ie coefficients as outcomes)\n\nVariance decomposition\n\nLearning from other data through pooling/shrinkage"
  },
  {
    "objectID": "mlm-2.html#get-new-example-here.",
    "href": "mlm-2.html#get-new-example-here.",
    "title": "MLM",
    "section": "GET NEW EXAMPLE HERE.",
    "text": "GET NEW EXAMPLE HERE."
  },
  {
    "objectID": "mlm-2.html#regressions-within-regressions",
    "href": "mlm-2.html#regressions-within-regressions",
    "title": "MLM",
    "section": "regressions within regressions",
    "text": "regressions within regressions\nHelps to take multilevel and split it into the different levels.\nLevel 1 is the smallest unit of analysis (students, waves, trials, family members)\nLevel 2 variables are what level 1 variables are “nested” in (people, schools, counties, families, dyads)\nWe are going to use level one components to run a regression, all the while level 1 is also estimating a regression"
  },
  {
    "objectID": "mlm-2.html#diferent-levels-and-regressions-within-regressions",
    "href": "mlm-2.html#diferent-levels-and-regressions-within-regressions",
    "title": "MLM",
    "section": "diferent levels and regressions within regressions",
    "text": "diferent levels and regressions within regressions\nTo sum up the first two ways to think about regression, we take a relationship that could be simplified by aggregation, but instead we ask regression questions at different levels of analysis.\nAt level 1 we can ask lower-unit questions e.g., if trials are nested within person, what predicts lengthier trials?\nLongitudinally, if level 1 is observations, then we can ask level 1/observation level questions like if you’re with people do you then feel happier"
  },
  {
    "objectID": "mlm-2.html#variance-decomposition",
    "href": "mlm-2.html#variance-decomposition",
    "title": "MLM",
    "section": "variance decomposition",
    "text": "variance decomposition\nFor standard regression, we think of error as existing in one big bucket called \\(\\varepsilon\\) . Everything that we do not know goes into that bucket, from measurement error to unmeasured important factors.\nFor MLMs we will be breaking up ( \\(\\varepsilon\\) ) into multiple buckets. These useful “buckets” (Us) are what we refer to as random/varying effects.\n\\[Y_{trials, i} = \\beta_{0i} + \\beta_{1i}X_{trial,i} + \\varepsilon_{trial}\\] \\[\\beta_{0} = \\gamma_{00} + U_{0i}\\] \\[\\beta_{1} = \\gamma_{10} +\\gamma_{11}Z_i+ U_{1i}\\]"
  },
  {
    "objectID": "mlm-2.html#shrinkagepartial-pooling",
    "href": "mlm-2.html#shrinkagepartial-pooling",
    "title": "MLM",
    "section": "shrinkage/partial pooling",
    "text": "shrinkage/partial pooling\n\nWe treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions.\nWe do this in standard regression where we make predictions based on values from the whole dataset (not just binned Xs). A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not overfitting our data!"
  },
  {
    "objectID": "mlm-2.html#complete-partial-and-no-pooling",
    "href": "mlm-2.html#complete-partial-and-no-pooling",
    "title": "MLM",
    "section": "Complete, partial and no pooling",
    "text": "Complete, partial and no pooling\n\nComplete assumes everyone is the same, with \\(U_{0i}\\) being zero for everyone.\nNo pooling is if we calculate every person’s effect with a regression, subtracting out he grand mean average.\nPartial pooling is in the middle, a weighted average between the two. For those with fewer trials there is less information for a particular individual, thus the complete pooling estimate will be given more weight. If someone has a lot of data, there weighted average is closer to no pooling.\nPartial pooling prevents both over and under fitting of your data, leading to increased out of sample predictions."
  },
  {
    "objectID": "mlm-2.html#basic-longitudinal-models",
    "href": "mlm-2.html#basic-longitudinal-models",
    "title": "MLM",
    "section": "Basic Longitudinal Models",
    "text": "Basic Longitudinal Models\nTo keep with the book, we are going to discuss DVs that take on different values at each timepoint t, for individual i \\({Y}_{ti}\\) Other naming schemes are equivalent such as the same \\({Y}_{ij}\\) where i’s are nested in j groups."
  },
  {
    "objectID": "mlm-2.html#empty-model",
    "href": "mlm-2.html#empty-model",
    "title": "MLM",
    "section": "Empty model",
    "text": "Empty model\nLevel 1 \\[{Y}_{ti} = \\beta_{0i}  + \\varepsilon_{ti}\\]\nLevel 2 \\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\\[{e}_{ti} \\sim \\mathcal{N}(0, \\sigma^{2})\\]\n\\[{U}_{0i} \\sim \\mathcal{N}(0, \\tau_{00}^{2})\\]"
  },
  {
    "objectID": "mlm-2.html#what-does-this-look-like",
    "href": "mlm-2.html#what-does-this-look-like",
    "title": "MLM",
    "section": "What does this look like?",
    "text": "What does this look like?"
  },
  {
    "objectID": "mlm-2.html#combined-equation",
    "href": "mlm-2.html#combined-equation",
    "title": "MLM",
    "section": "combined equation",
    "text": "combined equation\n\\[{Y}_{ti} = \\gamma_{00} + U_{0i}  + \\varepsilon_{ti}\\]\nAkin to ANOVA if we treat \\(U_{0i}\\) as between subjects variance & \\(\\varepsilon_{ti}\\) as within subjects variance.\n\n\\(\\gamma_{00}\\) is fixed or constant across people\n\\(U_{0i}\\) is random or varies across people"
  },
  {
    "objectID": "mlm-2.html#icc",
    "href": "mlm-2.html#icc",
    "title": "MLM",
    "section": "ICC",
    "text": "ICC\nBetween version variance over total variance. If the ICC is greater than zero, we are breaking standard regression assumptions as this is an index of dependency.\n\\[\\frac{U_{0i}}{U_{0i}+ \\varepsilon_{ti}}\\]\nICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals)."
  },
  {
    "objectID": "mlm-2.html#add-example",
    "href": "mlm-2.html#add-example",
    "title": "MLM",
    "section": "ADD EXAMPLE",
    "text": "ADD EXAMPLE"
  },
  {
    "objectID": "mlm-2.html#what-do-the-random-effects-look-like",
    "href": "mlm-2.html#what-do-the-random-effects-look-like",
    "title": "MLM",
    "section": "what do the random effects look like?",
    "text": "what do the random effects look like?\n\n\nCode\nlibrary(modelbased)\nrandom <- estimate_grouplevel(mod.1) \nhead(random)\n\n\nGroup | Level |   Parameter | Coefficient |   SE |        95% CI\n----------------------------------------------------------------\nID    |   901 | (Intercept) |        0.14 | 0.24 | [-0.33, 0.61]\nID    |  2301 | (Intercept) |        0.33 | 0.24 | [-0.14, 0.80]\nID    |  2302 | (Intercept) |        0.27 | 0.24 | [-0.20, 0.74]\nID    |  4701 | (Intercept) |       -0.04 | 0.27 | [-0.57, 0.48]\nID    |  4901 | (Intercept) |        0.01 | 0.24 | [-0.46, 0.48]\nID    |  5201 | (Intercept) |        0.59 | 0.24 | [ 0.12, 1.06]"
  },
  {
    "objectID": "mlm-2.html#base-r-dealing-with-random-effects",
    "href": "mlm-2.html#base-r-dealing-with-random-effects",
    "title": "MLM",
    "section": "base r dealing with random effects",
    "text": "base r dealing with random effects\ncoef = fixef + raneff\n\nglimpse(ranef(mod.1))\n\nList of 1\n $ ID:'data.frame': 8565 obs. of  1 variable:\n  ..$ (Intercept): num [1:8565] 0.1388 0.3318 0.2675 -0.0448 0.0101 ...\n  ..- attr(*, \"postVar\")= num [1, 1, 1:8565] 0.0581 0.0581 0.0581 0.0719 0.0581 ...\n - attr(*, \"class\")= chr \"ranef.mer\"\n\n\n\nglimpse(coef(mod.1))\n\nList of 1\n $ ID:'data.frame': 8565 obs. of  1 variable:\n  ..$ (Intercept): num [1:8565] 4.9 5.09 5.03 4.72 4.77 ...\n - attr(*, \"class\")= chr \"coef.mer\"\n\n\n\nfixef(mod.1)\n\n(Intercept) \n   4.760256"
  },
  {
    "objectID": "mlm-2.html#residuals",
    "href": "mlm-2.html#residuals",
    "title": "MLM",
    "section": "Residuals",
    "text": "Residuals\nTo get residuals and fitted scores\n\n\nCode\nlibrary(broom.mixed)\nexample.aug<- augment(mod.1, data = b5_long)\n\nexample.aug\n\n\n# A tibble: 22,081 × 21\n   Procedural__SID year    DOB   Sex     A     C     E     N     O ID    .fitted\n             <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <fct>   <dbl>\n 1             901 2005   1951     2  4.67  5     5     4     4    901      4.90\n 2             901 2009   1951     2  4.33  5     5     4.67  4    901      4.90\n 3             901 2013   1951     2  4.67  5     5     4.33  3.33 901      4.90\n 4            2301 2005   1946     1  5.67  5.67  5     4.33  4.33 2301     5.09\n 5            2301 2009   1946     1  5.67  5     4     5.33  5    2301     5.09\n 6            2301 2013   1946     1  4.67  5.33  5.33  3.67  5    2301     5.09\n 7            2302 2005   1946     2  4.67  4.67  4.67  3.67  5    2302     5.03\n 8            2302 2009   1946     2  4     6     5.33  3     4.33 2302     5.03\n 9            2302 2013   1946     2  3.33  5     4.67  3     5.67 2302     5.03\n10            4701 2005   1919     2  4     4.33  4.67  3.67  3    4701     4.72\n# … with 22,071 more rows, and 10 more variables: .resid <dbl>, .hat <dbl>,\n#   .cooksd <dbl>, .fixed <dbl>, .mu <dbl>, .offset <dbl>, .sqrtXwt <dbl>,\n#   .sqrtrwt <dbl>, .weights <dbl>, .wtres <dbl>\n\n\nCode\n# .fitted    = predicted values\n# .resid    = residuals/errors\n# .fixed     = predicted values with no random effects"
  },
  {
    "objectID": "mlm-2.html#predicted-scores",
    "href": "mlm-2.html#predicted-scores",
    "title": "MLM",
    "section": "Predicted scores",
    "text": "Predicted scores\nPredictors are super important for evaluating our model as well as graphing. Lots of packages have these capabilities. My favorites are tidybayes (for brms), marginaleffects (for all), modelbased (for all) and insight both from easystats.\nThese extend the flexibility of predict (base) and similar functions from emmeans"
  },
  {
    "objectID": "mlm-2.html#a-predictor-in-level-1",
    "href": "mlm-2.html#a-predictor-in-level-1",
    "title": "MLM",
    "section": "A predictor in level 1",
    "text": "A predictor in level 1\nLevel 1 is where you have data that repeats within your grouping or clustering data. Is your cluster classrooms? Then students are level 1. People will be our cluster and observations are level 1.\n\\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nNotice on the subscript of X that these predictors vary across cluster (i) and within the cluster (t) So if your clustering (i) is people, then t refers to different observations."
  },
  {
    "objectID": "mlm-2.html#person-predictions",
    "href": "mlm-2.html#person-predictions",
    "title": "MLM",
    "section": "person predictions",
    "text": "person predictions\nCan think of a persons score divided up into a fixed component as well as the random component.\n\\[{\\beta}_{1.26} = \\gamma_{10} \\pm U_{26}\\] Also call BLUPs or empirical bayes estimates"
  },
  {
    "objectID": "mlm-2.html#longidutidnal-example",
    "href": "mlm-2.html#longidutidnal-example",
    "title": "MLM",
    "section": "LONGIDUTIDNAL EXAMPLE",
    "text": "LONGIDUTIDNAL EXAMPLE\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: SMN7 ~ 1 + year + (1 | ID)\n   Data: example\n\nREML criterion at convergence: -675.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2308 -0.4868 -0.0377  0.4542  3.2337 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.001815 0.04261 \n Residual             0.001300 0.03606 \nNumber of obs: 216, groups:  ID, 88\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.104041   0.005733  18.147\nyear        0.001331   0.001755   0.758\n\nCorrelation of Fixed Effects:\n     (Intr)\nyear -0.426"
  },
  {
    "objectID": "mlm-2.html#error-structure",
    "href": "mlm-2.html#error-structure",
    "title": "MLM",
    "section": "Error Structure",
    "text": "Error Structure\nThe residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.\nG matrix (books term) \\[\\begin{pmatrix} {U}_{0j} \\\\ {U}_{1j} \\end{pmatrix}\n\\sim \\mathcal{N} \\begin{pmatrix}\n  0,      \\tau_{00}^{2} & \\tau_{01}\\\\\n  0,  \\tau_{01} & \\tau_{10}^{2}\n\\end{pmatrix}\\]\n\\[{e}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})\\] The \\({e}_{ij}\\) is structured as a NxN matrix, where n reflect number of waves\nNote that it is possible to impose a different error structure depending on your needs."
  },
  {
    "objectID": "mlm-2.html#decomposing-variance-for-random-intercept-model",
    "href": "mlm-2.html#decomposing-variance-for-random-intercept-model",
    "title": "MLM",
    "section": "Decomposing variance for random intercept model",
    "text": "Decomposing variance for random intercept model\n\\[\\text{Total variance CS} = \\begin{pmatrix}\n       \\tau_{00}^{2} + \\sigma^{2}& \\tau_{00}^{2} & \\tau_{00}^{2}\\\\\n       \\tau_{00}^{2} &  \\tau_{00}^{2} + \\sigma^{2} &  \\tau_{00}^{2}\\\\\n       \\tau_{00}^{2} & \\tau_{00}^{2} &   \\tau_{00}^{2} + \\sigma^{2}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "mlm-2.html#error-assumptions",
    "href": "mlm-2.html#error-assumptions",
    "title": "MLM",
    "section": "Error assumptions",
    "text": "Error assumptions\nLevel 1 residuals are independent for Level 1 units across people\nAND\nLevel 1 residuals are independent of random effects\nAND\nLevel 1 residuals are the same magnitude across people\nWe can modify a standard assumption: Level 1 residuals are independent within a person through different variance/covariance structures"
  },
  {
    "objectID": "mlm-2.html#centering",
    "href": "mlm-2.html#centering",
    "title": "MLM",
    "section": "centering",
    "text": "centering\nBecause mlms are regressions, and because mlms involve interactions, it is important to consider how your predictors zero point is defined.\nHow do you want your intercept interpreted? How do you want lower order terms in an interaction interpreted?\nWe will use these extensively to help disentangle within and between person variance."
  },
  {
    "objectID": "mlm-2.html#uncentered",
    "href": "mlm-2.html#uncentered",
    "title": "MLM",
    "section": "Uncentered",
    "text": "Uncentered\nThe default will give you predicted score of intercept when all predictors are zero.\nBecause most models will have a random intercept, it is important to keep in mind interpretations as we will be looking at variations around this value."
  },
  {
    "objectID": "mlm-2.html#grand-mean-centered",
    "href": "mlm-2.html#grand-mean-centered",
    "title": "MLM",
    "section": "Grand mean Centered",
    "text": "Grand mean Centered\nZero now represents that grand mean of the sample. Calculated by taking \\(x_{ti} - \\bar{x}\\)\nUseful as this is often our the default in other methods. Changes meaning of intercept but not slope.\nA related way to center is group grand mean centering where you take the mean of your grouping variables rather than the grand mean."
  },
  {
    "objectID": "mlm-2.html#group-mean-centering-person-centering",
    "href": "mlm-2.html#group-mean-centering-person-centering",
    "title": "MLM",
    "section": "group mean centering (person centering)",
    "text": "group mean centering (person centering)\nCalculated by taking \\(x_{ti} - \\bar{x_i}\\)\nCan change meaning of intercept and slope. Intercept is now a person’s average level rather than the samples average level (grand mean) and level when predictors = 0 (no centering)\nSlope at level 1 is the expected change relative to a person’s average."
  },
  {
    "objectID": "mlm-2.html#estimation",
    "href": "mlm-2.html#estimation",
    "title": "MLM",
    "section": "Estimation",
    "text": "Estimation\nWe need to identify: 1. the estimates of each parameter 2. some measure of precision of that estimate (SEs) 3. an index of overall model fit (deviance/-2LL/aic/bic)\nWe will use maximum likelihood (and variants of) as well as MCMC (Bayesian) for estimation.\nModel comparison is usually done through a likelihood ratio test distributed as a chi square."
  },
  {
    "objectID": "mlm-2.html#ml-vs-reml",
    "href": "mlm-2.html#ml-vs-reml",
    "title": "MLM",
    "section": "ML vs REML",
    "text": "ML vs REML\nREML = Restricted maximum likelihood\nSimilar to sample vs population estimates of SD where we do or don’t divide by n-1, ML downward biased random effect estimates.\nREML maximizes the likelihood of the residuals, so models with different fixed effects are not on the same scale and are not comparable. As a result, you cannot compare fixed models with likleihood metrics (aic) with REML. You can compare variance differences."
  },
  {
    "objectID": "mlm-2.html#transitioning-to-longitudinal-applications",
    "href": "mlm-2.html#transitioning-to-longitudinal-applications",
    "title": "MLM",
    "section": "transitioning to longitudinal applications",
    "text": "transitioning to longitudinal applications\nWe are going to fit a simple longitudinal model: a growth model. Growth model is just a fancy term for including TIME as our level 1 predictor where we are now creating lines for each person."
  },
  {
    "objectID": "mlm-2.html#within-person-empty-model",
    "href": "mlm-2.html#within-person-empty-model",
    "title": "MLM",
    "section": "within person empty model",
    "text": "within person empty model\n\\[{Y}_{ti} = \\gamma_{00} + U_{0i}  + \\varepsilon_{ti}\\]\nThis model is helpful in producing the simplest longitudinal model, one where it states: there is an average value \\(\\gamma_{00}\\) that people differ along \\(U_{0i}\\) . Because time is not in the model it assumes people do not change. \\(\\varepsilon_{ti}\\) reflects variation around each person’s predicted score ( \\(\\gamma_{00} + U_{0i}\\) )."
  },
  {
    "objectID": "mlm-2.html#adding-time",
    "href": "mlm-2.html#adding-time",
    "title": "MLM",
    "section": "Adding time",
    "text": "Adding time\nLevel 1:\n\\[{Y}_{it} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10}\\]\nAny time we have a level 1 variable we can model it at level 2. If we want to assume it is constant across people, we do not include a random effect."
  },
  {
    "objectID": "mlm-2.html#random-slope",
    "href": "mlm-2.html#random-slope",
    "title": "MLM",
    "section": "random slope",
    "text": "random slope\nLevel 1: \\[{Y}_{it} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nLevel 2:\n\\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10} + U_{1i}\\]\nCombined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{10} (X_{1i})+ U_{0i}  + U_{1i}(X_{1i}) + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "mlm-2.html#correlations-among-random-effects",
    "href": "mlm-2.html#correlations-among-random-effects",
    "title": "MLM",
    "section": "correlations among random effects",
    "text": "correlations among random effects\n\\[\\begin{pmatrix} {U}_{0i} \\\\ {U}_{1i} \\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} 0, & \\tau_{00}^{2} & \\tau_{01}\\\\ 0, & \\tau_{01} & \\tau_{10}^{2} \\end{pmatrix}\\]\nThe variances and the covariation (correlations) can be of substantive interest. What do each of these terms reflect? What if one of the terms was zero, what would that mean?"
  },
  {
    "objectID": "mlm-2.html#residual",
    "href": "mlm-2.html#residual",
    "title": "MLM",
    "section": "residual",
    "text": "residual\n\\[ {\\varepsilon}_{ti} \\sim \\mathcal{N}(0, \\sigma^{2})  \\] Much like in normal regression models we often use \\(\\sigma^{2}\\) as a means to describe the fit of the model"
  },
  {
    "objectID": "mlm-2.html#model-comparisons",
    "href": "mlm-2.html#model-comparisons",
    "title": "MLM",
    "section": "model comparisons",
    "text": "model comparisons\nIn setting up the basic growth model we have a series of questions to address:\n\nDo we need to add a time component?\nIf so, do we need to allow that to vary across people?\nif so, do we want to allow the intercept to correlate with the slope?\n\nUsually 1 & 2 are explicitly tested whereas 3 is more theoretical"
  },
  {
    "objectID": "mlm-2.html#centering-redux",
    "href": "mlm-2.html#centering-redux",
    "title": "MLM",
    "section": "centering redux",
    "text": "centering redux\nThe correlation among random intercept and slopes is directly related to centering of variables. The two standard choices for time is to center at the mean of time or at the start of time. Both have their pros and cons."
  },
  {
    "objectID": "mlm-2.html#gsoep-example",
    "href": "mlm-2.html#gsoep-example",
    "title": "MLM",
    "section": "GSOEP EXAMPLE",
    "text": "GSOEP EXAMPLE\n\n\n\n\nstr(b5_soep_long)\n\ntibble [151,186 × 6] (S3: tbl_df/tbl/data.frame)\n $ Procedural__SID: num [1:151186] 901 901 901 901 901 901 901 901 901 901 ...\n $ trait          : chr [1:151186] \"A\" \"A\" \"A\" \"C\" ...\n $ year           : chr [1:151186] \"2005\" \"2009\" \"2013\" \"2005\" ...\n $ value          : num [1:151186] 4.67 4.33 4.67 5 5 ...\n $ DOB            : num [1:151186] 1951 1951 1951 1951 1951 ...\n $ Sex            : num [1:151186] 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\nCode\nb5_long <- b5_soep_long %>% \n  pivot_wider(names_from = trait, values_from = value) %>% \n  mutate(ID = Procedural__SID) %>% \n  mutate(ID = as.factor(ID))\n\n\n\n\n\n\nstr(b5_long)\n\ntibble [22,098 × 10] (S3: tbl_df/tbl/data.frame)\n $ Procedural__SID: num [1:22098] 901 901 901 2301 2301 ...\n $ year           : chr [1:22098] \"2005\" \"2009\" \"2013\" \"2005\" ...\n $ DOB            : num [1:22098] 1951 1951 1951 1946 1946 ...\n $ Sex            : num [1:22098] 2 2 2 1 1 1 2 2 2 2 ...\n $ A              : num [1:22098] 4.67 4.33 4.67 5.67 5.67 ...\n $ C              : num [1:22098] 5 5 5 5.67 5 ...\n $ E              : num [1:22098] 5 5 5 5 4 ...\n $ N              : num [1:22098] 4 4.67 4.33 4.33 5.33 ...\n $ O              : num [1:22098] 4 4 3.33 4.33 5 ...\n $ ID             : Factor w/ 16719 levels \"901\",\"1202\",\"2301\",..: 1 1 1 3 3 3 4 4 4 7 ..."
  },
  {
    "objectID": "mlm-2.html#slope-example",
    "href": "mlm-2.html#slope-example",
    "title": "MLM",
    "section": "slope example",
    "text": "slope example\n\nmod.2 <- lmer(C ~ 1 + year + (1 | ID), data=b5_long)\n\nsummary(mod.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year + (1 | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42793.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2995 -0.4572  0.0548  0.4152  4.7573 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.1378   0.3712  \n Residual             0.3009   0.5485  \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.752544   0.007566 628.165\nyear2009    0.009782   0.008756   1.117\nyear2013    0.014584   0.009773   1.492\n\nCorrelation of Fixed Effects:\n         (Intr) yr2009\nyear2009 -0.621       \nyear2013 -0.565  0.488"
  },
  {
    "objectID": "mlm-2.html#visualizing-results-quickly",
    "href": "mlm-2.html#visualizing-results-quickly",
    "title": "MLM",
    "section": "Visualizing results (quickly)",
    "text": "Visualizing results (quickly)\nThere are many ways to do this. The parameters package paired with the see package, both from the easystats package, are useful in this regard.\n\nlibrary(parameters)\nlibrary(see)\n\nresult <- model_parameters(mod.4)"
  },
  {
    "objectID": "mlm-2.html#including-a-random-slope",
    "href": "mlm-2.html#including-a-random-slope",
    "title": "MLM",
    "section": "Including a random slope",
    "text": "Including a random slope\nlevel 1 \\[{Y}_{ti} = \\beta_{0i}  + \\beta_{1i}X_{ti} + \\varepsilon_{ti}\\]\nlevel 2 \\[{\\beta}_{0i} = \\gamma_{00} + U_{0i}\\]\n\n\\[{\\beta}_{1i} = \\gamma_{10}+ U_{1i}\\] combined: \\[{Y}_{ti} = \\gamma_{00} + \\gamma_{10}(X_{ti})+ U_{0i} + U_{1i}(X_{ti}) + \\varepsilon_{ti}\\]\n\\[{Y}_{ti} = (\\gamma_{00} + U_{0i}) + (\\gamma_{10}+  U_{1i})X_{ti} + \\varepsilon_{ti}\\]"
  },
  {
    "objectID": "mlm-2.html#adding-a-random-slope",
    "href": "mlm-2.html#adding-a-random-slope",
    "title": "MLM",
    "section": "Adding a random slope",
    "text": "Adding a random slope\n\nb5_long<- b5_long %>%  \n  mutate(year.c = (year - 2005))\n  \n  \nmod.6 <- lmer(C ~ 1 + year.c + (1 + year.c  | ID), data=b5_long)\n\nsummary(mod.6)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year.c + (1 + year.c | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42714\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0083 -0.4685  0.0461  0.4376  4.8649 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n ID       (Intercept) 0.1050567 0.32412      \n          year.c      0.0002853 0.01689  0.75\n Residual             0.2966451 0.54465      \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.753235   0.006765  702.59\nyear.c      0.001783   0.001230    1.45\n\nCorrelation of Fixed Effects:\n       (Intr)\nyear.c -0.591\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00437407 (tol = 0.002, component 1)"
  },
  {
    "objectID": "mlm-2.html#as-starting",
    "href": "mlm-2.html#as-starting",
    "title": "MLM",
    "section": "0 as starting",
    "text": "0 as starting\n\nb5_long<- b5_long %>%  \n  mutate(year.c = (year - 2005))\n  \n  \nmod.6 <- lmer(C ~ 1 + year.c + (1 + year.c  | ID), data=b5_long)\n\nsummary(mod.6)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year.c + (1 + year.c | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42714\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0083 -0.4685  0.0461  0.4376  4.8649 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n ID       (Intercept) 0.1050567 0.32412      \n          year.c      0.0002853 0.01689  0.75\n Residual             0.2966451 0.54465      \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.753235   0.006765  702.59\nyear.c      0.001783   0.001230    1.45\n\nCorrelation of Fixed Effects:\n       (Intr)\nyear.c -0.591\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00437407 (tol = 0.002, component 1)"
  },
  {
    "objectID": "mlm-2.html#as-mid",
    "href": "mlm-2.html#as-mid",
    "title": "MLM",
    "section": "0 as mid",
    "text": "0 as mid\n\nb5_long<- b5_long %>%  \n  mutate(year.cM = (year - 2009))\n  \n  \nmod.7 <- lmer(C ~ 1 + year.cM + (1 + year.cM | ID), data=b5_long)\n\nsummary(mod.7)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: C ~ 1 + year.cM + (1 + year.cM | ID)\n   Data: b5_long\n\nREML criterion at convergence: 42714\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0085 -0.4685  0.0461  0.4376  4.8649 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n ID       (Intercept) 0.1426960 0.37775      \n          year.cM     0.0002854 0.01689  0.83\n Residual             0.2966298 0.54464      \nNumber of obs: 22081, groups:  ID, 8565\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 4.760368   0.005535  860.07\nyear.cM     0.001783   0.001230    1.45\n\nCorrelation of Fixed Effects:\n        (Intr)\nyear.cM 0.166 \noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00601838 (tol = 0.002, component 1)"
  },
  {
    "objectID": "mlm-2.html#testing-significance",
    "href": "mlm-2.html#testing-significance",
    "title": "MLM",
    "section": "Testing significance",
    "text": "Testing significance\nMethods for testing single parameters From worst to best:\n\nWald Z-tests. Easy to compute. However, they are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.\nWald t-tests\nLikelihood ratio test.\n\nMarkov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals"
  },
  {
    "objectID": "mlm-2.html#likelhiood-ratio-test",
    "href": "mlm-2.html#likelhiood-ratio-test",
    "title": "MLM",
    "section": "Likelhiood ratio test",
    "text": "Likelhiood ratio test\nHow much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).\nLog Likelihood (LL) is derived from ML estimation. Larger the LL the better the fit. Deviance compares two LLs. Current model and a saturated model (that fits data perfectly).\nDeviance = -2[LL current - LL saturated]\nLL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out.\nDeviance = -2LL current model."
  },
  {
    "objectID": "mlm-2.html#likelhiood-ratio-test-1",
    "href": "mlm-2.html#likelhiood-ratio-test-1",
    "title": "MLM",
    "section": "Likelhiood ratio test",
    "text": "Likelhiood ratio test\nComparing 2 models is called a likelihood ration test. Need to have: 1. same data 2. nested models (think of constraining a parameter to zero)\nDistributed as chi-square with df equal to constraint differences between models.\n\n\nData: b5_long\nModels:\nmod.1: C ~ 1 + (1 | ID)\nmod.2: C ~ 1 + year + (1 | ID)\n      npar   AIC   BIC logLik deviance  Chisq Df Pr(>Chisq)\nmod.1    3 42778 42802 -21386    42772                     \nmod.2    5 42780 42820 -21385    42770 2.4256  2     0.2974\n\n\nWhat does this look like pictorially?"
  },
  {
    "objectID": "mlm-2.html#other-types-of-models",
    "href": "mlm-2.html#other-types-of-models",
    "title": "MLM",
    "section": "Other types of models",
    "text": "Other types of models\nDepending on your DV, you might not want to have a Gaussian sampling distribution. Instead you may want something like a Poisson or a negative binomial if you are using some sort of count data. You can do this somewhat with lme4. However, the BRMS package – which uses Bayesian estimation – has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. Maybe we will fit some of these later in the semester."
  }
]